[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistik mit R für Umweltwissenschaftler:innen",
    "section": "",
    "text": "Vorwort\nJürgen Dengler\nIch bin Ökologe, kein Statistiker. Trotzdem (oder vielleicht gerade deswegen) wurde ich vor gut drei Jahren, als ich am IUNR als Dozent und Leiter der Forschungsgruppe Vegetationsökologie gefragt, ob ich nicht den Statistikteil im “Research Methods”-Modul des neuen Masterstudiengangs “Umwelt und Natürliche Resourcen” übernehmen würde. Ich habe zugesagt, obwohl ich mir der doppelten Herausforderung klar war: (1) als statistische Autodidakt Statistik zu lehren und (2) dies nicht nur für ÖkologInnen, sondern für angehende UmweltingenieurInnen im Allgemeinen zu tun, deren Interessen von Umweltbildung bis zu Umwelttechnologien reichen und die gleichermassen im naturwissenschaftlichen wie im sozialwissenschaftlichen Bereichen unterwegs sind.\nDer Kurs hat sich über die Jahre weiterentwickelt, vor allem durch konstruktiv-kritisches Feedback der Studierenden. Während nur wenige der ehemaligen TeilnehmerInnen vermutlich von sich behaupten würden, im Modul zu begeisterten Statistikfans geworden zu sein, so konnte ich doch in nachfolgenden Mastermodulen (etwa der “Summer School Biodiversity Monitoring” oder bei Präsentationen von Masterarbeiten) feststellen, dass viele das Handwerkszeug sehr solide gelernt haben und souverän anwenden konnten. Manche konnten am Ende des Masterstudium durch stetiges Learning by doing in der offenen Plattform R sogar statistische Fähigkeiten vorweisen, die deutlich über das im Kurs selbst vermittelte hinausgehen. Ja, acht halbe Kurstage sind extrem wenig, um auch nur die wichtigsten Grundlagen der Statistik zu lernen. Wenn ihr erfolgreich sein wollt, müsst ihr also aktiv mitmachen und mehr Quellen nutzen als nur unsere Inputs im Modul.\nIch hatte eigentlich nicht vor, ein Skript zum Kurs zu erstellen, obwohl das Studierende auch in den Vorjahren immer wieder gewünscht haben. Der Aufwand dafür schien mir zu gross – auch in Relation zu den Stunden, die mir für den Kurs zur Verfügung stehen. Ausserdem fand ich, dass das Lernsetting in den Vorjahren mit einer Vorlesung mit vielen Interaktionen mit den Studierenden, gefolgt von der Vorführung und Diskussion von Demo-R-Skripten und schliesslich betreuten Übungen angemessen und recht effizient war. Dann kam bekanntlich Covid-19 und im Herbstsemester 2020 war alles anders. Wir haben entschieden das “Methodenmodul” aus epidemologischen Gründen ohne physischen Kontakt zu euch durchzuführen. Ich hätte wie andere Dozierende in dieser Situation mit Screencasts arbeiten können, aber ohne die Möglichkeit, dabei auf eure Fragen direkt eingehen zu können, schien mir das wenig erfolgsversprechend. Auch den ganzen Vormittag lang online-Kurs zu halten, schien mir für euch wie für uns Dozierende unzumutbar. Insofern habe ich mich nach Diskussionen mit den anderen Beteiligten entschieden, doch ein Skript zu erstellen. Die Idee ist, dass ihr es vorgängig zu den Kurstagen lest und wir dann in einem gemeinsamen Online-Raum auf Zoom, im Sinne eines “inverted classroom” eure offenen Fragen diskutieren können und ich ggf. Punkte, die nicht alle verstanden haben noch einmal “live” erklären kann.\nDas hier vorliegende Skript ist zunächst die Verschriftlichung der Vorlesungsfolien der letzten Jahre. Aber viele Aspekte, die auf den Folien nur in Stichpunkten auftauchten, da sie im Kurs live besprochen wurden, sind jetzt eben auch ausformuliert. Nebenbei wurde natürlich manch Anderes auch noch verbessert, ergänzt und aktualisiert. Nichtsdestotrotz ist es die erste Fassung dieses Skriptes und alle Unzulänglichkeiten seien mir nachgesehen. Verbesserungsvorschläge sind jederzeit willkommen.\nWichtig ist, dass dieses Skript nicht als alleiniges Lehrmaterial gedacht ist. Genauso wichtig sind die gemeinsamen Präsenz-Lektionen mit Diskussion des theoretischen Stoffes und der Vorführung (Demo) exemplarischer R-Codes sowie die Übungen und deren Besprechung. Ich empfehle euch auch, begleitend auch andere Quellen zu nutzen, insbesondere wenn einige von euch meine Erklärungen schwer verständlich finden sollten. Welche Form der Informationsbereitstellung jemand eingängig findet, ist individuell sehr verschieden. Für Statistik 1–5 empfehle ich euch insbesondere das Lehrbuch von Crawley (2015), welches das offizielle Begleitlehrbuch zum Kurs ist. Ich werde auch nicht alle Details aus Crawley (2015) im Kurs wiederholen. In den ersten drei Durchführungen haben wir noch das Buch von Logan (2010) verwendet, das ausführlicher ist und “Kochrezepte” auch für komplexere Fälle bietet, die über das hinausgehen, was wir im Kurs behandeln können. Der Vorteil von Crawley (2015) ist, dass das Buch knapper ist und nicht nur auf biologische Fälle, sondern auf beliebige Disziplinen bezogen. Trotzdem ist Logan (2010) weiterhin eine empfehlenswerte Quelle für inferenzstatistische Methoden. Leider gibt es nach meiner Sichtung von etwa zwei Dutzend Statistikbüchern mit R, keines das gleichermassen die Inferenzstatistik und die deskriptiv-multivariate Statistik in der für den Kurs angemessenen Tiefe behandelt. Man könnte das Mammutwerk von Crawley (2013) nennen, aber trotz über 1000 Seiten sind dort die multivariat-deskriptiven Methoden nur sehr kurz (aber immerhin) behandelt und es ist eher ein Kompendium als ein Lehrbuch. Insofern werde ich für Statistik 6–8 auf andere Quellen zurückgreifen, insbesondere auf das exzellente Lehrbuch von Borcard et al. (2018), das aber weitestgehend inferenzstatistischen Methoden aussen vorlässt und die multivariat-deskriptiven aus der alleinigen Sicht von ÖkologInnen beschreibt. Zu guter Letzt möchte ich noch das Buch von Quinn & Keough (2002) empfehlen, das m. E. die ganze Bandbreite statistischer Methoden für ÖkologInnen beschreibt und hervorragend mit vielen Beispielen erklärt, aber eben aus der “Vor-R-Zeit”, mithin ohne Beispiel-Code. Da nahezu alle aus meiner Sicht empfehlenswerten aktuellen Statistikbücher auf Englisch sind, dieses Skript jedoch auf Deutsch, habe ich im Skript wichtige Fachtermini in beiden Sprachen angegeben (Englisch ist dann kursiv), um eine leichtere Verknüpfung zu schaffen.\nIm Skript wird die Theorie beginnend mit den einfachsten statistischen Verfahren (die den Masterstudierenden schon geläufig sein sollten) sukzessive aufgebaut, wobei an geeigneten Stellen wichtige Grundsätze (z.B. unabhängigkeit der Messwerte, Voraussetzungen für Tests etc.) erklärt werden, die für die Statistik insgesamt relevant sind. Die Theorie ist immer mit dem entsprechenden R-Code kombiniert, einschliesslich der Interpretation der textlichen und grafischen Ausgaben von R. Das Skript enthält nur Auszüge des R-Codes, der in Gänze im Unterricht (in der jeweils zweiten Lektion) vorgestellt und besprochen wird. Da es in diesem Kursteil um das Verständnis der Statistik geht, wurde kein grosser Aufwand auf das “Optimieren” des visuellen Outputs gelegt, welches den Code of wesentlich verlängert und den Blick vom “Eigentlichen” abgelenkt hätte."
  },
  {
    "objectID": "index.html#quellen",
    "href": "index.html#quellen",
    "title": "Statistik mit R für Umweltwissenschaftler:innen",
    "section": "Quellen",
    "text": "Quellen\n\nBorcard, D., Gillet, F. & Legendre, P. 2018. Numerical ecology with R. 2nd ed. Springer, Cham, CH: 435 pp.\nCrawley, M.J. 2013. The R book. 2nd ed. John Wiley & Sons, Chichester, UK: 1051 pp.\nCrawley, M.J. 2015. Statistics – An introduction using R. 2nd ed. John Wiley & Sons, Chichester, UK: 339 pp.\nLogan, M. 2010. Biostatistical design and analysis using R: a practical guide. Wiley-Blackwell, Chichester, UK: 546 pp.\nQuinn, G.P. & Keough, M.J. 2002. Experimental design and data analysis for biologists. Cambridge University Press, Cambridge, UK: 537 pp."
  },
  {
    "objectID": "Statistik_1.html#lernziele",
    "href": "Statistik_1.html#lernziele",
    "title": "Statistik 1",
    "section": "Lernziele",
    "text": "Lernziele\n\n\n\n\n\n\nIhr…\n\nversteht, was Statistik im Kern leistet und warum Statistik für wissenschaftliche Erkenntnis (in den meisten Disziplinen) unentbehrlich ist;\nkönnt Angaben zu p-Werten oder Signifikanzlevels kritisch würdigen;\nwisst, wann man einen t-Test und wann einen Chi-Quadrat-Test verwendet und wie man das praktisch in R durchführt; und\nhabt eine grundlegende Idee, worauf es beim Berichten statistischer Ergebnisse, insbesondere in Abbildungen ankommt."
  },
  {
    "objectID": "Statistik_1.html#warum-brauchen-wir-statistik",
    "href": "Statistik_1.html#warum-brauchen-wir-statistik",
    "title": "Statistik 1",
    "section": "Warum brauchen wir Statistik?",
    "text": "Warum brauchen wir Statistik?\n\nEin Beispiel\nIch möchte die grundlegende Notwendigkeit von Statistik mit einem fiktiven Beispiel visualisieren. Gehen wir von einer einfachen Frage aus dem Zierpflanzenbau aus:\nUnterscheiden sich zwei verschiedene Sorten (Cultivare) in der Blütengrösse?\n\n\n\n\n\nUm diese Frage zu beantworten, vermessen wir die Blüten der beiden abgebildeten Individuen:\n\nIndividuum A: 20 cm2\nIndividuum B: 12 cm2\n\nMithin wäre unsere naive Antwort auf die Eingangsfragen: Ja, die Blüten von Sorte A sind grösser als jene von B. Wir können sogar sagen, um wie viel grösser (8 cm2 oder 67 %).\nNun haben Pflanzen (wie fast alle Objekte, mit denen wir uns beschäftigen, mit Ausnahme vielleicht von Elementarteilchen) eine gewisse Variabilität:\n\n\n\n\n\nFolglich ist es sinnvoller, für die Beantwortung der Frage jeweils mehrere Individuen zu vermessen. Wir greifen nun 10 Individuen jeder Sorte heraus und erzielen folgende Messergebnisse:\n\nIndividuen A1–A10 [cm2]: 20; 19; 25; 10; 8; 15; 13; 18; 11; 14\nIndividuen B1–B10 [cm2]: 12; 15; 16; 7; 8; 10; 12; 11; 13; 10\n\nWir erhalten für A einen Mittelwert von 15.3 cm2 und für B einen Mittelwert von 11.4 cm2 (was wir einfach in Excel ausrechnen können). Wir schliessen daher, dass die Blüten von A im Mittel 3.9 cm2 grösser sind als jene von B.\nWir könnten uns also zufrieden zurücklehnen und unserem Ergebnis, das wir mit etwas deskriptiver Statistik (Mittelwerte) erzielt haben, vertrauen. Wo liegt der Haken? Wir haben nicht alle existierenden Individuen der Sorten A und B vermessen (die “Grundgesamtheit”), sondern nur eine Stichprobe von jeweils 10 Individuen. Nun könnte es sein, dass KollegInnen von uns die gleiche Untersuchung mit jeweils anderen Stichproben von je 10 Individuen durchgeführt haben, etwa folgendermassen (mit ihren jeweiligen Schlussfolgerungen):\n\nMess-Serie 1:\n\nIndividuen A1–A10 [cm2]: 20; 19; 25; 10; 8; 15; 13; 18; 11; 14\nIndividuen B1–B10 [cm2]: 12; 15; 16; 7; 8; 10; 12; 11; 13; 10\nErgebnis: A = 15.3; B = 11.4; A – B = 3.9 cm2 → A ist grösser als B\n\nMess-Serie 2:\n\nIndividuen A1–A10 [cm2]: 20; 19; 25; 10; 8; 15; 13; 18; 11; 14\nIndividuen B1–B10 [cm2]: 12; 15; 16; 7; 8; 10; 12; 11; 13; 10\nErgebnis: A = 12.5; B = 11.3; A – B = 1.2 cm2 → A ist (wenig) grösser als B\n\nMess-Serie 3:\n\nIndividuen A1–A10 [cm2]: 20; 19; 25; 10; 8; 15; 13; 18; 11; 14\nIndividuen B1–B10 [cm2]: 12; 15; 16; 7; 8; 10; 12; 11; 13; 10\nErgebnis: A = 11.0; B = 11.0; A – B = 0.0 cm2 → A ist gleich gross wie B\n\nMess-Serie 4:\n\nIndividuen A1–A10 [cm2]: 20; 19; 25; 10; 8; 15; 13; 18; 11; 14\nIndividuen B1–B10 [cm2]: 12; 15; 16; 16; 14; 10; 12; 11; 13; 10\nErgebnis: A = 11.0; B = 12.9; A – B = – 1.9 cm2 → A ist kleiner als B\n\n\nWer hat nun Recht? Um das zu beantworten, benötigen wir die schliessende Statistik (Inferenzstatistik).\n\n\nFazit\n\nIn der Regel wollen wir nicht wissen, ob ein einzelnes Individuum der Sorte A sich von einem einzelnen Individuum der Sorte B unterscheidet.\nMeist interessiert uns, ob sich die Sorte A als solche von der Sorte B unterscheidet.\nDa es in der Regel nicht möglich ist, sämtliche existierenden Individuen beider Sorten (Grundgesamtheiten; engl. populations) zu vermessen, vermessen wir die Individuen in zwei Stichproben (engl. samples).\nDie Inferenzstatistik sagt uns dann, wie wahrscheinlich ein festgestellter Unterschied in den Mittelwerten der Stichproben einem tatsächlichen Unterschied in den Mittelwerten der Grundgesamtheiten entspricht."
  },
  {
    "objectID": "Statistik_1.html#warum-mit-r",
    "href": "Statistik_1.html#warum-mit-r",
    "title": "Statistik 1",
    "section": "Warum mit R?",
    "text": "Warum mit R?\n\nZugegeben: wir haben euch nicht gefragt…\n\nWas spricht dagegen?\nAuf den ersten Blick mag aus eurer Sicht ja einiges dagegensprechen\n\nkeine GUI (grafische Benutzeroberfläche) zum Klicken\nauf Englisch\nschwerer zu erlernen\n\n\n\nWas spricht dafür?\n\nR ist kostenlos & open source (unabhängig von teuren Lizenzen)\nR ist extrem leistungsfähig und immer up-to-date (da Tausende “ehrenamtlich” mitprogrammieren)\nR ist nah an den speziellen Bedürfnissen der einzelnen Disziplinen (durch zahlreiche spezielle Packages)\nR “zwingt” die Benutzenden dazu, ihr statistisches Vorgehen zu durchdenken (was zu besseren Ergebnissen führt)\nR gewährleistet eine sehr gute Dokumentation des eigenen Vorgehens (“Reproduzierbarkeit”), da der geschriebene R Code anders als eine Klickabfolge in einem kommerziellen Statistikprogramm mit GUI eingesehen und erneut durchgeführt werden kann\nR ist effizient, da man Code, den man einmal entwickelt hat, immer wieder verwenden bzw. für neue Projekte anpassen kann\nFür R gibt es umfangreiche Hilfe im Internet (googlen, spezielle Foren,…)\n\n\n\nFazit\nDer Kursleiter (J.D.) hat Statistik nicht in seinem Studium gelernt und es sich später im Laufe seiner Forscherlaufbahn mühsam sich selbst beigebracht. Damals gab es noch kein R. Dafür gab es teure kommerzielle Statistikprogramme wie SPSS und STATISTICA, durch die man sich mit einer grafischen Benutzeroberfläche durchklicken konnte und am Ende ein Ergebnis bekam. Nicht immer war ganz klar, was das Programm da gerechnet hatte, aber immerhin bekam man mit relativ wenigen Klicks ein numerisches Ergebnis oder eine Abbildung (oft allerdings in bescheidenem Layout) heraus. Häufig musste man aber erleben, dass das gewünschte statistische Verfahren im jeweiligen Programm in der gewünschten Version nicht implementiert war oder ein teures Zusatzpaket nötig gewesen wäre, das die eigene Universität nicht erworben hatte. Und wenn man dann an eine andere Universität wechselte, musste man oft feststellen, dass dort ein anderes Statistikprogramm erworben und genutzt wurde, für das man viele Dinge umlernen musste. Ganz zu schweigen von Zeiten ausserhalb einer Hochschule, wenn man keinen Zugriff auf ein kommerzielles Statistikprogramm hatte.\nAus dieser Sicht könnt ihr euch also glücklich schätzen, dass es heute R gibt und so leistungsfähig ist wie nie zuvor und auch dass das IUNR in der Ausbildung im Bachelor- und Masterlevel konsequent auf R setzt. Während es auf den ersten Blick vielleicht schwieriger erscheinen mag als die Benutzung von SPSS oder STATISTICA, bin ich überzeugt, dass ein Statistikkurs mit R euch bei gleichem Aufwand ein anderes Verständnislevel für Statistik ermöglichen wird als es Statistikkurse zu meiner Studienzeit taten. Nebenher bekommt ihr noch ein implizites Verständnis wie Algorithmen funktionieren, auch nicht ganz unwichtig in einer zunehmend digitalen Welt."
  },
  {
    "objectID": "Statistik_1.html#die-rolle-von-hypothesen-in-der-wissenschaft",
    "href": "Statistik_1.html#die-rolle-von-hypothesen-in-der-wissenschaft",
    "title": "Statistik 1",
    "section": "Die Rolle von Hypothesen in der Wissenschaft",
    "text": "Die Rolle von Hypothesen in der Wissenschaft\n\nRekapitulation\nIm Methodenmodul und sicher auch in euren vorausgehenden Studiengängen habt ihr euch bereits mit Hypothesen beschäftigt. Daher beginnen wir mit einem Arbeitsauftrag (allein oder im Austausch mit KommilitonInnen):\n\n\n\n\n\n\nArbeitsauftrag\n\n\n\nFormuliert jeweils in einem Satz die folgenden Punkte:\n\nEine beispielhafte Aussage, die den Ansprüchen an eine Hypothese genügt\nEine beispielhafte Aussage, die keine Hypothese ist\n\n\n\n\n\nWas ist eine Hypothese?\nEs gibt in der Literatur wie fast immer in der Wissenschaft verschiedene Formulierungen. Ich schlage die folgende vor:\n\n\n\n\n\n\nEine Hypothese ist eine aus einer allgemeinen Theorie abgeleitete Vorhersage für eine spezifische Situation.\n\n\n\nLeider wird der Begriff “Hypothese” heutzutage in der Wissenschaft “inflationär” und aus meiner Sicht sogar häufig falsch verwendet.\nZweifelhaft sind “ad hoc”-Hypothesen auf der Basis einer Vorabuntersuchung bzw. eines “Bauchgefühls”, aber ohne eine Erklärung des für das vorhergesagte Ergebnis verantwortlichen Mechanismus (also letztlich ohne Theorie dahinter). Wissenschaftstheoretisch sollte man nie dieselben Daten zum Aufstellen und zum Testen einer Hypothese verwenden!\nGänzlich falsch sind angebliche “Hypothesen”, die nachträglich aus den schon erzielten Ergebnissen abgeleitet werden.\nWarum findet man in der wissenschaftlichen Literatur wie auch in studentischen Arbeiten so viele “Hypothesen”, die wissenschaftstheoretisch dem Konzept einer Hypothese nicht gerecht werden? Der Grund dürfte darin liegen, dass viele von der Annahme geleitet werden, dass nur eine hypthesentestende Forschung eine gute/richtige Forschung ist. Tatsächlich ist aber hypothesengenierende Forschung genauso wichtig und richtig wie hypothesentestende Forschung. Es gilt also:\n\n\n\n\n\n\nWenn das Vorwissen nicht für eine plausibel begründete Hypothese ausreicht (hypothesentestende Forschung), formuliert man das Forschungsthema korrekterweise besser als offene Frage (hypothesengenerierende Forschung).\n\n\n\nDabei können offene Fragen meist mit (fast) den gleichen statistischen Verfahren addressiert werden wie Hypothesen. Allerdings sollten Hypothesen konkret sein, also nicht “A unterscheidet sich von B”, sondern entweder “A ist grösser als B” oder “A ist kleiner als B”. Hier würde also im Fall einer Hypothese ein einseitiger Test, im Fall einer offenen Frage ein zweiseitiger Test zur Anwendung kommen. Dazu aber später mehr.\n\n\nWissenschaftliches Arbeiten (in a nutshell)\nWenn wir modernes wissenschaftliches Arbeiten ganz knapp visualisieren, ergibt sich folgendes Bild:\n\n\n\n\n\nBei den ersten Schritten von den Beobachtungen bis zur Spekulation über die Musterursachen handelt es sich um hypothesengenerierende Forschung. Erst wenn man regelmässig, ähnliche Befunde hat, macht das Formulieren enier echten Hypothese Sinn, die nicht nur das gefundene Muster vorhersagt, sondern auch einen Mechanismus bereithält, der erklärt, wie es zustande gekommen ist. Eine solche Hypothese kann dann in einer neuen Untersuchung (mit neuen Daten!) getestet werden, die spezifisch darauf ausgelegt ist, alternative Erklärungsmöglichkeiten auszuschliessen (“Experiment”). Hypothesengenierende und hypothesentestende Forschung sind im modernen Forschungsablauf also beide gleichermassen nötig, aber in der Regel getrennt voneinander.\nIn einer Forschungsarbeit, die für das Testen einer zuvor in anderen Arbeiten erarbeiteten Hypothese, entwickelt wurde (“Experiment”), kann das Ergebnis entweder eine Bestätigung oder eine Falsifizierung sein. Wichtig ist, dass eine einmalige Bestätigung keine Verifizierung einer Hypothese ist, während eine einmalige Falsifizierung zur Widerlegung genügt. Eine Verifizierung einer Hypothese in einem absoluten Sinn ist grundsätzlich nicht möglich, absolute Wahrheit gibt es in der Wissenschaft nicht! Wenn man jedoch eine Hypothese mit immer neuen “Experimenten” unter immer neuen Rahmenbedingungen “herausfordert” und sie dabei nie falsifiziert wird, dann wird aus einer einfachen Hypothese zunehmend gesichertes Wissen. Wenn man dagegen eine Hypothese widerlegt hat, muss man zurückgehen. Die vorgeschlagene Erklärung für das gefundene Muster oder sogar das Muster an sich hat sich als nicht korrekt/nicht allgemeingültig herausgestellt. Man muss sich also einen anderen Mechanismus/eine andere Hypothese ausdenken und diese erneut testen. Dies geschieht dann nicht in derselben, sondern in einer folgenden wissenschaftlichen Arbeit.\nMit diesem Wissen über den Ablauf von wissenschaftlicher Erkenntnis und der Rolle des Hypothesentestens dabei habe ich noch eine Frage, zu der ihr euch bis zum Kurstag Gedanken machen solltet:\n\n\n\n\n\n\nFrage\n\n\n\nProfitiert Wissenschaft mehr von der Bestätigung oder von der Falsifizierung von Hypothesen? (Bitte begründet eure Antwort!)"
  },
  {
    "objectID": "Statistik_1.html#die-rolle-der-statistik-beim-hypothesengenerieren-und--testen",
    "href": "Statistik_1.html#die-rolle-der-statistik-beim-hypothesengenerieren-und--testen",
    "title": "Statistik 1",
    "section": "Die Rolle der Statistik beim Hypothesengenerieren und -testen",
    "text": "Die Rolle der Statistik beim Hypothesengenerieren und -testen\nWir haben gesehen, dass Hypothesen zentral für die moderne Wissenschaft sind, sowohl ihr Generieren als auch ihr Test. Doch welche Rolle spielt die Statistik dabei?\n\n\n\n\n\n\nStatistiche Verfahren, die implizit oder explizit Hypothesen testen, bezeichnet man als Inferenzstatistik (schliessende Statistik) – im Gegensatz zur deskriptiven Statistik.\n\n\n\n\nVon der Hypothese zur Nullhypothese…\nDie Herausforderung ist nun aber, wie oben gesehen, dass man eine Hypothese (Ha) (auch Forschungshypothese oder alternative Hypothese genannt) nicht verifizieren kann, sondern nur falsifizieren. In der Statistik behilft man sich daher mit einem Trick, der sogenannten Nullhypothese (H0). Die Nullhypothese ist die Negation der Hypothese, d. h. die Summe aller möglichen Beobachtungen, die mit der Hypothese nicht im Einklang sind. Wenn man nun die Nullhypothese falsifiziert, kann man indirekt die Hypothese bestätigen.\nIn unserem Beispiel von oben:\n\nHypothese (HA): Sorte A und Sorte B unterscheiden sich in ihrer Blütengrösse\nNullhypothese (HA0): Sorte A und Sorte B haben die gleiche Blütengrösse\n\nDas ist formal korrekt, wissenschaftlich ist die Forschungshypothese aber wenig überzeugend, weilschwerlich ein Mechanismus vorstellbar ist, der in Sorte B sowohl kleinere als auch grössere, nur keine gleich grossen Blüten hervorbringt. Insofern wäre das folgende Paar sinnvoller:\n\nHypothese (HB): Sorte A hat grössere Blüten als Sorte B\nNullhypothese (HB0): Sorte A hat kleinere oder gleich grosse Blüten wie Sorte B\n\nDie erste Forschungshypothese (HA) ist eine ungerichtete Hypothese und entspricht dem, was man in hypothesengenerierender Forschung implizit macht (wenn man also offene Fragen, aber keine konkreten Hypothesen hat). In diesem Fall wäre die zugehörige Forschungsfrage: “Unterscheiden sich die Sorten A und B in ihren Blütengrössen?”. Die zweite Forschungshypothese (HB) ist dagegen gerichtet und wäre für hypothesentestenden Forschung adäquat. In der hypothesentestenden Forschung sollten wir auch eine Begründung/einen Mechanismus anführen, der vermutlich zu dem vorhergesagten Ergebnis führt, etwa dass die Sorte A polyploid ist. Dies gehört zur Begründung der Forschungshypothese, aber ist nicht Bestandteil der Forschungshypothese.\n\n\nEinschub: Wichtige Termini in der Statistik\nBis hierher sind uns schon einige wichtige statistische Begriffe (wie Stichprobe und Grundgesamtheit) begegnet, deshalb sollen sie hier samt ihren englischen Pendants noch einmal rekapituliert werden:\n\n\n\n\n\n\n\n\n\nDeutscher Begriff\nEnglischer Begriff\nDefinition\nBeispiel(e)\n\n\n\n\nBeobachtung\nObservation\nexperimentelle bzw. Beobachtungseinheit\nPflanzenindividuum\n\n\nStichprobe\nSample\nalle beprobten Einheiten\ndie 20 untersuchten Pflanzenindividuen\n\n\nGrundgesamtheit\nPopulation\nGesamtheit aller Einheiten, über die eine Aussage getroffen werden soll\nalle Individuen der beiden Sorten\n\n\nMessung\nMeasurement\neinzelne erhobene Information\nBlütengrösse eines Individuums\n\n\nVariable\nVariable\nKategorie der erhobenen Information\nBlütengrösse, Sorte\n\n\n\nDer englische Begriff population führt oft zu Verwirrung, da er in der Statistik etwas anderes meint als in der Biologie. Population ist schlicht die Grundgesamtheit, die in seltenen Fällen einer biologischen Population entspricht, in den meisten Fällen aber nicht (etwa population of chairs). Auch Messung/measurement wird in der Statistik weiter als in der Allgemeinsprache verwendet, d. h. auch für Zählungen oder Erhebung von kategorialen Variablen.\n\n\nEinschub: Parameter vs. Prüfgrössen\nWenn wir in Inferenzstatistik betreiben, also von einer Stichprobe auf die Grundgesamtheit schliessen wollen, müssen wir zudem zwischen Parametern und Prüfgrössen unterscheiden. Unter Parameter (parameter) wird eine Grösse der deskriptiven Statistik für eine betimmte Variable in der Grundgesamtheit verstanden, über die wir eine Aussage treffen wollen, die wir aber nicht kennen. Dagegen ist eine Prüfgrösse (statistic) eine aus den Messungen der Variablen in der Stichprobe berechnete Grösse, die zur Schätzung des Parameters dient. Etwas verwirrend ist, dass stastitic (Prüfgrösse) und statistics (die Statistik als Fach) fast gleich lauten. Oft wird die Konvention verwendet, dass die Prüfgrössen mit kursiven lateinischen Buchstaben (z. B. \\(s^2\\)) und die korrespondierenden Parameter mit den äquivalenten griechischen Buchstaben (z. B. \\(\\sigma^2\\)) bezeichnet werden (siehe die folgende Tabelle):\n\n\n\n(aus Quinn & Keough 2002)\n\n\n\n\nStatistische Implementierung des Hypothesentestens (am Beispiel des t-Tests)\nWie lässt sich das Hypothesentesten nun mathematisch und statistisch umsetzen. Wir bleiben bei unserer offenen Forschungsfrage “Unterscheiden sich die Sorten A und B in ihren Blütengrössen?”, woraus sich die Forschungshypothese “Sorten A und B unterscheiden sich in ihren Blütengrössen” ergibt. Mit dem Mittelwert µ der Variablen (Blütengrösse) in den jeweiligen Grundgesamtheiten (A und B) lassen sich Forschungshypothese und Nullhypothese mathematisch wie folgt formulieren:\n\nHa: \\(\\mu_A \\neq \\mu_B\\)\nH0: \\(\\mu_A = \\mu_B\\) oder \\(\\mu_A - \\mu_B = 0\\)\n\nFür die Überprüfung der H0 gibt es eine Teststatistik (Prüfgrösse) den t-Wert, der wie folgt definiert ist:\n\\[\nt = \\frac{\\left( {\\overline{y}}_{A} - {\\overline{y}}_{B} \\right) - (\\mu_{A} - \\mu_{B})}{s_{{\\overline{y}}_{A} - {\\overline{y}}_{B}}}\n\\]\nDa für die \\(H_0\\) gilt \\(\\mu_A - \\mu_B = 0\\), lässt sich das vereinfachen zu:\n\\[\nt = \\frac{\\left( {\\overline{y}}_{A} - {\\overline{y}}_{B} \\right)}{s_{{\\overline{y}}_{A} - {\\overline{y}}_{B}}}\n\\]\nDie Prüfgrösse t ist also die Differenz der beiden Mittelwerte dividiert durch den Standardfehler der Differenz der beiden Mittelwerte. Wenn also die Differenz der Mittelwerte gross und/oder der Standardfehler dieser Differenz klein ist, so ist t weit von Null entfernt.\nWas sagt uns der berechnete t-Wert nun? Um daraus etwas schlussfolgern zu können, müssen wir ihn mit der theoretischen t-Verteilung vergleichen. Für diese gilt:\n\nSie ist symmetrisch, mit einem Maximum bei 0.\nDer genaue Kurvenverlauf variiert in Abhängigkeit von den Freiheitsgraden (degrees of freedom = df). Bei vielen Freiheitsgraden, d. h. einer grossen Stichprobengrösse (mehr dazu, wie sich die Stichprobenzahl in Freiheitsgrade übersetzt, folgt später), nähert sicht die t-Verteilung einer Normalverteilung (auch z-Verteilung genannt).\n\nDie allgemeine Konvention in der Statistik ist, dass die Nullhypothese dann verworfen wird, wenn die berechnete Prüfgrösse extremer ist als 95 % aller möglichen Werte bei der gegebenen Stichprobengrösse. Beim t-Test fragt man also, ob der berechnete t-Wert extremer ist als 95 % aller t-Werte der der Stichprobengrösse entsprechenden t-Verteilung. Da unsere Hypothese ungerichtet ist (also ist verschieden und nicht ist grösser/ist kleiner), benötigen wir einen zweiseitigen t-Test. Dieser bestimmt die “kritischen” t-Werte (tc), indem auf beiden Seiten quasi 2.5 % der Fläche des Integrals unter der Wahrscheinlichkeitsverteilung abgeschnitten werden, wie die folgende Abbildung veranschaulicht:\n\n\n\n(aus Quinn & Keough 2002)\n\n\nWenn also der berechnete t-Wert &gt; tc (oder &lt; –tc) ist, dann sind wir hinreichend sicher, dass sich die Mittelwerte nicht nur in der Stichprobe, sondern auch in der Grundgesamtheit unterscheiden.\n\n\nFehler I. und II. Art\nWichtig ist, dass es in der physischen Realität nie eine absolute Sicherheit gibt. Wenn wir also wir feststellen, dass die Wahrscheinlichkeit, dass die Nullhypothese zutrifft (oder präziser: dass das vorliegende Ergebnis oder ein extremeres bei Zutreffen der Nullhypothese aufgetreten wäre) kleiner als 5 % ist, gibt es eben doch Fälle gibt, in denen wir fälschlich die Nullhypothese verwerfen, d. h. das Vorliegend eines Effektes bejahen, obwohl er in der Realität (d. h. der Grundgesamtheit) nicht auftritt. Das bezeichnet man als Typ I-Fehler. Umgekehrt kann es aber auch passieren, dass man die Nullhypothese aufgrund des statistischen Tests beibehält, also einen Effekt nicht nachweist, obwohl er in der Realität existiert (Typ II-Fehler). Diese beiden Phänomene sind in der folgenden beiden Abbildungen visualisiert:\n\n\n\n\n\n\n\n\n\n\n\n(aus Quinn & Keough 2002)\n\n\n\n\n\nWie man der zweiten Visualisierung entnehmen kann, steigt die Wahrscheinlichkeit eines Typ II-Fehlers, je weiter man die akzeptierte Wahrscheinlichkeit eines Typ I-Fehlers reduziert. In der Statistik wir im Allgemeinen sehr viel stärker auf die Minimierung von Typ I-Fehlern fokusiert, d. h. man will vermeiden, dass man fälschlich einen Effekt behauptet, der in Realität nicht existiert, während es als weniger problematisch angesehen wird, einen vorhandenen, aber dann sehr schwachen Effekt, nicht nachgewiesen zu haben.\n\n\np-Werte und Signifikanzniveaus\nSignifikanzniveaus und p-Werte sind zentrale Termini in der am weitesten verbreiteten inferenz-statistischen Schule, der frequentist statistics (“Frequentistische Statistik”, aber ich habe den Begriff noch nie im Deutschen gehört). Deren Grundideen sind:\n\nDie beobachteten Werte werden als eine Beobachtung unter vielen möglichen Beobachtungen interpretiert, die zusammen eine Häufigkeitsverteilung ergeben.\nEs wird eine einzige wahre Beschreibung der Realität angenommen, der man sich mit bestimmten Irrtumswahrscheinlichkeiten annähern kann\n\n\n\n\n\n\n\nIn der frequentist statistics, sind die p-Werte das zentrale “Gütemass”. Als p-Wert bezeichnet man dabei die berechnete Wahrscheinlichkeit eines Typ I-Fehlers. Der p-Wert bezeichnet also die Wahrscheinlichkeit, dass man aufgrund des statistischen Tests einen Zusammenhang feststellt, ohne dass dieser in Realität existiert.\n\n\n\nAls statistisch signifikant bezeichnet man Ergebnisse, die unter einem bestimmten p-Wert liegen. Diese Schwellenwerte sind Konventionen und nicht “gottgegeben”. Traditionell werden drei Signifikanzniveaus verwendet (wozu R noch ein viertes hinzugefügt hat, das man mit “marginal signifikant” bezeichnen könnte), die wie folgt notiert werden:\n\n\n\n\n\n\n\n\nNotation\nBedeutung\n\n\n\n\n\n***\np &lt; 0.001\nhöchst signifikant; highly significant\n\n\n**\np &lt; 0.01\nhoch signifikant; very significant\n\n\n*\np &lt; 0.05\nsignifikant; significant\n\n\n.\np &lt; 0.1\nmarginal signifikant; marginally significant\n\n\n\nDie Schwellenwerte der Signifikanzniveaus (d. h. Schwellenwerte für akzeptierte Typ I-Fehler) werden auch mit α bezeichnet. Was man in einer Arbeit als signifikant betrachtet, sollte man vor Beginn der Untersuchung festlegen und im Methodenteil schreiben (“als Signifikanzschwelle verwenden wir \\(\\alpha = 0.05\\)” oder “als signifikant sehen wir Ergebnisse mit \\(p &lt; 0.05\\) an”). Es bietet sich normalerweise an, bei der allgemeinen Konvention von \\(\\alpha = 0.05\\) zu bleiben, es sei denn es sprechen spezifische Gründe dagegen. Ein Grund könnte sein, dass die Verwerfung der Nullhypothese/Annahme der Forschunshypothese schwerwiegende Folgen hätte und man sich daher besonders sicher sein will.\nDa sich ober-und unterhalb der genannten Schwellen nichts Fundamentales ändert, sollte man grundsätzlich die exakten p-Werte mit drei Nachkommastellen (z.B. “\\(p = 0.038\\)” bzw. wenn noch niedriger als “\\(p &lt; 0.001\\)”) angeben. Zur besseren Lesbarkeit können zusätzlich die korrespondierenden Signifikanzniveaus angegeben werden.\nEs ist wichtig, sich bewusst zu sein, dass statistisch signifikant nicht gleichbedeutend ist mit biologisch bzw. sozialwissenschaftlich bedeutsam. Ein Effekt kann statistisch hochsignikant sein (wg. grosser Stichprobengrösse) und trotzdem inhaltlich bedeutungslos (da die Effektgrösse minimal ist). Umgekehrt kann ein inhaltlich bedeutsamer Effekt evtl. nicht statistisch signifikant nachgewiesen werden, wenn man extrem wenige Replikate hatte.\nMit dem Kriterium “statistische Signifikanz”/p-Wert trennen wir unsere Ergebnisse in einem ersten Schritt in jene, die wir für belastbar halten und jene, die mit grosser Wahrscheinlichkeit “zufällig” (“Rauschen in den Daten”, Messungenauigkeit, etc.) zustande gekommen sind. Bei den belastbaren müssen wir dann immer noch ihre Relevanz (also die Effektstärke) beurteilen."
  },
  {
    "objectID": "Statistik_1.html#t-test-für-eine-metrische-variable-im-vergleich-von-zwei-gruppen",
    "href": "Statistik_1.html#t-test-für-eine-metrische-variable-im-vergleich-von-zwei-gruppen",
    "title": "Statistik 1",
    "section": "t-Test (für eine metrische Variable im Vergleich von zwei Gruppen)",
    "text": "t-Test (für eine metrische Variable im Vergleich von zwei Gruppen)\nBei den beiden vorausgehenden einfachen Tests haben wir jeweils binäre Daten bezüglich ihrer Häufigkeitsverteilung analysiert. Oft haben wir aber metrische Variablen als abhängige Grösse, etwa in unserem Blumenbeispiel:\n\n\n\nSorte A\nSorte B\n\n\n\n\n20\n12\n\n\n19\n15\n\n\n25\n16\n\n\n10\n7\n\n\n8\n8\n\n\n15\n10\n\n\n13\n12\n\n\n28\n11\n\n\n11\n13\n\n\n14\n10\n\n\n\nH0: Die beiden Sorten unterscheiden sich nicht in der Blütengrösse.\n\nStudents und Welch t-Test\nAls statistisches Verfahren kommt Students t-Test für zwei unabhängige Stichproben zum Einsatz (“Student” ist das Pseudonym für William Sealy Gosset, dem Erfinder des Tests, dessen Arbeitsvertrag in der Privatwirtschaft das Publizieren von Ergebnissen verbot).\n\\[\nt = \\frac{\\bar{X}_1-\\bar{X}_2}{s_p\\times \\sqrt{\\frac{1}{n_1}+\\frac{1}{n_2}}}\n\\]\nWobei \\(s_p\\) der gepoolten Varianz entspricht:\n\\[\ns_p = \\sqrt{\\frac{(n_1-1)s^{2}_{X_{1}}+(n_2-1)s^{2}_{X_{2}}))}{n_1+n_2-2}}\n\\]\nDer berechnete t-Wert wird mit der t-Verteilung für (n1 – 1) + (n2 – 1) Freiheitsgraden verglichen. Der klassische t-Test setzt Normalverteilung und gleiche Varianzen voraus:\nt.test(blume$a,blume$b, var.equal=T)\nWenn Varianzgleichheit nicht gegeben ist, verwendet man Welch’ t-Test. Dieser approximiert die Freiheitsgrade mit der Welch-Satterthwaite-Gleichung. Er setzt weiterhin Normalverteilung voraus, benötigt aber keine gleichen Varianzen. Welch’ t-Test kann/sollte also immer verwendet werden, wenn keine vorherigen Tests auf Varianzgleichheit durchgeführt werden und ist daher Standard (default) in R:\n\\[\nt = \\frac{\\bar{X}_1 - \\bar{X}_2}{s\\frac{}{\\Delta}}\n\\]\nWobei\n\\[\ns\\frac{}{\\Delta} = \\sqrt{\\frac{s^2_1}{n_1}+\\frac{s^2_2}{n_2}}\n\\]\nt.test(blume$a,blume$b, var.equal=F)\nt.test(blume$a,blume$b) \n\n\nEin- und zweiseitiger t-Test\nBislang war unsere Hypothese, dass irgendein Unterschied vorliegt (was, wie oben dargelegt, keine adäquate Forschungshypothese ist, sondern die implizite Hypothese, wenn man eine offene Frage formuliert, aber keine klare Theorie hat). Wenn es eine Theorie gibt, aus der sich eine klare Vorhersage treffen lässt, so enthält diese normalerweise auch eine Aussage über die Richtung des Effekts, also ob die Blüten von A grösser als jene von B sind oder umgekehrt. Dann verwendet man einen einseitigen t-Test, denn man je nach Richtung der Hypothese mit greater oder less spezifizieren muss. Bildlich gesprochen werden beim gängigen Signifikanzniveau von α = 0.05 beim beidseitigen t-Test je 2.5 % der Integralfläche links und rechts “abgeschnitten”, beim einseitigen t-Test dagegen 5 % auf einer Seite. Wenn der berechnete t-Wert in einem der abgeschnittenen “Dreiecke” liegt, ist das Ergebnis signifikant.\n\n\n\n(aus Quinn & Keough 2002)\n\n\nt.test(blume$a,blume$b)                        # zweiseitig\nt.test(blume$a,blume$b, alternative=\"greater\") # einseitig\nt.test(blume$a,blume$b, alternative=\"less\")    # einseitig\n\n\nGepaarter und ungepaarter t-Test\nBislang haben wir angenommen, dass die Individuen der beiden Sorten unabhängig voneinander jeweils zufällig ausgewählt wurden. Dann ist ein ungepaarter t-Test (default-Einstellung in R) richtig. Wenn jedoch je zwei Messwerte zusammengehören, etwa wenn je eine Pflanze der Sorten A und B gemeinsam in einem Topf wuchsen , so kommt ein gepaarter t-Test zur Anwendung. Da dieser mehr “Informationen” zur Verfügung hat, hat er mehr statistische “power”, wird i. d. R. also zu stärker signifikanten Ergebnissen führen:\nt.test(blume$a,blume$b, paired=T) # gepaarter t-Test"
  },
  {
    "objectID": "Statistik_1.html#binomial-test-für-die-häufigkeitsverteilung-einer-binomialen-variablen",
    "href": "Statistik_1.html#binomial-test-für-die-häufigkeitsverteilung-einer-binomialen-variablen",
    "title": "Statistik 1",
    "section": "Binomial-Test (für die Häufigkeitsverteilung einer binomialen Variablen)",
    "text": "Binomial-Test (für die Häufigkeitsverteilung einer binomialen Variablen)\nDer Binomial-Test ist eines der einfachsten statistischen Verfahren überhaupt. Er testet, ob die Verteilung einer binären Variable von einer Zufallsverteilung abweicht. Eine binomiale (binäre) Variable ist eine, die zwei mögliche Zustände hat, etwa lebend/tot, männlich/weiblich oder besser/schlechter. Wenn das Ergebnis zufällig wäre, müssten in der Stichprobe beide Ausprägungen ungefähr gleich häufig vertreten sein. Folglich testet der Binomialtest, wie wahrscheinlich es ist, dass die vorgefundene Häufigkeitsverteilung in der Stichprobe zustande gekommen wäre, wenn beide Zustände gleich häufig sind. Wenn diese Wahrscheinlichkeit &lt; 0.05 ist, nimmt man in der Statistik gewöhnlich an, dass der Unterschied in der Stichprobe einem realen Unterschied in der Grundgesamtheit ist.\nBetrachten wir den Frauenanteil im schweizerischen Nationalrat als Beispiel. Im Jahr 2019 waren 84 von 200 Mitgliedern weiblich (42%). Nehmen wir in guter Näherung an, dass im Stimmvolk das Geschlechterverhältnis 1:1 ist: Kann die Abweichung von 50 % unter den Mitgliedern noch durch Zufall erklärt werden oder deutet das auf eine “Bevorzugung” von Männern bei der Kandidat:innenaufstellung und im Wahlvorgang hin. Die Antwort liefert der Binomialtest, dem man die Zahl der “Erfolge” (weiblich: 82) und die Stichprobengrösse (200) übergeben muss:\nbinom.test(82,200)\n     Exact binomial test\n    \ndata: 84 and 200\nnumber of successes = 84, number of trials = 200, p-value = 0.02813\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.3507439 0.4916638\nsample estimates:\nprobability of success\n                  0.42\nDer Unterschied ist also signifikant (p &lt; 0.05), wir können die Nullhypothese (“keine Bevorzugung von Männern”) also verwerfen. Der Output sagt uns auch noch, dass ohne Bevorzugung / Benachteiligung eines Geschlechts der gegenwärtige Frauenanteil im Nationalrat nur zustande hätte kommen können, wenn der Frauenanteil im Stimmvolk zwischen 35 % und 49 % läge. Da dieser Bereich 50 % (also den der Nullhypothese ensprechenden Wert) nicht einschliesst, ist es logisch, dass diese verworfen wird. Der Test ist “symmetrisch”: Wir können also statt der Anzahl der weiblichen Nationalratsmitglieder auch jene der männlichen eingeben und bekommen den gleichen p-Wert\nbinom.test(116,200)\n    Exact binomial test\n\ndata: 116 and 200\nnumber of successes = 116, number of trials = 200, p-value = 0.02813\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.5083362 0.6492561\nsample estimates:\nprobability of success\n                  0.58"
  },
  {
    "objectID": "Statistik_1.html#chi-quadrat--bzw.-fishers-test-für-die-assoziation-zweier-binomialer-variablen",
    "href": "Statistik_1.html#chi-quadrat--bzw.-fishers-test-für-die-assoziation-zweier-binomialer-variablen",
    "title": "Statistik 1",
    "section": "Chi-Quadrat- bzw. Fishers Test (für die Assoziation zweier binomialer Variablen)",
    "text": "Chi-Quadrat- bzw. Fishers Test (für die Assoziation zweier binomialer Variablen)\nDie Frage beim Assoziationstest ist eine ähnliche wie beim Binomialtest. Wiederum geht es um binomiale Variablen, dieses Mal aber nicht um eine einzige, sondern um zwei an denselben Objekten erhobene Variablen, deren Zusammenhang man wissen will.\nIm folgenden Beispiel wollen wir wissen, ob die Augenfarbe und die Haarfarbe von Personen miteinander zusammenhängen. Die einfachste Form des Assoziationstests setzt zwei binomiale/binäre Variablen voraus, wir müssen also z. B. grüne Augen ausschliessen oder mit einer der beiden anderen Augenfarben zusammenfassen. Unsere Beobachtungsergebnisse von 114 Personen könnten wie folgt aussehen:\n\n\n\n\nBlaue Augen\nBraune Augen\n\n\n\n\nHelle Haare\n38\n11\n\n\nDunkle Haare\n14\n51\n\n\n\nSind diese Werte so erwartbar unter der Nullhypothese, dass Augenfarbe und Haarfarbe unabhängig voneinander sind? Anders als beim Binomialtest oben ist die Nullhypothese jedoch nicht die Gleichverteilung aller Merkmale bzw. Merkmalskombinationen. Vielmehr gehen wir von der gegebenen Häufigkeit der vier Einzelmerkmale aus. Wir müssen also berechnen, mit welcher Wahrscheinlichkeit die Kombination blaue Augen – helle Haare unter den 114 ProbantInnen auftreten sollte, wenn beide Merkmale unabhängig voneinander sind. Das geht folgendermassen:\n\n\n\n\n\n\n\n\n\n\nBlaue Augen\nBraune Augen\nZeilen Total \n\n\n\n\nHelle Haare\n\\(\\frac{49\\times 52}{114}\\)\n\\(\\frac{49\\times 62}{114}\\)\n49\n\n\nDunkle Haare\n\\(\\frac{64\\times 52}{114}\\)\n\\(\\frac{65\\times 62}{114}\\)\n65\n\n\nReihen Total\n52\n62\n114\n\n\n\n\n\n\n\nBlaue Augen\nBraune Augen\nZeilen total\n\n\n\n\nHelle Haare\n22.35\n26.65\n49\n\n\nDunkle Haare\n29.65\n33.35\n65\n\n\nReihen Total\n52\n62\n114\n\n\n\nDie beobachteten Werte (z. B. 38 Personen mit blauen Augen/hellen Haare) unterscheiden sich deutlich von den erwarteten Werten unter der Nullhypothese (22.35 Personen). Aber ist das auch statistisch signifikant?\n\nChi-Quadrat-Test\nDer traditionelle statistische Test für diese Frage ist Pearsons Chi-Quadrat-Test (auch Χ2-Test geschrieben). Wie t ist Χ2 eine Teststatistik, die abhängig von den Freiheitsgraden (df) einer ganz bestimmten Kurve folgt.\n\\[\nX^2= \\sum{\\frac{(O-E)^2}{E}}\n\\]\nWobei \\(O = \\text{observed}\\), \\(E = \\text{expected}\\)\n\n\n\n(aus Quinn & Keough 2002)\n\n\nWir können den Χ2-Wert in unserem Fall einfach händisch berechnen:\n\n\n\n\n\n\n\n\n\n\n\nO\nE\n\\((O-E)^2\\)\n\\(\\frac{(O-E)^2}{E}\\)\n\n\n\n\nHelle Haare & blaue Augen\n38\n22.35\n244.92\n10.96\n\n\nHelle Haare & braune Augen\n11\n26.65\n244.92\n9.19\n\n\nDunkle Haare & blaue Augen\n14\n29.65\n244.92\n8.26\n\n\nDunkle Haare & braune Augen\n51\n35.35\n244.92\n6.93\n\n\nX2\n\n\n\n35.33\n\n\n\nIst \\(\\chi^2 = 35.33\\) nun signifikant oder nicht? Dazu müssen wir noch die Freiheitsgrade berechnen und das Signifikanzniveau festlegen:\n\nFreiheitsgrade: \\((\\text{Spalten}-1)\\times(\\text{Zeilen}-1)=(2-1)\\times(\\text2-1)=1\\)\nSignifikanzlevel: z.B. \\(\\alpha = 0.05\\)\n\nTraditionell hätte man den kritischen Wert für diese Kombination in einer gedruckten Tabelle nachgeschlagen. Wir fragen einfach R, wobei wir 1 – α (in unserem Fall 1-0.05) eingeben müssen, da wir wissen wollen, ob wir im äussersten rechten Teil der Verteilungskurve liegen, also extremer als 95 % der Werte unter der Nullhypothese keiner Assoziation.\nqchisq(0.95,1)\n3.841495\nUnser berechneter Χ²-Wert (35.33) ist viel grösser als der kritische Wert (3.84), also gibt es eine Assoziation zwischen den Variablen (d. h. die Kombinationen blau/hell und braun/dunkel sind überproportional häufig). Wenn wir, wie oben empfohlen, einen präzisen p-Wert für die Assoziation wollen, erhalten wir ihn folgendermassen (beachte, dass chisq.test eine Matrix als Argument benötigt):\ncount &lt;- matrix(c(38,14,11,51), nrow = 2)\n\nchisq.test(count)\nPearson's Chi-squared test with Yates' continuity correction\ndata: count\nX-squared = 33.112, df = 1, p-value = 8.7e-09\nDie Assoziation ist also höchst signifikant (p &lt; 0.001).\n\n\nFishers exakter Test\nFür kleine Erwartungswerte in den Zellen (&lt; 5) ist der Chi-Quadrat-Test nicht zuverlässig. Dafür gibt es Fishers exakten Test.\ncount2 &lt;- matrix(c(3,5,9,1),nrow=2)\nfisher.test(count2)\n     Fisher's Exact Test for Count Data\n\ndata: count\np-value = 0.04299\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n0.001280876 1.102291244\nsample estimates:\nodds ratio\n0.08026151\nMan kann/sollte Fishers exakten Test jedoch grundsätzlich verwenden, da er mit der heutigen Rechenleistung von Computern kein Problem mehr darstellt. Angewandt auf unseren Haarfarben / Augenfarben-Datensatz ergibt sich:\ncount\n\n     [,1]  [,2]\n[1,]   38    11\n[2,]   14    51\nfisher.test(count)\n\n     Fisher's Exact Test for Count Data\n\ndata: count\np-value = 2.099e-09\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n  4.746351 34.118920\nsample estimates:\nodds ratio\n  12.22697\nWie man der Ausgabe entnehmen kann ist die Teststatistik hier die sogenannte odds ratio, ein Term für den es keine gute deutsche Übersetzung gibt. Sie bezeichnet die Wahrscheinlichkeit des Eintretens geteilt durch die Wahrscheinlichkeit des Nichteintretens. Aus der Umgangssprache und Wettspielen sind wir bereits vertraut mit odds ratios: “50:50-Chancen” bezeichnen nichts anderes als eine odds ratio von 1 (50 / 50 = 1). Bei einem Assoziationstest ist entspricht der odds ratio die Multiplikation der Wahrscheinlichkeiten auf der einen Diagonalen geteilt durch jene der anderen Diagonalen, also (38 x 51) / (14 x 11)."
  },
  {
    "objectID": "Statistik_1.html#wie-berichte-ich-statistische-ergebnisse",
    "href": "Statistik_1.html#wie-berichte-ich-statistische-ergebnisse",
    "title": "Statistik 1",
    "section": "Wie berichte ich statistische Ergebnisse?",
    "text": "Wie berichte ich statistische Ergebnisse?\n\nWelche relevanten Informationen benötige ich und wo finde ich sie?\nDie Ergebnisausgaben in R sind mitunter umfangreich. Da kommt es darauf an, effizient herausfiltern zu können, was welche Information darin bedeutet und welche davon man in einer wissenschaftlichen arbeit braucht. Hier ist die Ausgabe des vorhergehenden gepaarten t-Tests:\n     Paired t-test\n\ndata: blume$a and blume$b\nt = 3.4821, df = 9, p-value = 0.006916\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n  1.366339 6.433661\nsample estimates:\nmean of the differences\n                    3.9\nWelche Informationen davon werden benötigt:\n\nName des Tests (Methode)\nSignifikanz/p-Wert (Verlässlichkeit des Ergebnisses)\nEffektgrösse und -richtung (unser eigentliches Ergebnis!)\nggf. Wert der Teststatistik und Freiheitsgrade(“Zwischenergebnisse”)\n\nWerfen wir noch einmal einen Blick auf den Output von R:\n     Paired t-test\n\ndata: blume$a and blume$b\nt = 3.4821, df = 9, p-value = 0.006916\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n  1.366339 6.433661\nsample estimates:\nmean of the differences\n                    3.9\nWichtig ist es, bei aller “Begeisterung” für die p-Werte nicht unser eigentliches Ergebnis zu vergessen, d. h. die Antwort auf die Frage ob die Blüten von A oder von B grösser sind und wenn ja wie stark (blau). Ob Freiheitsgrade und der Wert der Teststatistik angegeben werden müssen, darüber gehen die Geschmäcker auseinander. Wenn man die Daten korrekt in R eingegeben hat, spezifiziert R die Freiheitsgrade automatisch und bei gegebenen Freiheitsgraden ist die Beziehung von t zu p eindeutig. Deshalb genügt es m. E. p anzugeben. (Aber wenn der Betreuer oder die Editorin auch noch t und df haben wollen, dann sollte man sie parat haben). Ein adäquater Satz im Ergebnisteil, der den obigen R output zusammenfasst, lautet daher:\n\n\n\n\n\n\nDie Blütengrösse unterschied sich hochsignifikant zwischen den beiden Sorten mit einem Mittelwert von 15.3 cm2 für Sorte A und 11.4 cm2 für Sorte B (gepaarter t-Test, p = 0.007, t = 3.482, FG = 9).\n\n\n\nOder auf Englisch:\n\n\n\n\n\n\nFlower sizes differed very significantly between the two cultivars with a mean size of 15.3 cm2 in cultivar A and 11.4 cm2 in the cultivar B (paired t-test, p = 0.007, t = 3.482, df = 9).\n\n\n\n\n\nText, Tabelle oder Abbildung?\nHier kommen ein paar wichtige Vorgaben und Empfehlungen:\n\nJedes Ergebnis nur 1x ausführlich darstellen, entweder als Abbildung, in einer Tabelle oder als Text\nWenn als Abbildung oder Tabelle, dann im Text mit einem zusammenfassenden Statement darauf verweisen, das nicht alle Details wiederholt\nSignifikante und nicht signifikante Ergebnisse berichten\nGängige Strategie:\n\nAbbildungen: für die wichtigsten signifikanten Ergebnisse\nTabellen: für die weiteren signifikanten Ergebnisse\nNur Text: für die nicht signifikanten Ergebnisse\n\n\n\n\nAbbildungen in wissenschaftlichen Arbeiten\nZumindest für die wichtigsten signifikanten Ergebnisse produzieren wir normalerweise Abbildungen. Dabei ist es wichtig, die folgenden Prinzipien zu beherzigen:\n\nAbbildungen (und Tabellen) sollten ohne den zugehörigen Text informativ sein, d. h. normalerweise p-Werte in der Abbildung/Tabelle bzw. Unter-/Überschrift angeben\nAchsen sind verständlich beschriftet (ausgeschriebene Variablennamen mit Einheit)\nKeine Abbildungsüberschrift (es gibt die Legende in der Abbildungsunterschrift)\nKeine überflüssigen Elemente (z. B. Rahmen, farbiger Hintergrund,horizontale und vertikale Linien)\nKlarer Kontrast, ausreichende Linienstärke und Schriftgrösse.\n\n\n\nAbbildungen mit “base R” oder mit ggplot2?\nIm Folgenden visualisiert mit den Boxplots, die zum t-Test gehören.\nIn “base R” geht das folgendermassen:\nboxplot(size~cultivar,data=blume.long)\n\n\n\n\n\nIn ggplot2 geht es folgendermassen (mit default-Einstellungen):\nlibrary(ggplot2)\n\nggplot(blume.long, aes(cultivar,size)) + geom_boxplot()\n\n\n\n\n\nGut ist, dass die Achsen automatisch beschriftet wurden. Störend ist der graue Hintergrund (reduziert Kontrast) und die weissen Gitternetzlinien (übeflüssig und dank des zu geringen Kontrasts eh kaum zu sehen).\nMan kann das in ggplot2 durch Wahl des vordefinierten theme_classic optimieren:\nggplot(blume.long, aes(cultivar,size)) + geom_boxplot()+\ntheme_classic()\n\n\n\n\n\nDas Ergebnis ist insgesamt OK, allerdings sind die Linien zu fein und die Schrift zu klein – jeweils relativ zur Gesamtgrösse der Abbildung.\nMan kann weiter optimieren durch Hinzufügen weiterer Steuerelemente:\nggplot(blume.long, aes(cultivar,size)) + \n    geom_boxplot(size=1) +\n    theme_classic() + \n    theme(axis.line = element_line(size=1), axis.ticks = element_line(size=1),axis.text = element_text(size = 20), axis.title =element_text(size = 20))\n\n\n\n\n\nJetzt passt es… Einzig könnte man noch den p-Wert einblenden und die Achsenbeschriftungen jeweils mit einem Grossbuchstaben beginnen.\nOb man die Grafiken mit ggplot2 oder base R gestaltet, sei jedem selbst überlassen. Beides hat Vor- und Nachteile. Was man aber vermeiden sollte, sind die Ausgaben von ggplot2 mit default-Einstellungen, da diese gängigen Standards für gute Grafiken widerspechen. Hier noch einmal zusammengefasst die Vor- und Nachteile beider Systeme:\nBase R:\n\nEinfache Syntax, daher geeignet für schnelles Plotten\nABER: Syntax variiert zwischen verschiedenen Plottbefehlen\nABER: “Finetunen” von Grafiken oftmals umständlich oder gar nicht möglich\nGeeignet für Vektoren (ggplot2 braucht dataframes o.ä)\nGeeignet für das Plotten von Modellen (plot(lm())\nEinfaches Plotten der Modelldiagnostik (plot(summary())\n\nVorteile ggplot2:\n\nLeistungsfähige, universelle Syntax, daher leicht anpassbar an den Bedarf, wenn man das Prinzip erst einmal verstanden hat\nViele Funktionen “out of the box”\nEinfachere Gestaltungsmöglichkeit (Farbskalen usw.)"
  },
  {
    "objectID": "Statistik_1.html#zusammenfassung",
    "href": "Statistik_1.html#zusammenfassung",
    "title": "Statistik 1",
    "section": "Zusammenfassung",
    "text": "Zusammenfassung\n\n\n\n\n\n\n\nWissenschaftliche Forschung zielt in der Regel entweder auf das Generieren oder das Testen von Hypothesen.\nInferenzstatistik ist das Set statistischer Verfahren (Tests), das sowohl für das Testen als auch das Generieren von Hypothesen) verwendet wird.\nInferenzstatistik ist notwendig, um zu bestimmen, wie wahrscheinlich ein beobachtetes Muster durch angenommenen Einflussgrössen (Variablen) und nicht durch (a) Messfehler oder (b) andere “Störgrössen” hervorgerufen wurde.\nDer p-Wert ist die Wahrscheinlichkeit eines Typ I-Fehlers, d. h. einen Effekt zu berichten, wo keiner ist; nach üblicher Konvention wird ein Effekt dann als hinreichend sicher (signifikant) angesehen, wenn p &lt; 0.05.\nMit einem Chi-Quadrat-Test (oder besser mit Fishers exaktem Test) kann man auf eine Assoziation zwischen zwei kategorialen Variablen testen.\nMit einem t-Test kann man auf Unterschiede in den Mittelwerten einer metrischen Variablen zwischen zwei Gruppen testen."
  },
  {
    "objectID": "Statistik_1.html#weiterführende-literatur",
    "href": "Statistik_1.html#weiterführende-literatur",
    "title": "Statistik 1",
    "section": "Weiterführende Literatur",
    "text": "Weiterführende Literatur\n\n**Crawley, M.J. 2015. *Statistics – An introduction using R**. 2nd ed. John Wiley & Sons, Chichester, UK: 339 pp.\n\nChapter 1 – Fundamentals\nChapter 6 – Two Samples\n\nQuinn, G.P. & Keough, M.J. 2002. Experimental design and data analysis for biologists. Cambridge University Press, Cambridge, UK: 537 pp."
  },
  {
    "objectID": "Statistik_2.html#lernziele",
    "href": "Statistik_2.html#lernziele",
    "title": "Statistik 2",
    "section": "Lernziele",
    "text": "Lernziele\n\n\n\n\n\n\nIhr…\n\nwisst, welche Voraussetzungen parametrische (und nicht-parametrische) Tests haben und welche Alternativen euch bei wesentlichen Verletzungen zur Verfügung stehen;\nkönnt eine ANOVA in R durchführen, versteht ihre Ergebnisse und könnt diese adäquat in Text und Abbildungen dokumentieren;\nhabt den Unterschied zwischen Korrelationen und Regressionen verstanden und könnt sie in R implementieren;\nkennt die Voraussetzungen und Gemeinsamkeiten aller linearen Modelle; und\nwisst, warum es nach der Berechnung eines linearen Modelles essenziell ist, die Residuen zu checken, und könnt die diagnostischen Grafiken von R dazu interpretieren."
  },
  {
    "objectID": "Statistik_2.html#varianzanalyse-anova-einstieg",
    "href": "Statistik_2.html#varianzanalyse-anova-einstieg",
    "title": "Statistik 2",
    "section": "Varianzanalyse (ANOVA): Einstieg",
    "text": "Varianzanalyse (ANOVA): Einstieg\n\nEinfaktorielle Varianzanalyse (One-Way ANOVA)\nEine ANOVA (Analysis of variance) ist die Verallgemeinerung des t-Tests für mehr als zwei Gruppen (Factor levels). Auch hier wollen wir wissen, ob/wie sich die Mittelwerte der abhängigen Variablen zwischen den Gruppen unterscheiden. Varianzanalyse heisst das Verfahren, weil der statistische Test zur Beantwortung der Frage das Verhältnis zweier Varianzen testet. Was es mit den zwei Varianzen auf sich hat, ist im Folgenden erklärt.\nGehen wir zurück zu unserem Blumenbeispiel. Die Idee der ANOVA ist, dass die Mittelwerte der Blütengrössen der beiden Sorten dann verschieden sind, wenn die Summe der Abweichungen (Residuen) vom Gesamtmittelwert “signifikant” grösser ist als die Summe der Abweichungen von den Sortenmittelwerten. Das ist in der folgenden Abbildung veranschaulicht. Die Punkte stellen die 20 Messwerte der Blütengrössen dar, wobei sie in der rechten Teilabbildung nach Sorten gruppiert sind. Der Gesamtmittelwert links und die beiden Sortenmittelwerte rechts sind als horizontale Linien dargestellt. Die vertikalen Linien sind die Residuen, als der Anteil der Varianz, welcher durch das jeweilige statistische Modell nicht erklärt wird. Das Modell links ist, dass die Blüten einheitlich gross sind, unabhängig von der Sorte, während das komplexere Modell rechts unterschiedliche Mittelwerte abhängig von der Sorte annimmt.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVarianz ist ein Mass für die Streuung von Werten um ihren Mittelwert. Mathematisch wird die Varianz wie folgt berechnet :\n\n\n\n\n\n\n\\[\n\\text{Varianz} = \\text{Summe der Abweichungsquadrate} / \\text{Freiheitsgrade}\n\\]\n(Summe der Abweichungsquadrate = Sum of squares = SS)\n\n\n\nAbweichungsquadrate sind dabei die quadrierten Werte der grünen (bzw. schwarzen und roten) vertikalen Linien in der obigen Abbildung. Die Distanzen werden quadriert, so dass negative Abweichungen gleichermassen zählen. Würde man nur die unquadrierten Werte aufsummieren, wäre das Ergebnis immer 0, da die horizontale Linie (der Mittelwert) ja genaus gelegt wurde, dass die positiven und negativen Abweichungen betragsmässig gleich sind. Ein zentraler Punkt der Varianzanalyse ist, dass sich die Gesamtsumme der Abweichungsquadrate (Total sum of squares) als die Summe zweier Teile (SSE und SSA) darstellen lässt:\n\n\n\n\n\n\n\\[\n\\text{SSY} = \\text{SSE} + \\text{SSA}\n\\]\n\nSSY = Total sum of squares\nSSE = Error sum of squares (entsprechend der unerklärte Varianz = Residuen)\nSSA = Sum of squares attributable to treatment (hier: Sorte)\n\n\n\n\nSchauen wir das zunächst beim Blumen-Datensatz an. Dazu müssen wir die Daten, die wir bislang im sogenannten wide format hatten (eine Spalte für Blütengrösse A und eine zweite für Blütengrösse B) im long format bereitstellen (eine Spalte für die Sorte und eine für die Blütengrösse). Generell ist das long format empfehlenswert, da viel universeller und von den meisten statistischen Verfahren verlangt.\nhead(blume.long)\n  cultivar size\n1        a   20\n2        a   19\n3        a   25\n4        a   10\n[…]\n11       b    8\n12       b   12\n13       b    9\nSchauen wir uns zunächst noch einmal das Ergebnis als “normalen” t-Test an:\nt.test(size~cultivar, blume.long, var.equal=T) \n    Two Sample t-test\n\ndata:  size by cultivar\nt = 2.0797, df = 18, p-value = 0.05212\nalternative hypothesis: true difference in means between group a and group b is not equal to 0\n95 percent confidence interval:\n -0.03981237  7.83981237\nsample estimates:\nmean in group a mean in group b \n           15.3            11.4\nNun nehmen wir dieselben Daten und analysieren sie mit einer Varianzanalyse. Der Befehl dazu ist aov (was für analysis of variance steht). Man kann sich die Ergebnisse der ANOVA mit summary und summary.lm anzeigen lassen und bekommt jeweils unterschiedliche Informationen (die wir beide benötigen):\nsummary(aov(size~cultivar))\n            Df Sum Sq Mean Sq F value Pr(&gt;F)  \ncultivar     1   76.0   76.05   4.325 0.0521 .\nResiduals   18  316.5   17.58  \nsummary.lm(aov(size~cultivar))\n[…]\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   15.300      1.326   11.54 9.47e-10 ***\ncultivarb     -3.900      1.875   -2.08   0.0521 . \nBeim ersten Output (summary) sehen wir eine typische “ANOVA-Tabelle” wie man sie als Ergebnis linearer Modelle erhält. Die Bedeutung der Abkürzungen ist wie folgt:\n\nDf = Degrees of freedom (Freiheitsgrade)\nSum Sq = Sum of squares (Summe der Abweichungsquadrate)\nMean Sq = Sum of squares / degrees of freedom (Quotient der beiden Werte)\nF value = Mean Sq (Treatment) / Mean Sq (Residuals) (Quotient derbeiden mittleren Abweichungsquadrate)\nPr(&gt;F) = Probability to obtaine a more extreme F value under the null hypothesis (p-Wert)\n\nDer F-Wert ist das Verhältinis der durch die Variable und die Residuen erklärten Varianzen (Mean squares), also \\(\\frac{76.05}{17.58} = 4.33\\). Der F-Wert (4.33) entsprichtdem quadrierte t-Wert (–2.08) aus der unteren Tabelle. Der p-Wert (0.052) in der obigen Tabelle ist also genau der gleiche wie im t-Test, was die Äquivalenz von ANOVA und t-Test zeigt. Dieser p-Wert steht für die Nullhypothese, dass sich die beiden Sorten nicht in ihrer Blütengrösse unterscheiden.\nDerselbe p-Wert taucht im summary.lm-Output unten in der zweiten Zeile auf. Aber für was steht der extrem kleine p-Wert in der ersten Zeile des summary.lm-Outputs (9.47 x 10–10)? In der Zeile steht (Intercept), also Achsenabschnitt. Hier ist der vorhergesagte Mittelwert für die erste Sorte (Cultivar a) gemeint. Die Nullhypothese zu dieser Zeile ist, dass die Blütengrösse dieser Sorte = 0 ist. Da Blütengrössen immer positive Werte haben (nie negativ und für eine existierende Blüte auch nie 0), ist das keine sinnvolle/relevante Nullhypothese. In den allermeisten Fällen bezieht sich der p-Wert in der ersten Zeile eines summary.lm-Outputs auf eine unsinnige/irrelevante Nullhypothese und wir können/müssen ihn ignorieren. Eine weitere wichtige Information liefert uns die zweite Tabelle aber noch: die Effektgrösse und -richtung. Dazu müssen wir in die Spalte Estimates schauen, welche die sogenannten Parameterschätzungen enthält. Im Falle einer ANOVA enthält die (Intercept)-Zeile den geschätzten Mittelwert für die alphabetisch erste Kategorie (bei uns also Cultivar a), währen das Estimate in der Zeile cultivarb für den Unterschied im Mittelwert von Cultivar b vs. Cultivar a steht, hier steht also die biologisch relevante Information, sprich: die Blüten von Cultivar b sind im Mittel 3.9 cm2 kleiner als jene von Cultivar a. Allerdings sind wir uns dieser Aussage nicht besonders sicher, da sie statistisch nur marginal signifikant ist (\\(p = 0.052\\)).\nWenn wir eine “echte” ANOVA mit drei oder mehr Kategorien durchführen, die also nicht mehr mit dem t-Test analysiert werden kann, sieht der Output vergleichbar aus, nur hat sich die Zahl der Freiheitsgrade in der ersten Zeile erhöht (immer Zahl der Kategorien – 1, bei 3 Kategorien also 2).\nsummary(aov(size~cultivar))\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ncultivar     2  736.1   368.0    18.8 7.68e-06 ***\nResiduals   27  528.6    19.6  \nIn diesem Fall gibt es also höchstsignifikante Unterschiede in der Blütengrösse zwischen den drei Sorten. Wir könnten das Ergebnis kurz und prägnant wie folgt wiedergeben:\n\n\n\n\n\n\nDie Blütengrösse unterschied sich höchstsignifikant zwischen den drei Sorten (ANOVA, p &lt; 0.001, F2;27 = 18.8; Abb. 1).\n\n\n\n\n\n\n\n\nAbb. 1. Boxplots der Blütengrössen der drei verglichenen Cultivare a, b und c (jeweils n = 10).\nZwei Anmerkungen: (1) Bei drei und mehr Kategorien kann man im Text nicht mehr effizient schreiben, welche Sorte sich wie von welcher anderen unterscheidet, deshalb bietet sich hier eher eine Visualisierung an (sofern die ANOVA signifikant ist). (2) Wenn man den F-Wert angeben möchte, so muss man im Subskript nachgestellt die Freiheitsgrade im Zähler (2) und im Nenner (27) angeben, die man der ANOVA-Tabelle entnehmen kann.\n\n\nPost-hoc-Test (Tukey)\nIn der vorhergehenden ANOVA wissen wir nun, dass es insgesamt ein signifikantes Muster gibt, dass also nicht alle drei Sorten der gleichen Grundgesamtheit angehören. Was wir nicht wissen, ist, welche Sorte sich von welcher anderen unterscheidet, und ggf. wie stark. Wenn die ANOVA insgesamt signifikant ist, muss das längst nicht heissen, dass jede Sorte sich von jeder anderen unterscheidet. Nun könnte man auf die Idee kommen, einfach für jedes Sortenpaar einen t-Test durchzuführen. Das Problem ist, dass man dann u. U. ziemlich viele Tests mit denselben Daten macht, und da summieren sich die Typ I-Fehlerraten schnell auf, sprich: bei vielen Tests werden rein zufällig manche ein signifikantes Ergebnis ergeben (mit α = 0.05 wird 5 % Irrtum zugelassen, d. h. im Durchschnitt liefert jeder zwanzigste Test ein falsch-positives Ergebnis). Um diesem Problem Rechnung zu tragen, gibt es sogenannte posthoc-Tests, die nach einer signifikanten ANOVA angewandt werden. Wenn die ANOVA nicht signifkant war, darf dagegen kein posthoc-Test angewandt werden! Der gängigste posthoc-Test ist jener von Tukey und findet sich u. a. im agricolae-Paket:\nlibrary(agricolae)\n\naov.1 &lt;- aov(size~cultivar, data=blume2)\n[…]\nComparison between treatments means\n\n      difference pvalue signif.        LCL       UCL\na - b        3.9 0.1388          -1.006213  8.806213\na - c       -8.0 0.0011      ** -12.906213 -3.093787\nb - c      -11.9 0.0000     *** -16.806213 -6.993787\nDas Ergebnis sagt uns, dass sich c von a und c von b, nicht aber b von a signifikant unterscheiden. Bei nur drei Kategorien kann man das noch so formulieren, bei vier, fünf oder mehr wird es aber schnell langatmig und komplex. Das lässt sich mit sogenannten homogenen Gruppen lösen. Hier versieht man die Kategorien mit gleichen Buchstaben, die sich nicht signifikant voneinander unterscheiden, ggf. kann dann eine Kategorie auch mehrere Buchstaben tragen. In unserem Fall wäre die Lösung also:\n\nCultivar a: A\nCultivar b: A\nCultivar c: B\n\nDiese Buchstaben kann man in die Ergebnisabbildung plotten oder als Superskript in einer Ergebnistabelle der Mittelwerte. Die folgende Abbildung zeigt ein Beispiel. Hier unterscheiden sich nur High und Low signifikant voneinander, da dies das einzige Paar ist, das keine gemeinsamen Buchstaben hat:\n\n\n\n(aus Quinn & Keough 2002)\n\n\nHier ist noch gezeigt, wie man die Beschriftung in die Boxplots bekommt:\naov.2 &lt;- aov(Sepal.Width ~ Species, data=iris)\nHSD.test(aov.2, \"Species\", console=TRUE)\nTreatments with the same letter are not significantly different.\n\n           Sepal.Width groups\nsetosa           3.428      a\nvirginica        2.974      b\nversicolor       2.770      c\nDie Buchstaben aus dem Output muss man dann manuell zur jeweiligen Art plotten (Reihenfolge der Arten beachten!)\nboxplot(Sepal.Width ~ Species, ylim=c(2,5), data=iris)\n\ntext(1, 4.8, \"a\")\ntext(2, 4.8, \"c\")\ntext(3, 4.8, \"b\")"
  },
  {
    "objectID": "Statistik_2.html#voraussetzung-statistischer-verfahren",
    "href": "Statistik_2.html#voraussetzung-statistischer-verfahren",
    "title": "Statistik 2",
    "section": "Voraussetzung statistischer Verfahren",
    "text": "Voraussetzung statistischer Verfahren\nIn Statistik 1 wurde kurz erwähnt, dass jeder statistische Test auf bestimmten Annahmen bezüglich der Werteverteilung in der Grundgesamtheit beruht. Beim klassischen t-Test nach Student sind das die Normalverteilung und die Varianzhomogenität.\n\nParametrische vs. nicht-parametrische Verfahren\nVerfahren, die auf dem folgenden gängigen Set von Voraussetzungen beruhen, werden als parametrische Verfahren bezeichnet. Es sind dies zugleich die “linearen Modelle” (doch zu diesem Begriff später mehr):\n\n\n\n\n\n\n\nNormalverteilung der Residuen\nVarianzhomogenität\nFeste x-Werte\nUnabhängigkeit der Beobachtungen / Zufällige Beprobung\n\n\n\n\nDem gegenüber gestellt werden so-genannte “nicht-parametrische” Verfahren. Der Begriff ist allerdings sehr irreführend, da nicht-parametrische Verfahren nicht etwa keine Voraussetzungen haben, sondern meist nur geringfügig schwächere als parametrische Verfahren. Die Voraussetzungen für die Anwendung gängiger nicht-parametrischer Verfahren sind:\n\n\n\n\n\n\n\nDie Verteilung der Residuen kann einer beliebigen Funktion folgen, muss aber für die verschiedenen Faktorlevels (Kategorien) gleich sein\nFeste x-Werte\nUnabhängigkeit der Beobachtungen / Zufällige Beprobung\n\n\n\n\nDiese beiden Listen, weisen auf zwei weitverbreitete Irrtümer in der Statistik hin, die in älteren Statistikbüchern regelmässig falsch dargestellt wurden und die auch heute noch in Statistikursen an Hochschulen oft falsch gelehrt werden:\n\n\n\n\n\n\n\nNur die Residuen des statistischen Models sollten normalverteilt sein. Dagegen ist es gleichgültig, ob die Werte der abhängigen Variablen normalverteilt sind und erst recht gilt das für die unabhängigen Variablen.\nDie Varianzhomogenität ist wichtiger als Normalverteilung der Residuen.\nDie naive Empfehlung, bei kleinsten Abweichungen von der Varianzhomogenität oder Normalverteilung auf ein nicht-parametrisches Äquivalent auszuweichen, ist im besten Fall unvorteilhaft (da nicht-parametrische Verfahren meist eine geringere Teststärke haben), im schlimmsten Fall falsch (wie die Voraussetzungen des nicht-parametrischen Verfahrens gleichermassen verletzt sind).\n\n\n\n\nIn der Folge ist zu beobachten, dass vielfach vorschnell und unnötig auf “nicht-parametrische” Verfahren ausgewichen wird. Dagegen sprechen viele Gründe dafür, in fast allen Fällen mit parametrischen Verfahren zu arbeiten:\n\n\n\n\n\n\n\nParametrische Verfahren sind recht robust gegen die Verletzung der Voraussetzung, d. h. sie liefern selbst recht starken Abweichungen noch (fast) korrekte p-Werte:\nLaut Quinn & Keough (2002) haben Simulationen Folgendes gezeigt:\n\n\\(n_1 = n_2 = 6\\): selbst bei bis zu vierfacher SD noch korrekte p-Werte\n\\(n_1 = 11\\), \\(n_2 = 21\\): Wenn SD1 = 4 SD2, dann entspricht ein berechneter \\(p = 0.05\\) in Wirklichkeit \\(p = 0.16\\)\n\nmit n1 und n2 = Stichprobengrösse für Faktorlevels 1 und 2 und SD = Standardabweichung\nDie meisten komplexeren statistischen Verfahren existieren ohnehin nur in einer parametrischen Variante.\nDank Datentransformationen und Generalisierungen linearer Modelle kann man auch mit Nicht-Normalität der Residuen und Varianzinhomogenität = Heteroskedasitzität umgehen.\n\n\n\n\n\n\nWie testet man die Voraussetzungen? (klassischer Weg)\nDer “klassische” (aber nicht zielführende!!!) Rat in vielen Statistikbüchern/-kursen ist die Anwendung statistischer Tests für Normalität und Varianzhomognität. Für die Normalität (beachten, dass die Residuen, nicht dir Rohdaten getestet werden müssen, also im Fall einer ANOVA die Werte jeder Kategorie für sich). Es gibt u.a. den Kolmogorov-Smirnov-Test (mit Lillefors-Korrektur) und den Sharpiro-Wilks-Test:\nshapiro.test(blume$b)\nFür das Testen der Varianzhomogenität gibt es u.a. den F-Test zur Varianzhomogenität und den Levene-Test (im Paket car):\nvar.test(blume$a, blume$b)\nlibrary(car)\nleveneTest(blume$a, blume$b,center = mean)\nWenn die p-Werte dieser Tests &lt; 0.05 sind, dann liegt eine statistisch signifikante Abweichung von der jeweiligen Voraussetzung vor. Die klassische Konsequenz war, dann auf ein nicht-parametrisches Verfahren auszuweichen. Studierende und viele PraktikerInnen lieben diese scheinbar simple Schwarz-weiss-Sicht, die ein klares Prozedere vorzugeben scheint. Leider bringen diese Tests für die Entscheidung zwischen parametrischen und nicht-parametrischen Verfahren NICHTS. Die Gründe sind eigentlich einfach:\n\nDie genannten Tests testen allesamt die Wahrscheinlichkeit der Abweichung, nicht den Grad der Abweichung (wobei Letzteres der relevante Punkt ist).\nDamit werden einerseits bei kleinen Stichproben auch problematische Abweichungen nicht erkannt, bei grossen Stichproben harmlose Abweichungen dagegen “moniert” (man sollte sich bewusst sein, dass Variablen in der realen Welt niemals perfekt normalverteilt oder perfekt varianzhomogen sind)\n\nDeshalb wird in modernen Lehrbüchern ausdrücklich davon abgeraten, die genannten Tests für diesen Zweck zu verwenden (z. B. Quinn & Keough 2002).\n\n\nWie testet man die Voraussetzungen? (empfohlener Weg)\nDa die “klassischen” numerischen Tests nichts helfen, bleibt nur ein Weg, selbst wenn er zunächst unbefriedigend und subjektiv erscheinen mag. Moderne statistische Lehrbücher empfehlen heute, Normalverteilung der Residuen und Varianzhomogenität visuell zu prüfen und nur bei groben Verletzungen über Gegenmassnahmen nachzudenken.\nIm Fall von t-Tests bzw. ANOVAs ist die einfachste Möglichkeit, nach Faktorlevels gruppierte Boxplots zu betrachten. Alternativ gingen auch Histogramme, allerdings sind diese nur bei grossen n aussagekräftig:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFür die Beurteilung der Varianzhomogenität betrachtet man am besten die Höhe der Boxen im Boxplot. Wenn sie ähnlich hoch sind, ist alles OK, wenn sie sehr stark abweichen, hat man evtl. ein Problem. Sehr stark meint aber, siehe oben, wirklich sehr stark, d. h. wenn die Box in einer Kategorie mehr als 4-mal so hoch ist wie in einer anderen (bei gleichen/ähnlichen Replikatzahen), und ab mehr als doppelt so hoch bei erheblich verschiedenen Replikatzahlen. Im vorliegenden Fall ist die Varianz in Gruppe 1 etwa 2.5-mal so hoch wie in Gruppe 2, da die Zahl der Replikate aber identisch war, wäre das noch OK.\nZur Beurteilung der Normalverteilung bzw. des entscheidenden Aspekts der Normalverteilung, der Symmetrie, sind ebenfalls die Boxplots aufschlussreich. Eine starke Verletzung liegt vor, wenn der Median weit ausserhalb der Mitte der Box liegt oder wenn der obere “whisker” viel länger als der untere ist.\nAusserdem gibt es noch das Central Limit Theorem (CLT) in der Statistik. Dieses Theorem besagt, dass wenn eine betrachtete Variable selbst schon ein Mittelwert ist, sie zwingend einer Normalverteilung folgt. In diesem Fall ist also gar kein Test nötig/sinnvoll. Wenn man sich auf das CLT berufen will, kann man z. B. Quinn & Keough (2002) zitieren.\n\n\nWas tun, wenn die Voraussetzungen verletzt sind? (nicht-parametrische Verfahren)\nBei Verletzung der Voraussetzungen, kann man auf nicht-parametrische Verfahren ausweichen, was OK ist, wenn man sich völlig klar darüber ist, welche Voraussetzungen diese ihrerseits haben:\nDas nicht-parametrische Äquivalent zum t-Test ist der Wilcoxon-Rangsummen-Test. Er funktioniert, indem Werte in Ränge transformiert und summiert werden (W-statistic). Nachteile sind, dass er sehr konservativ ist (d. h. tendenziell zu hohe p-Werte schätzt) und zudem keine exakten p-Werte berechnen kann, wenn “Bindungen” (ties) vorliegen (d. h. mehrere Beobachtungen identische Werte aufweisen). Ausserdem sei noch einmal betont, dass der Wilcoxon-Test zwar keine Annahme über die Verteilung der Werte pro Gruppe macht, jedoch voraussetzt, dass diese in jeder Gruppe gleich ist.\nwilcox.test(blume$a, blume$b)\nFerner gibt es Randomisierungs-t-Tests. Diese haben den Vorteil, dass keine Annahme über die Verteilung getroffen werden muss (die Verteilung wird aus den Daten generiert). Zugleich müssen die Beobachtungen noch nicht einmal unabhängig sein. Allerdings testet man hier strenggenommen auch nicht auf Unterschiede in den Grundgesamtheiten, sondern ermittelt die Wahrscheinlichkeit, die beobachteten Unterschiede zufällig erzielt zu haben. Wer mehr über Randomisierungs-Tests wissen will, findet in Logan (2010: 148–150) weitergehende Infos.\nIm Fall der ANOVA gibt es zwei Situationen:\n\nWir haben starke Abweichungen von der Normalverteilung der Residuen, aber ähnliche Varianzen. Dann kann der Kruskal-Wallis-Test zum Einsatz kommen (ebenfalls ein Rangsummen-Test). Der zugehörige posthoc-Test ist der Dunn-Test mit Benjamin-Hochberg-Korrektur der p-Werte (wegen multiplem Testen):\nkruskal.test(data = blume2, size~cultivar)\nlibrary(FSA)\n\ndunnTest(data = blume2, size~cultivar, method = \"bh\")\nWenn dagegen die Varianzen sehr heterogen sind, die Residuen aber relativ normal/symmetrisch, wie in der folgenden Abbildung, kann der Welch-Test eingesetzt werden:\n\n\n\n\n\n\noneway.test(data=blume2, size~cultivar, var.equal=F)\n\n\nWas tun, wenn die Voraussetzungen verletzt sind? (Transformationen)\nStatt auf nicht-parametrische Verfahren auszuweichen, kann man auch Transformationen anwenden. Da es um die Verteilung der Residuen geht, muss primär die abhängige Variable für Transformationen in Betracht gezogen werden, manchmal hilft aber auch die Transformation einer unabhängigen Variablen (weitergehende Infos siehe Fox & Weisberg 2019: 161–169).\nWenn man über die Anwendung von Transformationen nachdenkt, sind zwei Aspekte relevant: (1) Entgegen manchen Behauptungen sind untransformierte Daten (linear Skala) nicht per se natürlicher/richtiger. Auch die lineare Skala ist eine Konvention. Viele Naturgesetze (z. B. unsere Sinneswahrnehmung) funktionieren dagegen auf einer Logarithmusskala. (2) Wenn man die abhängige Variable transformiert, muss man sich aber klar darüber sein, dass man dann strenggenommen Hypothesen über die transformierten Daten, nicht über die ursprünglichen Werte testet. Achtung: Wenn man die Analysen mit tranformierten Daten durchführt, darf man für die Ergebnisdarstellung die Rücktransformation mittels der jeweiligen Umkehrfunktion nicht vergessen!\nGängige Transformation für die abhängige Variable sind die folgenden:\nLogarithmus-Transformation:\n\nGut bei rechtsschiefen Daten/wenn die Varianz mit dem Mittelwert zunimmt.\nDie “natürlichste” Transformation.\nNatürlicher Logarithmus (log) oder Zehnerlogarithmus (log10) möglich.\nWerte müssen &gt; 0 sein.\n\nlog (x + Konstante)-Transformation:\n\nFindet man häufig in der Literatur, wenn abhängige Variablen transformiert werden sollen, die auch Nullwerte enthalten\nEs werden unterschiedliche Konstanten (x) addiert, mal 1, mal 0.01. Es ist aber völlig willkürlich, ob man 1000000 oder 0.00000001 oder 3.24567 addiert, hat aber starken Einfluss auf die Ergebnisse\nAuch lassen sich die Ergebnisse nach so einer komplexen Transformation schlecht interpretieren (da man dann ja eine Hypothese über die transformierten Daten testet, s. o.)\nIn Übereinstimmung mit Wilson (2007) rate ich daher dringend von derlei Transformationen ab!\n\nWurzeltransformation:\n\nHat einen ähnlichen Effekt wie die Logarithmus-Transformation, lässt sich im Gegensatz zu dieser auch beim Vorliegen von Nullwerten anwenden (Werte müssen nur positiv sein).\nDie “Stärke” der Transformation kann man durch die Art der Wurzel kontinuierlich einstellen: Quadratwurzel, Kubikwurzel, 4. Wurzel,…\n\n“arcsine”-Transformation:\nasin(sqrt(x))\\*180/pi\n\nWurde traditionell für Prozentwerte (Proportionen) und andere abhängige Variablen empfohlen, die zwischen 0 und 1 bzw. 0 und 100% begrenzt sind (z. B. Quinn & Keough 2002).\nNach neueren Untersuchungen (Warton & Hui 2011) wird eher davon abgeraten.\n\nRangtransformation:\n\nIm Prinzip das, was “nicht-parametrische” Verfahren machen.\nGrösster Informationsverlust von allen genannten Verfahren (noch grösser wäre der Informationsverlust nur bei Überführung der metrischen abhängigen Variablen in Kategorien oder gar in eine Binärvariable).\n\nDie folgenden Abbildungen visualisieren exemplarisch die Effekte unterschiedlicher Transformationen auf die Werteverteilung (ganz links sind jeweils die untransformierten Daten, die Transformation rechts hat jeweils eine deutlich bessere Annäherung an die Normalverteilung erzielt).\n\n\n\n  \n\n\n\n\n(aus Quinn & Keough 2002)\n\n\n\nMeist muss man nur die abhängige Variable transformieren. Es gibt aber Spezialfälle, wo man erst nach Transformation der abhängigen und der unabhängigen Variable eine adäquate Residuenverteilung erzielt. Dies ist insbesondere dann der Fall, wenn wir eine in Wirklichkeit nicht-lineare Beziehung mit einem linearen Modell abbilden. Wenn etwa im Falle einer einfachen linearen Regression (s. u.) in Wirklichkeit ein Potenzgesetz (y = a xb) vorliegt, erzielt man näherungsweise Varianzhomogenität und Normalverteilung der Residuen nur, wenn man a und b logarithmustransformiert."
  },
  {
    "objectID": "Statistik_2.html#mehrfaktorielle-anova",
    "href": "Statistik_2.html#mehrfaktorielle-anova",
    "title": "Statistik 2",
    "section": "Mehrfaktorielle ANOVA",
    "text": "Mehrfaktorielle ANOVA\nBislang haben wir uns eine ANOVA mit nur einem Prädiktor, d. h. einer kategorialen Variablen mit zwei bis vielen Ausprägungen, angeschaut. Das Prinzip lässt sich aber auch auf zwei und mehr kategoriale Prädiktoren ausweiten. Man spricht dann von einer mehrfaktoriellen ANOVA. Im Optimalfall sollten alle Kombinationen Faktorlevels aller Prädiktorvariablen auftreten (dann spricht man von einem vollfaktoriellen Design), am besten sogar in gleicher/ähnlicher Häufigkeit.\nBetrachten wir exemplarisch die Situation mit zwei Prädiktoren (zweifaktorielle Varianzanalyse, two-way ANOVA). Hierzu haben wir in unserem Blumenbeispiel neben den drei Sorten noch ein weiteres “Treatment” hinzugefügt, nämlich, ob die Pflanzen im Gewächshaus (house = yes) oder im Freiland (house = no) aufgezogen wurden. Der Boxplot in der explorativen Datenanalyse sieht wie folgt aus:\n\n\n\n\n\nWir haben nun zwei Möglichkeiten, die zweifaktorielle Varianzanalyse durchzuführen, mit oder ohne Berücksichtigung von Interaktionen:\nsummary(aov(size~cultivar+house))\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ncultivar     2  417.1   208.5   5.005     0.01 *  \nhouse        1  992.3   992.3  23.815 9.19e-06 ***\nResiduals   56 2333.2    41.7     \nsummary(aov(size~cultivar*house))\n               Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ncultivar        2  417.1   208.5   5.364   0.0075 ** \nhouse           1  992.3   992.3  25.520 5.33e-06 ***\ncultivar:house  2  233.6   116.8   3.004   0.0579 .  \nResiduals      54 2099.6    38.9     \nOhne Interaktion (oben) verknüpfen wir die beiden Prädiktoren einfach mit “+”; wenn wir die Interaktion auch analysieren wollen (unten), dann verwenden wir “*” zur Verknüpfung. Ein Interaktion läge dann vor, wenn sich die Auswirkung von Gewächshaus vs. Freiland zwischen den Sorten unterschiede, etwa in einem Fall positiv, im anderen neutral oder negativ. Wir sehen, dass die untere ANOVA mit dem Interaktionsterm im Output eine dritte Zeile cultivar:house enhält, welcher die Signifikanz der Interaktion angibt (in unserem Fall also marginal signifikant).\nLiegt eine signifikante Interaktion vor, dann nimmt man zur Ergebnisdarstellung am besten eine Grafik, einen sogenannten Interaktionsplot, da sich die Interaktion schon bei zweifaktoriellen ANOVAs schwer in Worte fassen lässt und noch schwerer bei dreifaktoriellen ANOVAs mit potenziell einer Dreifachinteraktion und drei Zweifachinteraktionen:\ninteraction.plot(cultivar,house,size)\n\n\n\n\n\nDie Interaktion war nicht signifikant, was sich darin zeigt, dass die Linienzüge für yes und no einigermassen parallel sind, d. h. im Gewächshaus alle drei Kultivare grösser waren. Allerdings haben sich die drei Kultivare nicht völlig konsistent verhalten: der positive Einfluss von Gewächshaus war bei Sorte c viel grösser als bei den anderen beiden (was zu einem p-Wert der Interaktion nahe an der Signifikanzschwelle geführt hat).\n# Visualisierung 2-fach-Interaktion etwas elaborierter \n# mit ggplot\n\nlibrary(sjPlot)\nlibrary(ggplot2)\ntheme_set(theme_classic())\n\naov &lt;- aov(size ~ cultivar * house, data = blume3)\nplot_model(aov, type = \"pred\", terms = c(\"cultivar\", \"house\") )\n\n\n\n\n\nMit sjPlot kann man auch gut 3-fach-Interaktionen visualisieren, wie das folgende Beispiel zur Auswirkung von Managment und Hirschbeweidung (fenced = keine Hirsche) über zwei Versuchsjahre auf den Pflanzenartenreichtum zeigt:\n\n\n\n\n\naov.deer &lt;- aov(Species.richness ~ Year * Treatment * Plot.type, data = Riesch)\nplot_model(aov.deer, type = \"pred\", terms = c(\"Year\", \"Treatment\", \"Plot.type\"))"
  },
  {
    "objectID": "Statistik_2.html#korrelationen",
    "href": "Statistik_2.html#korrelationen",
    "title": "Statistik 2",
    "section": "Korrelationen",
    "text": "Korrelationen\nPearson-Korrelationen analysieren den Zusammenhang zwischen zwei metrischen Variablen** und beantworten dabei die folgenden Fragen:\n\nGibt es einen linearen Zusammenhang?\nIn welche Richtung läuft er?\nWie stark ist er?\n\nWichtig dabei ist, dass Korrelationen keine Kausalität voraussetzen oder annehmen. Es gibt also keine abhängige und unabhängige Variable, keine Unterscheidung in Prädiktor- und Antwortvariable. Logischerweise liefern Korrelationen dann auch identische Ergebnisse, wenn x- und y-Achse vertauscht werden.\nDie folgenden fünf Abbildungen zeigen verschiedene Situationen. Bei (a) liegt eine positive Korrelation vor, bei (b) eine negative und bei (c)–(e) keine Korrelation. Bei (e) erkennt man zwar visuell eine Beziehung (ein “Peak” in der Mittel, also eine unimodale Beziehung), aber das ist eben kein linearer Zusammenhang.\n\n\n\n(aus Quinn & Keough 2002)\n\n\nBei der Pearson-Korrelation betrachtet man die beiden Parameter Kovarianz (reicht von −∞ bis +∞) und die Korrelation, welche die Covarianz auf den Bereich von –1 bis +1 standardisiert. Pearsons Korrelationskoeffizient r ist der Schätzer für die Korrelation basierend auf der Stichprobe:\n\n\n\n(aus Quinn & Keough 2002)\n\n\nDie implizite Nullhypothese (H0) ist nun ρ = 0. Die Teststatistik ist das uns schon bekannte t mit \\(t = \\ \\frac{r}{s_{r}}\\) , wobei sr für den Standardfehler von r steht und bei n – 2 Freiheitsgraden gestet wird.\nDie Pearson-Korrelation ist die “parametrische” Variante der Korrelationen. Ihre Anwendung hat zwei Voraussetzungen (in Klammern ist angegeben, wie man ihr Vorliegen visuell überprüfen kann):\n\nLinearität (Überprüfung mit einem xy-Scatterplot)\nBivariate Normalverteilung (Überprüfung mit Boxplots beider Variablen)\n\nWenn diese Voraussetzungen ungenügend erfüllt sind, kann man auf nicht-parametrische Äquivalente ausweichen. Diese testen auf monotone, nicht auf lineare Beziehungen, liefern allerdings keine exakten Ergebnisse bei Bindungen (d.h. wenn der gleiche Wert mehrfach vorkommt):\n\nFür 7 ≤ n ≤ 30: Spearman-Rang-Korrrelation (rs) (im Prinzip Pearsons r für rangtransformierte Daten)\nFür n &gt; 30: Kendall’s tao (τ)\n\nHier noch der R Code für alle drei Möglichkeiten:\ncor.test(df$Species.richness, df$N.deposition, method = \"pearson\")\ncor.test(df$Species.richness, df$N.deposition, method = \"spearman\")\ncor.test(df$Species.richness, df$N.deposition, method = \"kendall\")"
  },
  {
    "objectID": "Statistik_2.html#einfache-lineare-regressionen",
    "href": "Statistik_2.html#einfache-lineare-regressionen",
    "title": "Statistik 2",
    "section": "Einfache lineare Regressionen",
    "text": "Einfache lineare Regressionen\n\nIdee\nEinfache lineare Regressionen sind konzeptionell und mathematisch ähnlich zu Pearson-Korrelationen. Oft werden beide Verfahren daher fälsch auch begrifflich durcheinandergeworfen. Der entscheidende Unterschied ist, dass wir für eine Regression eine theoretisch vermutete Kausalität haben müssen. Damit haben wir, anders als bei einer Korrelation, eine fundamentalte Unterscheidung in:\n\nX: unabhängige Variable (independent variable), Prädiktorvariable (predictor)\nY: abhängige Variable (dependent variable), Antwortvariable (response)\n\nBei Visualisierungen ist zu beachten, dass die unabhängige Variable immer auf der x-Achse dargestellt wird, die abhängige dagegen auf der nach oben gerichteten y-Achse.\nMathematisch wird eine lineare Regression analysiert, indem die bestangepasste Gerade durch die Punktwolke des xy-Scatterplots gelegt wird. Dabei sieht das lineare Modell folgendermassen aus:\n\nGeradengleichung: \\(y = b_0 + b_1 x\\)\nStatistisches Modell: \\(y_i = \\beta_0 + \\beta_1 x_i + \\epsilon\\), wobei \\(\\epsilon_i\\) das Residuum des i-ten Daten­punktes ist, d. h. seine vertikale Abweichung vom vorhergesagten Wert\n\nMit einer einfachen linearen Regression testet man die folgenden beiden Nullhypothesen:\n\n\\(H_0\\): \\(\\beta_0 = 0\\) (Achsenabschnitt [intercept] der Grundgesamtheit ist Null) (diese erste Nullhypothese ist, ähnlich wie bei Varianzanalysen, in den meisten Fällen wissenschaftlich nicht relevant)\n\\(H_0\\): \\(\\beta_0 = 1\\) (Steigung [slope] der Grundgesamtheit ist Null)\n\nDie folgende Abbildung veranschaulicht die verschiedenen Möglichkeiten:\n\n\n\n(aus Logan 2010)\n\n\n\n\nStatistische Umsetzung\nEs mag vielleicht zunächst überraschen, aber ähnlich wie beim Vergleich von Mittelwerten zwischen kategorischen Ausprägungen kategorischer Variablen, liegt auch der linearen Regression eine Varianzanalyse zugrunde:\n\n\n\n\n\n\n\n\n(aus Quinn & Keough 2002)\n\n\nWiederum ist die Teststatistik ein \\(F\\)-ratio, nämlich \\(F = \\frac{\\text{MS}_\\text{Regressionen}}{\\text{MS}_\\text{Residual}}\\), wobei MS für die mittleren Quadratsummen steht, also die Quadratsummen (SS) geteilt durch die Freiheitsgrade (df). Wie oben unter der Varianzanalyse schon erwähnt, folgt \\(F\\) einer \\(t^2\\)-Verteilung.\n\n\nImplementierung in R\nDas Kommando zum Berechnen einfacher linearer Regressionen lautet lm. Wie bei einem Mittelwertvergleich mittels Varianzanalyse gibt es dann zwei verschiedene Ansichten des Ergebnis-Outputs, die jeweils verschiedene Teilaspekte zeigen (Hier am Beispiel der Beziehung von Pflanzenartenreichtum zur Stickstoffdeposition):\nlm &lt;- lm(Species.richness~N.deposition, data = df)\n\nanova(lm)       # ANOVA-Tabelle, 1. Möglichkeit\nsummary.aov(lm) # ANOVA-Tabelle, 2. Möglichkeit\nResponse: Species.richness\n             Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nN.deposition  1 233.91 233.908  28.028 0.0001453 ***\nResiduals    13 108.49   8.346 \nDie anova-Ansicht liefert uns die oben besprochene ANOVA-Tabelle, einschliesslich der Signifikanz der Steigung (hier \\(p = 0.0001\\)). Weitere erforderliche Aspekte des Ergebnisses sehen wir in der summary-Ansicht:\nsummary(lm) # Regressionskoeffizienten\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  25.60502    1.26440  20.251 3.25e-11 ***\nN.deposition -0.26323    0.04972  -5.294 0.000145 ***\n[…]\nResidual standard error: 2.889 on 13 degrees of freedom\nMultiple R-squared:  0.6831,    Adjusted R-squared:  0.6588 \nF-statistic: 28.03 on 1 and 13 DF,  p-value: 0.0001453\nWie wir sehen, tauchen wiederum der F-Wert (28.03) und sogar zweimal der p-Wert der Steigung (0.0001) auf, daneben auch der i. d. R. bedeutungslose p-Wert des Achsenabschnitts (intercept) (3.25 x 10-11).\nWerfen wir noch einmal einen Blick auf den Output von R:\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  25.60502    1.26440  20.251 3.25e-11 ***\nN.deposition -0.26323    0.04972  -5.294 0.000145 ***\n[…]\nResidual standard error: 2.889 on 13 degrees of freedom\nMultiple R-squared:  0.6831,    Adjusted R-squared:  0.6588 \nF-statistic: 28.03 on 1 and 13 DF,  p-value: 0.0001453\nWir benötigen\n\nName des Verfahrens (Methode): Einfache lineare Regression (mit der Methode der kleinsten Quadrate).\nSignifikanz (Verlässlichkeit des Ergebnisses): p-Wert der Steigung, nicht der p-Wert des Achsenabschnittes (wird nach üblicher Konvention auf drei Nachkommastellen gerundet oder, wenn unter 0.001, dann als p &lt; 0.001 angegeben).\nEffektgrösse und -richtung (unser eigentliches Ergebnis!): Im Falle einer linearen Regression ist das die Funktionsgleichung, die sich aus den Schätzungen der Koeffizienten ergibt.\nErklärte Varianz (Relevanz des Ergebnisses): Wie viel der Gesamtvariabilität der Daten wird durch das Modell erklärt? Ob \\(R^2\\) oder \\(R_\\text{adj.}^2\\) angegeben werden sollte, wird unterschiedlich gesehen, jedenfalls sollte man explizit sagen, was gemeint ist. \\(R^2\\) ist übrigens der quadrierte Wert von Pearsons Korrelationskoeffizienten r.\nggf. Wert der Teststatistik mit den Freiheitsgraden (“Zwischenergebnisse”): \\(F_{1,2} = 11.34\\)\n\nEin adäquater Ergebnistext könnte daher wie folgt lauten:\n\n\n\n\n\n\nDie Variable b nahm hochsignifikant mit der Variablen a zu (Funktionsgleichung: \\(b = 5.02 + 0.42 * a\\) , \\(F_{1,2} = 11.34\\), \\(p = 0.010\\), \\(R^2 = 0.586\\).\n\n\n\nBei einem signifkanten Ergebnis bietet sich auch noch eine Visualisierung mittels Scatterplot an, in den die Regressionsgerade geplottet ist:\nplot(b~a,xlim=c(0,25),ylim=c(0,20))\nabline(lm(b~a))\n\n\n\n\n\n\n\nVoraussetzungen\nEinfache lineare Regressionen basieren auf drei Vorausetzungen:\n\nLinearität\nNormalverteilung (der Residuen!)\nVarianzhomogenität\n\nFür das meistverwendete Verfahren der kleinsten Abweichungsgquadrate (wie bislang besprochen; ordinary least squares = OLS), auch als Modell I-Regressionen bezeichnet, muss zudem gelten:\n\nFeste x-Werte, d. h.\n\nx-Werte vom Experimentator gesetzt ODER\nFehler in den x-Werten viel kleiner als in den y-Werten\n\nSowie auch für folgende Fälle:\n\nHypothesentest \\(H_0: \\beta_1 = 0\\) im Fokus, nicht der exakte Wert von β~1\nFür prädiktive Modelle\nWenn keine bivariate Normalverteilung vorliegt\n\n\n\n\nAlternativen zur Methode der kleinsten Quadrate (OLS)\nWenn keine der oben unter Punkt 4 genannten Voraussetzungen erfüllt ist, dann sollte eine sogenannte Modell-II-Regression (Nicht-OLS-Regression) durchgeführt werden. Hier stehen als Möglichkeiten die Major axis regression, die Ranged major axis regression und die Reduced major axis regression zur Verfügung. Details finden sich in Logan (2010: 173–175), woraus aus die folgende Visualisierung stammt:\n\n\n\n(aus Logan 2010)\n\n\nIn R stehen solche Methoden u. a. im Paket lmodel2 zur Verfügung:\nlibrary(lmodel2)\nlmodel2(b~a)\nRegression results\n  Method Intercept     Slope Angle (degrees) P-perm (1-tailed)\n1    OLS  5.019254 0.4170422        22.63820                NA\n2     MA  4.288499 0.4648040        24.92919                NA\n3    SMA  3.067471 0.5446097        28.57314                NA\nWie man sieht, unterscheiden sich die beiden Modell-II-Ergebnisse deutlich von Modell I (OLS)."
  },
  {
    "objectID": "Statistik_2.html#lineare-modelle-allgemein",
    "href": "Statistik_2.html#lineare-modelle-allgemein",
    "title": "Statistik 2",
    "section": "Lineare Modelle allgemein",
    "text": "Lineare Modelle allgemein\n\nWas macht ein lineares Modell aus?\nDie meisten statistischen Verfahren, die wir bis zu diesem Punkt angeschaut haben, gehören zu den linearen Modellen. Dieser Begriff wird häufig weitgehend synonym mit “parametrischen Verfahren” verwendet, ist aber treffender. Von den bisherigen Verfahren gehören die folgenden zu den linearen Modellen:\n\nPearson-Korrelation\nt-Test\nVarianzanalyse\nEinfache lineare Regression\n\nWas macht nun lineare Modelle aus:\n\nVoraussetzungen: Normalverteilung der Residuen und Varianzhomogenität\nIn R kann man sie (mit Ausnahme der Pearson-Korrelation) mit dem Befehl lm abbilden (ja, auch die Varianzanalyse!)\nVarianzanalysen und lineare Regressionen nutzen beide ANOVA-Tabellen mit F-ratios als Testverfahren\nLineare Modelle lassen sich als Linearkombination der Prädiktoren schreiben, d. h.:\n\nPrädiktoren werden nicht als Multiplikator, Divisor oder Exponent anderer Prädiktoren verwendet\ndie Beziehung muss aber nicht zwingend linear sein.\n\n\n\n\nWelche Verfahren gehören zu den linearen Modellen?\nNeben den schon besprochenen einfachen Verfahren gehören auch eine ganze Reihe komplexerer Vefahren zu den linearen Modellen, die aber alle den vorstehenden Bedingungen entsprechen. Die meisten werden wir in Statistik 3 besprechen. Logan (2010: 165) hat eine recht umfassende folgende Übersicht erstellt. Darin sind metrische Prädiktoren als x, x1 und x2 bezeichnet, kategoriale als A bzw. B. Was unter R Model formula steht, würde im jeweiligen Fall in die Klammern des lm-Befehls gesetzt:\n\n\n\n(aus Logan 2010)\n\n\n\n\nTesten der Voraussetzungen von linearen Modellen (Modelldiagnostik)\nWie geschrieben, haben lineare Modelle bestimmte Voraussetzungen. Selbst wenn lineare Modelle recht robust gegen Verletzungen der Vorassetzungen sind, so muss man doch jedes Mal, nachdem man ein lineares Modell gerechnet hat, prüfen, ob die Voraussetzungen erfüllt waren. Es geht hier primär um die Voraussetzungen Varianzhomogenität, Normalverteilung der Residuen und Linearität.\nWichtig ist, zu verstehen, dass man zunächst das lineare Modell rechnen muss und erst nachträglich prüfen kann, ob die Voraussetzungen erfüllt waren. Das liegt daran, dass die Kernannahmen Varianzhomogenität und Normalverteilung der Residuen sich auf das Modell, nicht auf die Originaldaten beziehen. Einzig für t-Tests und ANOVAs kann man diese beiden Punkte auch in der explorativen Datenanalyse vor dem Berechnen des Modells erkunden, für lineare Regressionen und komplexere Modelle geht das nicht. Wenn der nachträgliche Test zeigt, dass eine der Voraussetzungen schwerwiegend verletzt war, bedeutet das, dass man das Modell neu spezifizieren muss, etwa durch eine geeignete Transformation der abhängigen Variablen.\nDas Überprüfen der Voraussetzungen (= Modelldiagnostik) erfolgt visuell mittels der sogenannten Residualplots, die man mit dem generische plot-Befehl bekommt, wenn man als Argument das Ergebnis eines linearen Modells hat. Man bekommt dann vier Plots, die man am besten in einem 2 x 2-Arrangement ausgibt (das macht der erste Befehl):\npar(mfrow = c(2, 2)) # 4 Plots in einem Fenster\nplot(lm)\nBetrachten wir zwei Fälle, zunächst das Beispiel von eben:\n\n\n\n\n\nund die zugehörigen Residualplots:\n\n\n\n\n\nIn diesem Fall ist alles OK. Man muss vor allem die oberen beiden Teilabbildungen betrachten. Links oben kann man gut erkennen, wenn Linearität oder Varianzhomogenität verletzt wären, rechts oben dagegen, wenn die Normalverteilung der Residuen verletzt wäre. Zu berücksichtigen ist, dass reale Daten nie perfekt linear, varianzhomogen und normalverteilt sind.\nUns interessieren nur massive Abweichungen. Wir würden sie wie folgt erkennen:\n\nLinearität: Eine Verletzung erkennen wir in der linken oberen Abbildung, wenn wir eine “Wurst” bzw. “Banane” sehen, also wenn die linken Punkte alle unter der gepunktelten Linie, die mittleren alle darüber und die rechten wieder alle darunter lägen (oder umgekehrt).\nVarianzhomogenität: Eine Verletzung erkennen wir in der linken oberen Abbildung, wenn die Punktwolke einen starken Keil (meist nach rechts offen) beschreibt.\nNormalverteilung der Residuen: Eine Verletzung erkennen wir in der rechten oberen Abbildung, wenn die Punkte sehr stark von der gestrichelten Linie abweichen, insbesondere wenn sie eine ausgeprägte Treppenkurve bilden.\n\nDie beiden unteren Abbildungen sind für die Diagnostik weniger wichtig. Links unten haben wir eine skalierte Version der Abbildung links oben. Die Abbildung rechts unten zeigt uns, ob bestimmte Datenpunkte übermässigen Einfluss auf das Gesamtergebnis haben. Das wären Punkte mit einer Cook’s distance über 0.5 und insbesondere über 1. In solchen Fällen sollten wir noch einmal kritisch prüfen, ob (a) evtl. ein Eingabefehler vorliegt und (b) der bezeichnete Punkt wirklich zur Grundgesamtheit gerechnet werden sollte. Wenn aber beide Aspekte nicht zu beanstanden sind, dann gibt es auch keinen Grund, den entsprechenden Datenpunkt auszuschliessen; wir müssen uns nur bewusst sein, dass er das Gesamtergebniss übermässig stark beeinflusst.\nZum Schluss kommt noch ein Beispiel, bei dem die Modellvoraussetzungen einer linearen Regression klar nicht erfüllt sind.\n\n\n\n\n\n\n\n\n\n\nHier sind die Voraussetzungen klar nicht erfüllt: (a) es liegt starke Varianzinhomogenität vor (links oben als nach rechts offener Keil erkennbar, links unten als klar ansteigende Kurve); (b) die Normalverteilung der Residuen ist auch nicht gegeben (im Q-Q-Plot rechts oben weichen die Punkte stark von der theoretischen Kurve ab und bilden stattdessen eine Treppenkurve). Schliesslich sehen wir rechte unten auch noch, dass es einen extrem einflussreichen Datenpunkt mit Cook’s distance &gt; 1 und einen weiteren mit Cook’s distance &gt; 0.5 gibt.\nIn diesem Fall schlussfolgern wir, dass das Modell fehlspezifiziert war. Da die Varianz mit dem Mittelwert zunimmt, während zugleich keine Null-Werte unter der abhängigen Variablen auftreten, wäre eine Logarithmus-Transformation der abhängigen Variablen hier vermutlich ein zielführendes Vorgehen. Dieses sollten wir ausprobieren und anschliessend wiederum die Residualplots betrachten."
  },
  {
    "objectID": "Statistik_2.html#zusammenfassung",
    "href": "Statistik_2.html#zusammenfassung",
    "title": "Statistik 2",
    "section": "Zusammenfassung",
    "text": "Zusammenfassung\n\n\n\n\n\n\n\nt-Tests und ANOVAs sind parametrische Verfahren, um auf Unterschiede in den Mittelwerten einer metrischen Variablen zwischen zwei bzw. beliebig vielen Gruppen zu testen.\nKorrelationen testen auf einen linearen Zusammenhang zwischen zwei metrischen Variablen, ohne Kausalität anzunehmen.\nEinfache lineare Regressionen machen das Gleiche unter Annahme eines gerichteten Zusammenhangs (d. h. wenn es eine unabhängige und eine abhängige Variable gibt).\nParametrische Verfahren basieren auf bestimmten Annahmen zur Streuung der Daten, sind aber robust gegenüber deren Verletzung.\nDie Voraussetzungen parametrischer Verfahren beziehen sich auf die Residuen, nicht auf die unabhängigen, noch auf die abhängigen Variablen per se.\nSowohl lineare Regressionen als auch ANOVAs gehören zu den linearen Modellen und können in R mit dem Befehl lm spezifiziert werden."
  },
  {
    "objectID": "Statistik_2.html#weiterführende-literatur",
    "href": "Statistik_2.html#weiterführende-literatur",
    "title": "Statistik 2",
    "section": "Weiterführende Literatur",
    "text": "Weiterführende Literatur\n\nCrawley, M.J. 2015. Statistics – An introduction using R. 2nd ed. John Wiley & Sons, Chichester, UK: 339 pp.\n\nChapter 7 – Regression: pp. 114–139\nChapter 8 – Analysis of Variance: pp. 150–167\n\nFox, J. & Weisberg, S. 2019. An R companion to applied regression. 3rd ed. SAGE Publications, Thousand Oaks, CA, US: 577 pp.\nLogan, M. 2010. Biostatistical design and analysis using R. A practical guide. Wiley-Blackwell, Oxford, UK: 546 pp.\n\npp. 151-166 (lineare Modelle)\npp. 167-207 (Korrelation und einfache lineare Regression)\npp. 254-282 (Einfaktorielle ANOVA)\npp. 311-359 (Mehrfaktorielle ANOVA)\n\nQuinn, G.P. & Keough, M.J. 2002. Experimental design and data analysis for biologists. Cambridge University Press, Cambridge, UK: 537 pp.\nWarton, D.I. & Hui, F.K.C. 2011. The arcsine is asinine: the analysis of proportions in ecology. Ecology 92: 3–10.\nWilson, J.B. 2007. Priorities in statistics, the sensitive feet of elephants, and don’t transform data. Folia Geobotanica 42: 161–167."
  },
  {
    "objectID": "Statistik_3.html#lernziele",
    "href": "Statistik_3.html#lernziele",
    "title": "Statistik 3",
    "section": "Lernziele",
    "text": "Lernziele\n\n\n\n\n\n\nIhr…\n\nwisst, wofür ANCOVA steht, wann dieses statistische Verfahren zum Einsatz kommt und wie das praktisch geht.\nversteht, wann es Sinn macht, quadratische Terme in eine Regression einfliessen zu lassen und warum das dann trotzdem noch ein lineares Modell ist;\nkönnt lineare Regressionen mit mehreren Prädiktoren in R implementieren und wisst, welche Aspekte ihr bei der Modellspezifikation und bei der Auswahl des “besten” Modells beachten müsst; und\nkennt die Gütemasse des information theoretician approach und könnt sie interpretieren."
  },
  {
    "objectID": "Statistik_3.html#genereller-ablauf-einer-statistischen-analyse",
    "href": "Statistik_3.html#genereller-ablauf-einer-statistischen-analyse",
    "title": "Statistik 3",
    "section": "Genereller Ablauf einer statistischen Analyse",
    "text": "Genereller Ablauf einer statistischen Analyse\nDas folgende Schema zeigt den generellen Ablauf einer statistischen Analyse, wie er für alle schon besprochenen und auch alle noch kommenden Verfahren gilt:\n\n\n\n\n\nEin zentrales Element ist die Modelldiagnostik, die wir in Statistik 2 am Ende behandelt haben. Leider wird sie oft vergessen! Basierend auf den Ergebnissen der Modelldiagnostik kann man entweder die Ergebnisse fertigstellen oder aber man muss zu den initialen Schritten zurückgehen. Möglicherweise war das gewählte statistische Verfahren schon nicht adäquat oder das Verfahren war in Ordnung, nur die Details der Spezifizierung (etwa Transformationen von Daten) müssen nachgebessert werden."
  },
  {
    "objectID": "Statistik_3.html#covarianzanalyse-ancova",
    "href": "Statistik_3.html#covarianzanalyse-ancova",
    "title": "Statistik 3",
    "section": "Covarianzanalyse (ANCOVA)",
    "text": "Covarianzanalyse (ANCOVA)\nWie wir schon bei “Lineare Modelle allgemein” in Statitik 2 gesehen haben, lassen sich metrische und kategoriale Variablen in einem einzigen linearen Modell kombinieren. Eine ANCOVA macht genau dieses, ist also im Prinzip eine Kombination aus ANOVA und linearer Regression. Stellen wir uns vor, wir hätten einen Datensatz von Körpergewichten von Kindern unterschiedlichen Alters (age: metrisch) und Geschlechts (sex: kategorial/binär, dargestellt als blau und rot). Eine ANCOVA testet nun, ob und wie sich das Gewicht in Abhängigkeit von beiden Faktoren verhält. Dabei gibt es im Prinzip sechs verschiedene Möglichkeiten/Ergebnisse:\n\n\n\n(aus Crawley 2015)\n\n\nWie andere lineare Modelle auch, kann man eine ANCOVA mittels aov oder mittels lm spezifizieren. Es ist zu beachten, dass hier die Reihenfolge der Variablen wichtig ist:\nsummary(aov(weight~age\\*sex))\nIm vollen Modell (full model, global model) wurden vier Parameter gefittet (2 Steigungen und 2 Achsenabschnitte). Das haben wir durch das “*”-Zeichen spezifiziert. Dieses sagt, dass nicht nur Alter und Geschlecht unabhängig voneinander einen (additiven) Effekt haben, sondern dass der Effekt des Alters je nach Geschlecht unterschiedlich sein könnte, also die Gewichtszunahme mit. Jedoch sind oft nicht alle bedeutsam. Es ist daher wichtig, das Modell so lange zu vereinfachen, bis nur noch bedeutsame Parameter übrig sind. Dann hat man das minimal adäquate Modell.\nFür die Modellvereinfachung gibt es unterschiedliche Strategien (mehr dazu später bei den “Multiplen linearen Regressionen”). Man muss jedenfalls schrittweise vorgehen, d. h. immer nur einen Parameter löschen und dann das neue Modell anschauen. Wenn von den Parametern welche nicht signifikant sind, könnte man z. B. zunächst den am wenigsten signifikanten löschen und dann das neue Model betrachten, usw.\nAlternativ kann man auch ANOVAs zum Vergleich zweier unterschiedlich komplexer Modelle verwenden. Das klingt zunächst schräg, da wir bislang ANOVAs verwendet haben, um innerhalb eines Modelles zu sehen, ob etwa die durch die Steigung erklärte Varianz signifikant ist. Den gleichen Ansatz kann man aber auch verwenden, um zwei unterschiedlich komplexe Modelle miteinander zu vergleichen. Wichtig ist nur, dass das eine Modell im anderen geschachtelt ist:\nanova(lm(weight~age\\*sex), lm(weight~age+sex))\nDas komplexere Modell ist jenes mit “*”, das einfachere jenes mit “+”, da dort eine einheitliche Gewichtszunahme mit dem Alter angeommen wird. Wenn die ANOVA nun ein signifikantes Ergebnis liefert, heisst das, dass der zusätzliche Parameter des komplexeren Modells (die Interaktion Alter x Geschlecht) mehr erklärt als zufällig zu erwarten und daher beibehalten werden sollte. Wenn die ANOVA ein nicht-signifkantes Ergebnis liefert, sollten wir uns für das einfachere Modell (jenes mit “+”) entscheiden."
  },
  {
    "objectID": "Statistik_3.html#polynomische-regressionen",
    "href": "Statistik_3.html#polynomische-regressionen",
    "title": "Statistik 3",
    "section": "Polynomische Regressionen",
    "text": "Polynomische Regressionen\nEine quadratische Regression (Polynom 2. Ordnung) ist die einfachste Möglichkeit, eine sogenannte unimodale (humpshaped) Beziehung von abhängiger zur unabhängigen Variablen mathematisch abzubilden. Unimodal/humpshaped meint, dass die Kurve ein Maximum hat, d. h. die abhängige Variable für mittlere Werte der Prädiktorvariablen den höchsten Wert aufweist. Für viele Beziehungen sind solche unimodalen Kurvenverläufe theoretische vorhergesagt und/oder theoretisch nachgewiesen. In der Ökologie gilt das z. B. für die Beziehung des Artenreichtums zu so unterschiedlichen Faktoren wie Störungshäufigkeit (intermediate disturbance hypothesis, IDH), Boden-pH-Wert und Produktivität/Biomasse.\nDas statistische Modell für eine quadratische Beziehung ist:\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2\n\\]\nIn R wird eine quadratische Regression folgendermassen codiert:\nsummary(lm(f~e+I(e^2)))\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept) -2.239308   3.811746  -0.587  0.56777   \ne            1.330933   0.360105   3.696  0.00306 **\nI(e^2)      -0.031587   0.007504  -4.209  0.00121 **\nWichtig ist, dass man den quadratischen Term im lm-Befehl nicht einfach als e^2 eingeben kann, sondern I(e\\^2) schreiben muss. Eine signifikante unimodale Beziehung ist dann gegeben, wenn die Parameterschätzung für den Quadratischen Term (also e^2) negativ ist – man hat eine nach unten offene Parabel. Ist der quadratische Term dagegen signifikant positiv, hat man eine nach oben offene Parabel, also eine u-förmige Beziehung (Minimum für die abhängige Variable bei intermediären Werten der Prädiktorvariablen).\nWichtig ist, dass man wie bei allen statistischen Modellen nachträglich die Modellvoraussetzungen prüft.\nIm vorhergehenden Beispiel sah es mit einer einfachen linearen Regression so aus (Code, Ergebnisplot und Residualplots):\nplot(f~e,xlim=c(0,40),ylim=c(0,20))\nabline(lm(f~e),col=\"blue\")\n\n\n\n\n\n\n\n\n\n\nMan ahnt schon im Scatterplot mit der gefitteten einfachen linearen Regression, dass etwas mit dem Modell nicht stimmt, was durch die Bananenform im Residualplot links oben unterstrichen wird: die Beziehung ist evident nicht linear.\nNach Hinzufügen des quadratischen Terms sieht man schon im Scatterplot mit der gefitteten Funktion, dass es viel besser passt, aber erst recht in den Residualplots. Mit predict kann man jede Funktion plotten, die als Ergebnis einer Regressionsanalyse herauskommt. Im Prinzip zerlegt man die x-Achse in viele kleine Segmente und plottet dann jeweils Geraden zwischen zwei aufeinander folgenden vorhergesagten Punkten.\nxv &lt;- seq(0,40,0.1)\nplot(f~e,xlim=c(0,40),ylim=c(0,20))\nyv2 &lt;- predict(lm.quad,list(e=xv))\nlines(xv,yv2,col=\"red\")\n\n\n\n\n\n\n\n\n\n\nBezüglich des statistische Vorgehens ist zu beachten, dass man den quadratischen Term nur im Modell behalten sollte, wenn er signifikant ist (bei nur einem quadratischen Term der p-Wert aus summary, sonst ggf. mit anova testen oder AICc-Werte (siehe später) vergleichen). Dagegen muss der lineare Term (hier: e) dann beibehalten werden, wenn der quadratische Term signifikant ist, selbst wenn der lineare Term nicht signifkant ist. (Wenn beide nicht signifikant sind, fallen dagegen beide raus).\nWenn es theoretische Gründe gibt, kann man in gleicher Weise auch Polynome höherer Ordnung implementieren. Wichtig ist, im Hinterkopf zu behalten, dass eine polynomische Regression fast immer eine deutliche Simplifizierung der Realität darstellt. Sie ist ein probates und einfaches Mittel, um zu testen, ob die Beziehung signifikant unimodal ist. Dagegen ist sie problematisch als prädiktives Modell, da sie oft negative Werte für die abhängige Variable voraussagt, zumindest ausserhalb des gefitteten Bereichs. Negative Werte sind aber vielfach theoretisch unmöglich (z. B. Artenzahlen, Stoffkonzentrationen,…)."
  },
  {
    "objectID": "Statistik_3.html#multiple-lineare-regressionen",
    "href": "Statistik_3.html#multiple-lineare-regressionen",
    "title": "Statistik 3",
    "section": "Multiple lineare Regressionen",
    "text": "Multiple lineare Regressionen\n\nVorgehen\nAnalog zur mehrfaktoriellen ANOVA, sind multiple lineare Regressionen einfach lineare Regressionen mit mehreren Prädiktoren. Das statistische Modell lautet also folgendermassen (wobei x1 … xi metrische Variablen sind):\n\\[\ny_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i}+ (...) +\\beta_j x_{j,i}\n\\]\nIn R wird das wie folgt codiert:\nmodel1 &lt;- lm (y ~ x1 + x2 + x3, data = mydata)\nMöglich sind aber auch folgende Modelle:\nmodel2 &lt;- lm (y ~ x1 + x2 + I(x2\\^2), data = mydata)\nmodel3 &lt;- lm (y ~ x1 + x2 + log10(x3), data = mydata)\nmodel4 &lt;- lm (y ~ x1 + x2 + x1:x2, data = mydata)\nUnd für ein konkretes Beispiel (Abhängigkeit der Vogelabundanz in isolierten Waldinseln von verschiedenen Umweltvariablen (YR.ISOL = year since isolation, ALT = altitude, GRAZE = grazing):\nmodel &lt;- lm (ABUND ~ YR.ISOL + ALT + GRAZE, data=loyn)\nsummary(model)\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -73.58185  107.24995  -0.686 0.495712    \nYR.ISOL       0.05143    0.05393   0.954 0.344719    \nALT           0.03285    0.02679   1.226 0.225618    \nGRAZE        -4.01692    0.99881  -4.022 0.000188 ***\nUnd wie immer schauen wir die Residualplots an, die eigentlich ziemlich gut aussehen:\npar(mfrow=c(2,2))\nplot(model)\n\n\n\n\n\nAllerdings dürfen wir uns hier im Falle einer multiplen Regression noch nicht zufrieden zurücklehnen, sondern müssen uns zunächst noch zwei potenziellen Problemen annehmen: (1) Korrelation zwischen den Prädiktoren und (2) Overfitting.\n\n\nProblem 1: Korrelation zwischen den Prädiktoren\nDamit lm verlässliche Parameterschätzungen liefern kann, müssen die Prädiktoren (hinreichend) unabhängig (man spricht auch von: orthogonal) sein. Das muss man vor dem Fitten des Models testen und dann von Paaren hochkorrelierter Variablen jeweils eine ausschliessen.\nEs gibt zwei gängige Testmöglichkeiten:\n\nKorrelationmatrix: nur Parameter mit |r| &lt; 0.7 werden beibehalten (manchmal findet man auch andere Schwellenwerte, etwa 0.6 oder 0.75: wie eigentlich alles in der Statistik, ist es keine Schwarz-weiss-Welt).\nVariance inflation factor (VIF): \\(\\text{VIF}_{i}\\  = \\frac{1}{1 - R_{i}^2}\\) , mit \\(R_i^2\\) aus dem Modell Prädiktor i gegen alle übrigen Prädiktoren\n\nDer VIF sagt uns, dass der Standardfehler (SE) des Prädiktors um \\(\\sqrt{VIF}\\) grösser ist als im orthogonalen Fall. Meist werden Variablen bis \\(\\text{VIF} = 5\\), manchmal bis \\(\\text{VIF} = 10\\) akzeptiert.\nDie Berechnung der Korrelationsmatrix geht in R sehr einfach:\ncor &lt;- cor(loyn[,2:7])\ncor\nDas Ergebnis ist allerdings unübersichtlich. Man kann es vereinfachen, indem man nur jene Werte darstellt, die über dem selbstgewählten Schwellenwert (hier 0.6) liegen.\ncor[abs(cor)&lt;0.6] &lt;- 0\ncor\n        AREA    YR.ISOL DIST LDIST      GRAZE ALT\nAREA       1  0.0000000    0     0  0.0000000   0\nYR.ISOL    0  1.0000000    0     0 -0.6355671   0\nDIST       0  0.0000000    1     0  0.0000000   0\nLDIST      0  0.0000000    0     1  0.0000000   0\nGRAZE      0 -0.6355671    0     0  1.0000000   0\nALT        0  0.0000000    0     0  0.0000000   1\nWenn man die Schwelle bei 0.6 ansetzt, müsste man also von den beiden Variablen GRAZE und YR.ISOL eine aus dem Modell entfernen, da sie zu stark negativ korreliert sind. Dabei sind drei Dinge wichtig:\n\nStatistisch gibt es kein klares Argument, welche von mehreren hoch-korrelierten Variablen man im vollen Modell streichen sollte (man könnte höchstens zusätzlich den VIF heranziehen). Inhaltlich macht es Sinn, diejenige Variable beizubehalten, die (a) besser interpretierbar ist oder (b) häufiger in vergleichbaren Studien gebraucht wurde.\nMan sollte im Methodenteil dokumentieren welche Variable(n) wegen positiver/negativer Korrelation mit welcher anderen aus dem vollen Modell gestrichen wurden.\nBei der Interpretation der Ergebnisse stehen die beibehaltenen Variablen auch für die jeweils gestrichenen hochkorrelierten Variablen (zumindest zu einem erheblichen Teil).\n\nDie Berechnung der VIF’s geht wie folgt:\nlibrary(car)\nvif(model)\n YR.ISOL      ALT    GRAZE \n1.679995 1.200372 1.904799 \nHier sieht man nicht, welche Variable mit welcher anderen korrliert ist, man bekommt nur ein Gesamtranking. Da die VIF-Werte aller drei Variablen unter 5 sind, können alle beibehalten werden. Wenn mehrere Variablen einen VIF &gt; 5 haben, muss man schrittweise immer die Variable mit dem höchsten VIF-Wert entfernen und die VIF-Werte dann neuberechnen. Sie ändern sich, wenn eine Variable wegfällt, da sie die Gesamt-Korrelationsstruktur des Datensatzes widerspiegeln.\n\n\nProblem 2: Overfitting\nDas Problem des Overfitting soll mit der folgenden Simulation veranschaulicht werden: zu einer Stichprobe von sechs Beobachtungen mit zwei numerischen Variablen werden schrittweise polynomische Modelle höher Ordnung gefittet.\nDer Code dafür ist:\nlm=lm(y~x)\nxy &lt;- seq(from=0,to=10,by=0.1)\nyv &lt;- predict(lm,list(x=xv))\nlines(xv,yv)\nlm2=lm(y~x+I(x\\^2))\nxy &lt;- seq(from=0,to=10,by=0.1)\nyv &lt;- predict(lm2,list(x=xv))\nlines(xv,yv)\n\n[usw.]\nDas Ergebnis sieht folgendermassen aus:\n\n\n\n\n\n\n\\(R^2 = 0.012\\)\n\n\n\n\n\n\n\n\\(R^2 = 0.111\\)\n\n\n\n\n\n\n\n\n\n\\(R^2 = 0.170\\)\n\n\n\n\n\n\n\n\\(R^2 = 0.875\\)\n\n\n\n\n\n\n\n\n\n\\(R^2 = 1.000\\)\n\n\n\n\n\nWir sehen, dass die erklärte Varianz kontinuierlich vom 2-Parameter-Modell (Achsenabschnitt und Steigung) zum 6-Parameter-Modell (Achsenabschnitt, Parameter für \\(x\\) bis \\(x^5\\)) zunimmt. Ein polynomische Modell (\\(n -1\\)). Ordnung erzielt immer 100% Anpassung and die Daten (\\(R^2 = 1\\)), wenn man $n Beobachtungen hat. Aber ist das Modell deswegen auch besonders korrekt oder aussagekräftig? Das darf bezweifelt werden. Ein gutes Modell wäre ja eines, das die zugrunde liegende Gesetzmässigkeit erkennt und daher auch für die Interpolation und Extrapolation geeignet ist.\nEs zeigt sich, dass die gute Anpassung an die Daten (good fit, hier gemessen als R2) nur der eine Aspekt eines guten Modells ist. Zugleich sollte es möglichst einfach (parsimonous) sein, d. h. das Beobachtete mit möglichst wenigen Annahmen erklären. Es gilt das folgende Prinzip, das auf den mittelalterlichen Philosophen Willliam of Ockham (ca. 1288–1347 zurückgeht).\n\n\n\n\n(Skizze aus einer Handschrift von Ockhams Summa logicae)\n\n\n\n\n\n\n\n\nOckham’s razor = Law of parsimony (Sparsamkeitsprinzip)\nWesenheiten dürfen nicht über das Notwendige hinaus vermehrt werden\nFormulierung von Johannes Clauberg (1622–1665)\n\n\n\n\n\nModellvereinfachung\nNun stellt sich die Frage, wie wir vom vollen Modell (full model, global model) also jenem nach Entfernung hochkorrelierter Variablen zum “besten” Modell gelangt, das also eine bestmögliche Kombination von guter Anpassung an die Daten (Fit) und Parsimonie aufweist. Dieses anzustrebende statistische Modell wird auch minimal adäquates Modell (mininum adequate model) genannt.\nGanz generell gilt: Man sollte maximal \\(p = \\frac{n}{3}\\) Parameter fitten (wobei n = Zahl der Datenpunkte/Beobachtungen und bei \\(p\\) auch der Achsenabschnitt [\\(b_0\\)] mitgezählt wird).\nMögliche Kriterien für das “beste” Modell (minimum adequate model):\n\nHöchster \\(R^2_{adj.}= 1 - \\frac{\\text{SS}_\\text{Resudial}/[n - (p + 1)]}{\\text{SS}_\\text{Total}/ (n - 1)}\\) (vgl. \\(R^2 = \\frac{\\text{SS}_\\text{Regression}}{\\text{SS}_\\text{Total}} = 1-\\frac{\\text{SS}_\\text{Regression}}{\\text{SS}_\\text{Total}}\\)) Ist nicht wirklich zielführend, da der “Strafterm” (um den \\(R^2\\) reduziert wird) zu gering ist, um wirklich für Parsimonität zu sorgen.\nSchrittweise Modellvereinfachung ausgehend vom “maximalen Modell” Durch: Entfernen von (a) nicht-signifikanten Interaktionen, (b) nicht-signifikanten quadratischen Termen und schliesslich (c) nicht-signifkanten linearen Variablen.\n\nDie schrittweise Modellvereinfachung kann wiederum auf drei verschiedene Weisen geschehen (die meist, aber nicht immer, die gleichen Ergebnisse liefern):\n\nSchrittweise die am wenigsten signifkanten Terme entfernen, bis alle signifikant sind:\nmodel1 &lt;- lm (ABUND ~ YR.ISOL + ALT + GRAZE, data=loyn)\nsummary(model1)\nmodel2 &lt;- update(model1,~.-YR.ISOL)\nsummary(model2)\nMittels ANOVA schrittweise Modelle vergleichen und Terme hinzufügen, wenn signifikent, bzw. entfernen, wenn nicht\nanova(model1,model2)\nEine automatische Funktion zum schrittweisen Hinzufügen (forward selection) oder Löschen (backward selection) oder beidem verwenden (es gibt verschiedene Packages, bei Interesse bitte googlen).\n\nVarianten a bis c sind im Prinzip OK, man muss sich aber bewusst sein, dass gerade bei vielen Variablen dieses schrittweise Vorgehen nicht zwingend das wirklich beste Modell findet, sondern man in einem “lokalen Optimum” landen kann (als Alternative siehe die dredge-Funktion unter “Information theoretician approach und multimodel inference”).\n\n\nVarianzpartitionierung\nWenn man das minimal adäquate Modell gefunden hat, will man oft noch wissen, wie bedeutsam die einzelnen enthaltenen Variablen sind. Bedeutsamkeit/Relevanz haben wir weiter oben als \\(R^2\\) (erklärte Varianz) ausgedrückt. Wir können uns also anschauen, welche Anteile der erklärten Varianz auf welche Variablen zurückgehen. Da unsere Variablen (auch nach einem Korrelationstest und Ausschluss der besonders hoch korrelierten) nicht völlig orthogonal = unabhängig voneinander sind, verhalten sich die Varianzen nicht additiv. Vielmehr ist die erklärte Varianz in einem Modell mit zwei Variablen meist niedriger als die Summe der Varianzen der beiden Einzelmodelle. In einer Varianzpartitionierung wird die Varianz jeder Variablen daher in eine unabhängige (independent, I) und eine gemeinsame (joint, J) Komponente zerlegt:\nlibrary(hier.part)\n\n\nloyn.preds &lt;- with(loyn, data.frame(YR.ISOL,ALT,GRAZE))\nhier.part(loyn$ABUND,loyn.preds,gof=\"Rsqu\")\n$IJ\n                 I          J     Total\nYR.ISOL 0.11892853 0.13444049 0.2533690\nALT     0.06960132 0.07926823 0.1488696\nGRAZE   0.30019854 0.16562324 0.4658218 \n$I.perc\n               I\nYR.ISOL 24.33428\nALT     14.24131\nGRAZE   61.42441 \nDer grösste Teil (61%) der insgesamt erklärten Varianz dieses Drei-Parameter-Models wird hier also durch den Faktor Grazing erklärt.\n\n\nErgebnisdarstellung: partielle Regressionen und 3-D-Grafiken\nWährend sich die ermittelte Beziehung zwischen Antwort- und Prädiktor-Variable auch bei nichtlinearen Verläufen einfach mit predict visualisieren lässt, solange man nur eine Prädiktorvariable hat (selbst wenn sie in transformierter Weise im lm eingespeist wird), ist das bei mehreren Prädiktoren eine Herausforderung. Hier seien zwei Möglichkeiten kurz erwähnt:\n\nPartielle Regressionen (sie zeigen wie die Beziehung aussähe, wenn all übrigen Faktoren konstant wären\nlibrary(car)\navPlots(model, ask=F)\n\n\n\n\n\n3D Response surfaces (es gibt Packages, um dasselbe auch für zwei Prädiktoren gleichzeitig zu machen; dies mach insbesondere Sinn, wenn auch quadratische Terme dabei sind; bei Interesse bitte googlen)"
  },
  {
    "objectID": "Statistik_3.html#information-theoretician-approach-und-multimodel-inference",
    "href": "Statistik_3.html#information-theoretician-approach-und-multimodel-inference",
    "title": "Statistik 3",
    "section": "Information theoretician approach und multimodel inference",
    "text": "Information theoretician approach und multimodel inference\n\nVergleich mit frequentist statistics\nEs gibt zwei grundlegende statistische Philosophien:\nFrequentist statistics (“klassisiche” Statistik)\n\nAlles, was wir bislang gemacht haben\nGrundannahme: Es gibt ein einziges richtiges Modell der Wirklichkeit, dem man sich mit Irrtumswahrscheinlichkeitenannähern kann\nNutzt p-Werte\n\nInformation theoretician approach\n\nDas, was wir in diesem Unterkapitel besprechen\nGrundannahme: Es kann ähnlich gute Modelle der Wirklichkeit geben, es gibt nicht das eine wahre Modell\nNutzt keine p-Werte\nDafür AIC (Akaike information criterion) oder BIC (Bayesian information criterion)\nModellmittelung (model averaging) möglich\n\n\n\nMasse der Modellgüte: AIC, BIC, AICc, \\(\\Delta_i\\), Evidence ratios, Akaike weights\nDie folgende Übersicht zeigt die wichtigsten Gütemasse im Vergleich. Wie schon besprochen, berücksichtigt \\(R_\\text{adj.}^2\\) (nahezu) ausschliesslich den Fit (also die Anpassung der Kurve an die Daten). Dagegen berücksichtigen die Informationskriterien Fit und Komplexität (Komplexität meint das Gegenteil von Parsimonität). Bei AICc und BIC = SC fliesst schliesslich auch noch die Zahl der Datenpunkte ein:\n\n\n\n(aus Johnson & Omland 2004)\n\n\nDabei gilt für AIC:\n\\(AIC = \\text{n}(ln(RSS)) - \\text{n} \\times ln(n) + 2 (k+1)\\) mit:\n\nRSS = Residual sum of squares\nk = Parameter des Models, inkl. Achsenabschnitt\nn = Anzahl der Beobachtungen/Replikate\n\nAICc ist der AIC für “kleine” Stichprobengrössen\n(wobei “klein” bis zu 40 k reicht, also bei 2 Parametern wie in einer einfachen linearen Regression “gross” erst bei 81 Datenpunkten begänne). Deshalb und da sich für grosses n AICc asymptotisch AIC nähert, sollte man einfach immer AICc verwenden.\nAIC und BIC entstammen wiederum etwas unterschiedlichen Philosophien. Auf die Unterschiede gehen wir nicht im Detail ein. Die Ergebnisse basierend auf BIC und AICc sind in dem Kontext wie wir sie hier vorstellen (BIC mit nicht-informativen priors) nahezu gleich. BIC wird relevant, wenn man informative priors verwenden kann (aber das sprengt den Kurs).\nEs gilt folgendes für AIC, AICc und BIC analog:\n\nDer absolute Wert eines Informationskriteriums ist belanglos (ob also -1000, 0.1 oder +1000000). Informationskriterien können nur im Vergleich zweier Modelle für die gleichen Daten sinnvoll angewandt werden. Dann ist das Modell mit dem niedrigeren Wert das bessere (bei gemeinsamer Betrachtung von Fit und Komplexität).\n∆i = AICi – AICmin ∆i ist die Differenz im AIC (oder eines anderen Informationskriteriums) zwischen einem bestimmten Modell i und dem jeweils besten Modell im Vergleich. Dabei wird meist die folgende Konvention verfolgt:\n\nwenn ∆i ≤ 2: Modelle sind statistisch “gleichwertig”\nwenn ∆i &gt; 4: Modell nicht relevant\n\nLikelihood von Modell gi für die Daten: \\(L = exp(-\\frac{1}{2}\\Delta_i)\\)\nEvidence ratio: (etwa: wie vielfach besser ist das beste Modell verglichen mit Modell i?) \\(ER = \\frac{L_{best}}{L_i}\\)\nAkaike weights: Normalisierte Likelihoods über alle verglichenen Modelle: \\(W_i = \\frac{exp(-\\frac{1}{2}\\Delta_i)}{\\sum{[exp(-\\frac{1}{2}\\Delta_j]}}\\)\n\n∆i, Likelihood, ER und Akaike weights stehen alle für die gleiche Information in verschiedenen Darstellungen/Transformationen. Als besonders praktisch erweisen sich die Akaike weights Wi. Nach ihrer Definition summieren sich die Akaike weights aller verglichenen Modelle zu 1. Wi kann daher als die Wahrscheinlichkeit interpretiert werden, dass Modell i unter den verglichenen Modellen das beste ist.\nDa AIC und p-Werte aus verschiedenen und nicht kompatiblen statistischen Philosophien stammen, sollte man in einer mit Informationskriterien arbeitenden Studie nicht zusätzlich auch noch p-Werte angeben. \\(R^2\\)-Werte sind dagegen in beiden “statistischen Welten” sinnvoll und wichtig.\n\n\nMultimodel inference\nDer Charme der Informationskriterien ist, dass sie sich besonders gut dann eignen, wenn man viele verschiedene Modelle vergleicht, etwa weil man ein grössere Zahl von potenziellen Prädiktoren erhoben hat, mit denen man eine abhängige Variable erklären will, etwas in einer multiplen Regression oder einer mehrfaktoriellen ANOVA oder einem sonstigen komplexen Modell. Wenn man sich ein globales Modell mit n Termen (Achsenabschnitt und neun Steigungen für Prädiktorvariablen, transformierte Prädiktorvariablen oder Interaktionen zwischen Prädiktorvariablen) vorstellt, beinhaltet das 2n Einzelmodelle für alle möglichen Kombinationen der Terme von 0 bis n Prädiktoren. Bei n = 10 wären das bereits 1024 verschiedene Modelle. Diese alle zu berechnen ist ein grosser Aufwand, weswegen man früher versucht hat, in solchen Fällen das minimal adäquate Modell in einer weniger rechenaufwändigen Weise zu finden, indem man eine stepwise forward/backward variable selection durchgeführt hat (siehe Kapitel “Modellvereinfachung” oben). Heute ist das Ausrechnen von 1000 Modellen selbst auf einem einfachen Notebook nur noch eine Sache von Sekunden, d.h. man kann seine Entscheidung effektiv auf dem Vergleich aller mit den verfügbaren Variablen möglichen Teilmodelle gründen. Die dredge-Funktion im MuMIn-Paket macht genau dieses. Bis etwa 15 Terme (d. h. 32768 zu vergleichende Modelle) funktioniert dredge auch auf einfachen Notebooks noch im Bereich weniger Minuten (aber man muss schon merklich auf das Ergebnis warten); jeder weitere Term führt aber zu einer Verdopplung der Rechenzeit.\nSchauen wir uns das anhand des schon bekannten loyn-Datensatzes (Vogelvorkommen in Waldfragmenten) an:\nlibrary(MuMIn)\nglobal.model &lt;- lm (ABUND ~ YR.ISOL + ALT + GRAZE, data=loyn)\noptions(na.action=\"na.fail\")\nallmodels &lt;- dredge(global.model)\nallmodels\nModel selection table \n     (Int)     ALT    GRA  YR.ISO df   logLik  AICc delta weight\n3   34.370         -4.981          3 -194.315 395.1  0.00  0.407\n4   28.560 0.03191 -4.597          4 -193.573 395.9  0.84  0.267\n7  -62.750         -4.440 0.04898  4 -193.886 396.6  1.46  0.196\n8  -73.580 0.03285 -4.017 0.05143  5 -193.087 397.4  2.28  0.130\n6 -348.500 0.07006        0.18350  4 -200.670 410.1 15.03  0.000\n5 -392.300                0.21120  3 -203.690 413.8 18.75  0.000\n2    5.598 0.09515                 3 -207.358 421.2 26.09  0.000\n1   19.510                         2 -211.871 428.0 32.88  0.000\nWie man sieht, wurde hier zunächst ein globales Modell mit den drei Prädiktoren YR.ISOL, ALT und GRAZE erstellt. Im nächsten Schritt wurde dann mit der dredge-Funktion dann ein Objekt allmodels generiert, das die 23 = 8 möglichen Teilmodelle enthält. In der Tabellenausgabe sieht man, dass unter diesen Modell Nr. 3, das nur einen Achsenabschnitt und GRAZE enthält mit einem Akaike weight von 0.407 das beste Modell ist. Allerdings unterscheiden sich die Modelle Nr. 4 und 7 um weniger als 2 AICc-Einheiten, sind also als praktisch gleichwertig zu betrachten. Sie haben daher auch nur etwas geringere Variable importances von 0.267 und 0.196.\nAnders als bei der frequentist statistician-Ansatz geht es nicht darum, ein einziges bestes Modell zu finden, sondern eine Aussage über ein Ensemble von plausiblen Modellen zu treffen. Es gibt hier zwei gängige Ansätze, Variable importance und Model averaging.\nVariable importance steht dabei für die Summe der Wi-Werte aller Teilmodelle, die eine bestimmte Variable enthalten. Da Wi selbst von 0 bis 1 reicht, gilt dies auch für die Variable importance. Eine Variable importance von 1 bedeutet dabei, dass alle plausiblen Modelle die entsprechende Variable beinhalten. Mithin sagt uns die Variable importance wie bedeutsam eine bestimmte Variable innerhalb der Menge der verglichenen Teilmodelle ist. Aber Achtung: Variable importance hat nichts mit Signifikanz oder p-Werten zu tun!!! Es gibt keine generelle Konvention, ab welcher Variable importance eine Variable als bedeutsam angesehen wird, aber häufig wird 50 % als Schwelle verwendet. In R geht das folgendermassen:\nimportance(allmodels)\n                     GRAZE ALT  YR.ISOL\nImportance:          1.00  0.40 0.33   \nN containing models:    4     4    4 \nWährend logischerweise jede der drei Variablen in jeweils vier Teilmodellen vorkommt, unterscheiden sie sich erheblich in der Variable importance. Alle nach der obigen Tabelle relevanten Modelle (∆i &lt; 4) enthalten GRAZE, aber nur je zwei von ihnen auch die beiden anderen Variablen. Entsprechend ist die Variable importance von GRAZE nahe 1, während sie von ALT und YR.ISOL unter 0.5 liegt.\nModel averiging ist eine andere interessante Möglichkeit des Information theoreticion-Ansatzes und der Multimodel inference. Hier werden quasi alle möglichen Modelle oder alle Modelle mit einem ∆i unter einem bestimmten Schwellenwert zu einem gemittelten Modell zusammengefasst, gewichtet nach ihrem Wi-Wert. Am Ende bekommt man eine einzige gemittelte Funktion, deren Funktionsparameter man interpretieren und die man plotten kann.\navgmodel &lt;- model.avg(get.models(dredge(model,rank=\"AICc\"),subset=TRUE))\nsummary(avgmodel)\nfull average) \n            Estimate Std. Error Adjusted SE z value Pr(&gt;|z|)    \n(Intercept) -0.29874   77.23966    78.39113   0.004    0.997    \nGRAZE       -4.64605    0.89257     0.91048   5.103    3e-07 ***\nALT          0.01282    0.02311     0.02340   0.548    0.584    \nYR.ISOL      0.01631    0.03883     0.03941   0.414    0.679 \nMan beachte, dass der Output auch einen p-Wert enthält, obwohl dieser im AIC-Kontext nicht sinnvoll ist."
  },
  {
    "objectID": "Statistik_3.html#zusammenfassung",
    "href": "Statistik_3.html#zusammenfassung",
    "title": "Statistik 3",
    "section": "Zusammenfassung",
    "text": "Zusammenfassung\n\n\n\n\n\n\nZusammenfassung\n\n\n\n\nEine ANCOVA kommt zur Anwendung, wenn auf die abhängige Variable sowohl eine kategoriale als auch eine metrische Prädiktorvariable einwirken.\nAuch eine polynomiale Regression ist ein lineares Modell und kann u. a. dazu dienen, auf einfache Weise einen unimodalen Zusammenhang zu beschreiben.\nMultiple Regressionen sind lineare Regressionen mit mehreren Prädiktoren.\nBei multiplen Regressionen muss man die weitgehende Unabhängigkeit der ins globale Modell eingespeisten Variablen sicherstellen.\nFür die Suche nach dem minimalen adäquaten Modell kommen unterschiedliche Strategien infrage, wie die schrittweise Entfernung nicht-signifikanter Terme aus dem globalen Modell oder Auswahl des besten Modells aus allen möglichen Modellen mittels AICc.\nAICc ist ein Gütemass im information theoretician approach. AICc-Werte sind nur im Vergleich mit anderen AICc-Werten für die gleichen Daten informativ; dann bezeichnet der niedrigste AICc-Wert das beste Modell.\n“Frequentist approach” (“Standardstatistik”) und “information theoretician approach” sind zwei verschiedene statistische “Philosophien”, die man nicht in ein und derselben Auswertung kombinieren sollte: also entweder p-Werte oder AICc-Werte; \\(R^2\\) macht dagegen in beiden “Welten” Sinn."
  },
  {
    "objectID": "Statistik_3.html#weiterführende-literatur",
    "href": "Statistik_3.html#weiterführende-literatur",
    "title": "Statistik 3",
    "section": "Weiterführende Literatur",
    "text": "Weiterführende Literatur\n\nCrawley, M.J. 2015. Statistics – An introduction using R. 2nd ed. John Wiley & Sons, Chichester, UK: 339 pp.\n\nChapter 7: Regression (pp. 140–141)\nChapter 9: Analysis of Covariance\nChapter 10: Multiple Regression\nChapter 12: Other Response Variables (p. 233 [AIC])\n\nBurnham, K.P. & Anderson, D.R. 2002. Model selection and multimodel inference – a practical information-theoretic approach. 2nd ed. Springer, New York, US: 488 pp.\nJohnson, J.B. & Omland, K.S. 2004. Model selection in ecology and evolution. Trends in Ecology and Evolution 19: 101–108.\nLogan, M. 2010. Biostatistical design and analysis using R. A practical guide. Wiley-Blackwell, Oxford, UK: 546 pp., v.a.\n\npp. 208-253 (Multiple und nicht-lineare Regressionen)\n\nQuinn, P.Q. & Keough, M.J. 2002. Experimental design and data analysis for biologists. Cambridge University Press, Cambridge, UK: 537 pp."
  },
  {
    "objectID": "Statistik_4.html#lernziele",
    "href": "Statistik_4.html#lernziele",
    "title": "Statistik 4",
    "section": "Lernziele",
    "text": "Lernziele\n\n\n\n\n\n\nIhr…\n\nhabt den Unterschied zwischen linearen und nicht-linearen Regressionen verstanden und könnt eine einfache nicht-lineare Regression in R implementieren;\nhabt verstanden, worin sich GLMs von linearen Regressionen unterscheiden und wann sie zur Anwendung kommen; könnt die beiden häufigsten GLM-Typen logistische Regression und (Quasi-) Poisson-Regression in R richtig anwenden und die Ergebnisse interpretieren; und\nwisst, wofür LOWESS und GAM stehen und wie man sie anwendet."
  },
  {
    "objectID": "Statistik_4.html#von-linearen-modellen-zu-glms",
    "href": "Statistik_4.html#von-linearen-modellen-zu-glms",
    "title": "Statistik 4",
    "section": "Von linearen Modellen zu GLMs",
    "text": "Von linearen Modellen zu GLMs\n\nZwei Beispiele\nNehmen wir an, wir wollten modellieren, wie viele Besucher an einem Strandabschnitt zur Mittagszeit in Abhängigkeit von der herrschenden Lufttemperatur anzutreffen sind. Unsere Daten sehen folgendermassen aus und mit den bekannten Methoden können wir ein lm rechnen, dessen Ergebnis signifikant ist und sogar recht viel der Gesamtvarianz erklärt:\n\n\n\n\n\n\n\n\n\n\nUnsere abhängige Variable ist eine Zählung und verhält sich daher anders als eine echte metrische Variable (etwa einer Messung des pH-Wertes). Zähldaten stellen lineare Modelle (lm) vor vier Probleme:\n\nLineare Modelle sagen immer auch das Auftreten negativer Werte voraus, wohingegen absolute Häufigkeiten immer positive Ganzzahlen sind (im obigen Beispiel würde das Modell bereits im gefitteten Bereich, unter etwa 12 °C, negative Menschen vorhersagen).\nNahezu immer sind Zähldaten rechtsschief verteilt, also nicht normalverteilt und auch nicht symmetrisch\nBei Zähldaten nimmt nahezu immer die Varianz mit dem Mittelwert zu.\nZähldaten folgen keiner kontinuierlichen (wie die Normalverteilung), sondern einer diskreten Verteilung.\n\nTheoretisch sind also die Voraussetzungen für ein lineares Modell bei Zähldaten nie erfüllt. In der Praxis gibt es aber Situationen, wo die Verletzung der Annahmen für das Modell nicht weiter problematisch ist und man mit einem lm zu korrekten Aussagen gelangen kann. Relativ problemlos funktioniert das (und wird auch noch häufig getan), wenn (a) alle Werte der Antwortvariablen weit von 0 entfernt sind und (b) die Werte der Antwortvariable um deutlich weniger als eine Grössenordnung (d.h. Faktor 10) variieren. Im obigen Beispiel beträgt der Quotient des grössten und kleinsten Wertes der Antwortvariablen 2000 / 12 = 167. Mit etwas Erfahrung sehen wir schon im Scatterplot, dass hier Linearität und Varianzhomogenität verletzt sind.\nEin anderes Beispiel, bei dem ein lineares Modell offensichtlich und immer scheitern würde, wäre eine Befragung von Touristen an Tagen unterschiedlicher Temperatur, ob sie schwimmen gegangen sind. Das Ergebnis könnte wie folgt aussehen (stark gekürzte Tabelle, an jedem Tag (d.h. bei gleicher Temperatur) wurden jeweils mehrere Touristen befragt):\n\n\n\n\n\nBei solchen “binären Daten” bestehen zwei hauptsächliche Probleme für lineare Modelle:\n\nDie Werteverteilung ist nach unten und nach oben begrenzt.\nEs gibt überhaupt nur zwei mögliche Werte, nein und ja, als 0 und 1 codiert.\n\n\n\nDie Idee der Generalized linear models (GLMs)\nGeneralized linear models (GLMs) verallgemeinern lineare Modelle (LMs), um Fälle wie die geschilderten (Zähldaten, Binärdaten, für weitere Beispiele siehe Crawley (2015)) modellieren zu können. “Generalisiert” heissen die GLMs aus folgenden drei Gründen:\n\nAlle LMs sind im Begriff GLM eingeschlossen (aber viele GLMs sind keine LMs).\nDie Verteilung der “Zufallskomponente” (= Residuen) kann sich von einer Normalverteilung unterscheiden (muss aber aus der exponentiellen Familie von Verteilungen sein).\nDie abhängige Variable kann auf verschiedene Weise mit den Prädiktoren verknüpft (linked) sein.\n\n\n\nDie drei Komponenten eines GLM\nEin GLM setzt sich aus drei Komponenten zusammen, die relativ frei kombiniert werden können (aber für bestimmte Zufallskomponenten gibt es Standard-Link-Funktionen):\n\nZufallskomponente (d. h. die Verteilung der Residuen):\n\nnormal\nbinomial: z. B. ja/nein, tot/lebendig\nPoisson: Zähldaten (funktioniert aber nicht immer)\ngamma\nnegativ binomial (Dispersionsparameter muss geschätzt werden)\n\nSystematische Komponente (d. h. die x-Werte): es ist alles möglich, was wir schon von LMs her kennen:\n\nkontinuierliche (metrische) Prädiktoren\nkategoriale Prädiktoren\nInteraktionen von Prädiktoren\npolynomiale Funktionen\njewede Kombination aus den vorhergehenden Elementen\n\nLink-Funktion:\n\nIdentität (identity)\nlog (für Zähldaten)\nlogit (für Binärdaten)\n\n\n\n\nMögliche Verteilungen von Werten und von Varianzen\nWas mit verschiedenen Verteilungen der Residuen gemeint ist, veranschaulichen die folgenden beiden Abbildungen von vier Häufigkeitsverteilungen mit dem gleichen Mittelwert. Oben sind die kontinuierliche Normalverteilung und unten drei unterschiedliche diskrete Verteilungen (Poisson, negativ-binomial) zu sehen:\n\n\n\n\n\nAuch die Beziehung von Varianzen zum (vorhergesagten) Mittelwert müssen keinesfalls immer konstant sein, wie wir das von den linearen Modellen kennen. Vielmehr zeigen viele Datentypen eine systematische Veränderung der Varianz mit dem Mittelwert:\n\n\n\n(aus Crawley 2015)\n\n\n\n\nTypen von GLMs\nEine Übersicht gängige GLM-Typen bietet die folgende Tabelle (man beachte die uneinheitliche Gross-/Kleinschreibung der Verteilungen):\n\n\n\nübersetzt und modifiziert nach Šmilauer 2017\n\n\nMan beachte, dass ein GLM mit Normalverteilung (gaussian) und identity-Link identisch mit einem LM ist.\nWenn man dieser Anleitung strikt folgen würde (was auch Smilauer 2017 nicht tut), dürfte man LMs nur dann verwenden, wenn die Antwortvariable auch negative Werte annehmen kann und ansonsten ein Gamma-GLM rechnen. In Realität werden Gamma-GLMs aber fast ausschliesslich für death and failure-Daten verwendet, bei denen die Varianz mit dem Quadrat des Mittelwertes zunimmt.\nGLMs mit binomialer, Poisson, Gamma- und Gauss (Normal)-Verteilung sind in Base R implementiert, für negative.binomial benötigt man das Package MASS. In diesem Kurs gehen wir im Detail nur auf die beiden meistbenutzten GLM-Typen ein, Poisson-Regression für Zähldaten und logistische Regression für Binärdaten. Mehr zu den übrigen Typen findet man u. a. in Crawley (2015), Dunn & Smyth (2018) und Fox & Weisberg (2019)\n\n\nDas Fitten und die Modellgüte von GLMs\nBei einem linearen Modell (LM) wird die Lösung durch Minimierung der Summe der Abweichungsquadrate erzielt. Diese Lösung lässt sich direkt, immer eindeutig und sogar von Hand ausrechnen. GLMs dagegen fitten die Modelle in einem iterativen Verfahren, indem die Likelihood maximiert wird. Deswegen spricht man auch von Maximum likelihood (ML). Nach erfolgtem Fitten werden die Werte mit der Umkehrfunktion der Link-Funktion auf die originale Skala zurücktransformiert.\nAls Mass der Variabilität oder lack of fit wird bei GLMs die Devianz D verwendet, die folgendermassen definiert ist:\n\\[\nD_i = -2 × \\text{log likelihood} (\\text{Modell}_i | \\text{Daten})\n\\]\nJe nach GLM-Typ wird die Devianz anders berechnet:\n\n\n\n(aus Crawley 2015)"
  },
  {
    "objectID": "Statistik_4.html#poisson-regressionen-für-zähldaten",
    "href": "Statistik_4.html#poisson-regressionen-für-zähldaten",
    "title": "Statistik 4",
    "section": "Poisson-Regressionen für Zähldaten",
    "text": "Poisson-Regressionen für Zähldaten\n\nBerechnung\nDie Struktur des glm-Befehls in R ist genau identisch mit jenem des lm-Befehls. Nur muss man zusätzlich die Verteilung (family) und ggf. die Link-Funktion (wenn nicht die Standard-Link-Funktion der jeweiligen Verteilung) angeben. Schauen wir uns nun die Ergebnisse für unsere Zähldaten der Strandbesucher an, zunächst mit einem LM, dann mit einem Gauss-GLM und schliesslich mit einem Poisson-GLM:\nlm.strand &lt;- lm(Besucher~Temperatur)\nglm.gaussian &lt;- glm(Besucher~Temperatur,family=gaussian)\nglm.poisson &lt;- glm(Besucher~Temperatur,family=poisson)\n\nsummary(lm.strand)\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -855.01     290.54  -2.943 0.021625 *  \nTemperatur     67.62      11.80   5.732 0.000712 ***\nsummary(glm.gaussian)\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -855.01     290.54  -2.943 0.021625 *  \nTemperatur     67.62      11.80   5.732 0.000712 ***\nsummary(glm.poisson)\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) 3.500301   0.056920   61.49   &lt;2e-16 ***\nTemperatur  0.112817   0.001821   61.97   &lt;2e-16 ***\nWie nach den Erläuterungen im vorigen Kapitel zu erwarten war, sind die Ergebnisse des LMs und des Gauss-GLMs vollkommen identisch. Jene des Poisson-GLMs sind dagegen anders, insbesondere viel höher signifikant.\n\n\nInterpretation und Visualisierung der Ergebnisse\nIm Falle des lm können wir aus den Parameter-Schätzungen (Spalte Estimate im summary) direkt die sich ergebende Funktionsgleichung aufschreiben:\n\\[\n\\text{Besucher} = -855 + 68 \\times \\text{Temperatur}/\\text{°C}\n\\]\nBei einem glm sind die Parameter-Schätzungen dagegen nicht direkt interpretierbar, da sie sich auf eine transformierte Skala beziehen, welche durch die Link-Funktion angegeben ist. Die Standard-Link-Funktion bei einem Poisson-GLM ist log, also der natürliche Logarithmus (ln). Unser Ergebnis lässt sich damit wie folgt schreiben:\n\\[\n\\ln(\\text{Besucher}) = 3.50 + 0.11 \\times \\text{Temperatur}/\\text{°C}\n\\]\nDa uns aber nicht ln (Besucher), sondern die Besucherzahl selbst interessiert, müssen wir die Umkehrfunktion der Link-Funktion anwenden, bei ln also exp. Es ergibt sich:\n\\[\n\\text{Besucher} = \\exp(3.50 + 0.11 \\times \\text{Temperatur}/\\text{°C})\n\\]\nDamit können wir auch die vorhergesagten Werte für verschiedene Temperaturen berechnen:\n\\[\\begin{align*}\n0 \\text{°C}: \\text{Besucher} &= \\exp(3.50) = 33 \\\\\n30 \\text{°C}: \\text{Besucher} &= \\exp(3.50 + 30 \\times 0.11) = \\exp(6.83) = 925\n\\end{align*}\\]\nWenn wir das Ganze Plotten wollen, benötigen wir den predict- und den lines-Befehl. Wie man, sieht muss auch hier auf die vorhergesagten Werte beim Plotten noch die Umkehrfunktion (exp) angewandt werden:\nxv &lt;- rep(0:40,by=.1)\nplot(Temperatur,Besucher,xlim=c(0,40))\nyv &lt;- predict(lm.strand,list(Temperatur=xv))\nlines(xv, yv,lwd=3,col=\"blue\")\nyv2 &lt;- predict(glm.poisson,list(Temperatur=xv))\nlines(xv, exp(yv2),lwd=3,col=\"red\")\n\n\n\n\n\n\n\nOverdispersion als Problem\nMathematisch beschreibt die Poisson-Verteilung Ereignisse pro Zeiteinheit, wenn sie mit einer bestimmten Rate (Mittelwert) erfolgen, die Ereignisse selbst aber unabhängig voneinander sind. Für ökologische/umweltwissenschaftliche Zähldaten sind diese Voraussetzungen oft nicht exakt gegeben, sie folgen daher nicht immer genau einer Poisson-Verteilung, sondern weisen teilweise eine Overdispersion auf. Für eine Poisson-Regression wird eine \\(\\text{Dispersion} = \\frac{\\text{Residual deviance}}{\\text{Residual degrees of freedom}} = 1\\) angenommen. Wenn die Dispersion wesentlich/signifkant grösser als 1 ist, liegt Overdispersion vor. Residual deviance und Residual degrees of freedom findet man im summary des glm:\nsummary(glm.poisson)\n[…]\n(Dispersion parameter for poisson family taken to be 1)\n    Null deviance: 6011.8  on 8  degrees of freedom\nResidual deviance: 1113.7  on 7  degrees of freedom\nAIC: 1185.1\nMan sieht hier, dass der Quotient von 1113.7 und 7 weit höher als 1 ist. Mit dem Dispersionstest im Package AER kann man formal auf einen signifikanten Unterschied testen:\nlibrary(AER)\ndispersiontest(glm.poisson)\ndata:  glm.poisson\nz = 3.8576, p-value = 5.726e-05\nalternative hypothesis: true dispersion is greater than 1\nsample estimates:\ndispersion \n  116.5467 \nWenn man eine signifikante Overdispersion gefunden hat, gibt es zwei Lösungsmöglichkeiten:\n\nQuasi-Poisson-Verteilung: Hierbei schätzt der Algorithmus den Dispersionsparameter aus den Daten und passt die angenommene Verteilung entsprechend an. Die Methode ist im Befehl glm in Base R implementiert:\nglm.quasi &lt;- glm(Besucher~Temperatur,family=quasipoisson)\nsummary(glm.quasi)\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)  3.50030    0.69639   5.026  0.00152 **\nTemperatur   0.11282    0.02227   5.065  0.00146 **\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n(Dispersion parameter for quasipoisson family taken to be 149.6826)\nMan sieht, dass im Vergleich zur Berechnung mit einem einfachen Poisson-GLM die Parameterschätzungen nicht verändert haben, jedoch die Signifikanzen niedriger ausgefallen sind (d. h. höhere p-Werte).\nNegativ-binomiale Verteilung: Oftmals erzielt man damit ähnliche, in besonderen Fällen allerdings auch deutlich andere Ergebnisse. Was besser ist, hängt vom Einzelfall ab und ist u. U. recht “tricky”. Weitere Details, siehe Ver Hoef & Boveng (2007)."
  },
  {
    "objectID": "Statistik_4.html#logistische-regressionen-für-binärdaten",
    "href": "Statistik_4.html#logistische-regressionen-für-binärdaten",
    "title": "Statistik 4",
    "section": "Logistische Regressionen für Binärdaten",
    "text": "Logistische Regressionen für Binärdaten\nLogistische Regressionen werden für alle binären Antwortvariablen verwendet, etwa für Vorkommensdaten (Inzidenzdaten). Das folgende Abbildungspaar zeigt links, was passieren würde, wenn man solche Daten mit einem lm fitten würde und rechts, die korrekte Modellierung mit einem logistischen glm:\n\n\n\n(aus Logan 2010)\n\n\n\nPrinzipielles Vorgehen\n\nDie abhängige Variable muss als Vektor vorliegen, der entweder nur 0 und 1 enthält oder aber ein Faktor mit genau zwei Level ist.\nEs wird ein glm mit family=binomial gerechnet.\nDer voreingestellte Link ist logit, alternativ geht auch log-log.\nOverdispersion ist bei Binärdaten nicht relevant.\nWie bei allen (multiplen) Modellen müssen wir eine Modellvereinfachung des vollen Modells vornehmen, wofür im Prinzip die gleichen drei Methoden zur Verfügung stehen, die wir schon kennen:\n\nModellselektion I: sukzessive Vereinfachung durch Entfernen nicht-signifkanter Terme.\nModellselektion II: sukzessive Vereinfachung mittels Vergleich der Devianzen zweier Modelle mit Chi-Quadrat-Test (Achtung: Unterschied zu lm, wo wir eine ANOVA, d. h. eine F-Test verwendet haben).\nModellselektion III: mittels AICc: Berechnung aller möglichen Modelle und dann entweder Auswahl jenes mit dem niedrigsten AICc oder Multimodel inference.\n\n\n\n\nDie Theorie dahinter\nDas “logit” (L) ist ein zentrales Element der logistischen Regression. Ein logit ist als der natürliche Logarithmus eines “odds” definiert. “Odds” hatten wir schon kurz beim Vierfelder-Assoziationstest (Chi-Quadrat- bzw. Fishers exakter Test). Sie bezeichnen die Wahrscheinlichkeit eines Ereignisses durch die “Gegenwahrscheinlichkeit”. Es gilt also Folgendes:\n\\[\nL = \\ln\\left( \\frac{p}{1 - p} \\right)\n\\]\nWarum arbeitet man mit “odds” und “logits”? Wenn man nur p modellieren würde, wären die möglichen Werte auf 0 … 1 begrenzt. “Odds” dagegen können Werte zwischen 0 und ∞ annehmen. Der Logarithmus schliesslich sorgt für eine symmetrische Verteilung der originalen Wahrscheinlichkeiten unter 50 % (jetzt zwischen –∞ und 0) und der originalen Wahrscheinlichkeiten über 50 % (jetzt zwischen 0 und +∞).\nBei GLMs wir ja immer die abhängige Variable mit der Link-Funktion transformiert. Damit modelliert eine logistische Regression das folgende Modell (in einer multiplen logistischen Regression ggf. auch mit x1, x2 usw.):\n\\[\n\\ln\\left( \\frac{\\pi(y)}{1 - \\pi(y)} \\right) = \\ \\beta_{0} + \\beta_{1}x\n\\]\n\n\nModelldiagnostik und Ergebnisse\nDie Beurteilung von Validität und Güte/Relevanz eines logistischen Modells unterscheidet sicher erheblich von einem lm:\n\nEine visuelle Inspektion der Residualplots ist hier nicht informativ.\nEs gibt diverse numerische Goodness-of-fit-Tests für das Modell, am einfachten der Vergleich der Abweichung der Devianz (\\(G^2\\)) von der geforderten \\(\\chi^2\\)-Verteilung.\nDas konventionelle Gütemass \\(R^2\\) funktioniert ebenfalls nicht. Statt dessen kann man die Modellgüte mit einem Pseudo-\\(R^2\\) ausdrücken:\n\n\\[\nR^2= 1 - \\frac{\\text{Devianz Total}}{\\text{Devianz Residuen}}\n\\]\nDa nicht die abhängige Variable (d. h. die Auftretenswahrscheinlichkeit), sondern ihr logit modelliert wurde, muss man die beiden Parameterschätzungen erst in informative Grössen übersetzen. Es sind dies:\n\nLagemass (d. h. bei welchem x1-Wert ist die Wahrscheinlichkeit von 0 und 1 gleich hoch; auch als “LD50” = “lethal” does for 50% of the individuals” bezeichnet, basierend auf Anwendungen on logistischen Regressionen in Toxizitätstests): \\(- \\beta_0 / \\beta_0\\)\nSteilheitsmass (d.h. wie scharf/steil ist der Übergang von 0 zu 1, ausgedrückt als die relative Änderung der “odds” bei Zunahme von x1 um eine Einheit): \\(\\exp(\\beta_1)\\)\n\n\n\nUmsetzung in R\nSchauen wir uns diese ganzen Schritte im Fall unseres Bade-Beispiels an, also der Wahrscheinlichkeit, dass eine Person am Strand schwimmen geht in Abhängigkeit von der Temperatur. Die Definition des Modells in R ist wie gehabt einfach:\nmodel &lt;- glm(bathing~temperature,data=bathing,family=\"binomial\")\nsummary(model)\nCoefficients\n            Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)  -5.4652     2.8501  -1.918   0.0552 .\ntemperature   0.2805     0.1350   2.077   0.0378 *\nDie uns interessierenden Aspekte Modelldiagnostik, Modellgüte und Kurvenverlauf müsse wir uns daher erst händisch aus dem abgespeicherten Objekt model extrahieren, indem wir auf einzelne darin abgespeicherte Daten zurückgreifen:\n# Modeldiagnostik (wenn nicht signifikant, dann OK)\n1 - pchisq (model$deviance,model$df.resid)\n[1] 0.6251679\n# Modellgüte (pseudo-R2)\n1 - (model$dev / model$null)\n[1] 0.4775749\n# Steilheit der Beziehung (relative Änderung der \n# odds von x + 1 vs.x)\n\nexp(model$coefficients[2])\ntemperature\n1.323807\n# LD50 (also hier: Temperatur, bei der 50% der Touristen baden)\n-model$coefficients[1]/model$coefficients[2]\n(Intercept)\n19.48311\nDer erste Wert gibt die Steilheit der Beziehung an und ob sie ansteigend oder fallend ist, wobei 1 keinen Effekt, &gt;1 eine ansteigende Häufigkeit und &lt; 1 eine fallende Häufigkeit bezeichnen. Der zweite Wert (man beachte das Minus-Zeichen in der Formel!) gibt den x-Wert an, für den die berechnete Wahrscheinlichkeit (Vorkommenswahrscheinlichkeit, Sterbewahrscheinlichkeit, usw.) genau 50 % ist.\nGanz einfach vorzustellen ist eine logistische Funktion auch mit diesen Werten noch nicht. Deswegen sollten wir im Falle signifkanter logistischer Regressionen immer zwei Dinge tun: (1) Die Funktionsgleichung angeben und (2) Das Ergebnis visualisieren.\n\nDie Funktionsgleichung zu extrahieren, ist etwas vertrackt, da wir ja nicht die Auftretenswahrscheinlichkeit y, sondern ihren logit modelliert haben. Übersetzt bedeuten die Estimate-Werte unseres summary also:\n\n\\[\n\\ln{(y/1-y}) = b_0 + b_1x\n\\]\nWir formen sukzessive um, um nach y aufzulösen:\n\\[\\begin{align*}\n\\ln{(y/1-y}) &= b_0 + b_1x \\\\\n\\\\\ny/(1-y) &= \\exp(b_0 + b_1x)\\\\\n\\\\\ny &= (\\exp(b_0 + b_1 x))(1-y) \\\\\n\\\\\ny + y \\exp(b_0+b_1 x) &= \\exp(b_0 + b_1 x)\\\\\n\\\\\ny (1 + \\exp(b_0+b_1 x)) &= \\exp(b_0 + b1 x)\\\\\n\\\\\ny &= \\exp(b_0 + b_1 x)/(1+ \\exp(b_0+b_1x))\n\\end{align*}\\]\nOder mit den Werten in unserem Fall:\n\\[\ny = \\exp(-5.47 + 0.28 x)/(1+ \\exp(-5.47 + 0.28 x))\n\\]\n\nDas Visualisieren geht relativ einfach mit dem predict-Befehl (hier einschliesslich Standardfehler):\n\nxs &lt;- seq(0,30,l=1000)\nmodel.predict &lt;-predict(model,type=\"response\",se=T,newdata=data.frame(temperature=xs))\nplot(bathing~temperature,data=bathing,xlab=\"Temperature (°C)\",ylab=\"% Bathing\",pch=16, col=\"red\")\npoints(model.predict$fit ~ xs,type=\"l\")\nlines(model.predict$fit+model.predict$se.fit ~ xs, type=\"l\",lty=2)\nlines(model.predict$fit-model.predict$se.fit ~ xs, type=\"l\",lty=2)"
  },
  {
    "objectID": "Statistik_4.html#nicht-lineare-regressionen",
    "href": "Statistik_4.html#nicht-lineare-regressionen",
    "title": "Statistik 4",
    "section": "Nicht-lineare Regressionen",
    "text": "Nicht-lineare Regressionen\n\nBeispiele\nNicht-lineare Regressionen finden für funktionelle Beziehungen Anwendung, bei der sich die abhängige Grösse nicht als Linearkombination der Prädiktorvariable(n) darstellen lässt, z. B. wenn diese in Potenzen oder Quotienten auftaucht. (Eine polynomiale Regression ist dagegen, wie wir gesehen haben, immer noch ein lineares Modell, wenngleich eine nicht-lineare Beziehung modelliert wird.)\nZwei häufige Anwendungen nicht-linearer Regressionen sind die Potenzfunktion und verschiedene Sättigungsfunktionen:\nBeispiel 1: Potenzfunktion\n\\(y=b_0x^b\\), oft auch als \\(y = c x^z\\)\n\nDieses dürfte die am häufigeten verwendet nicht-lineare Funktion sein; sie tritt in fast allen Wissensdisziplinen auf (Nekola & Brown 2007).\nb0 bzw. c bezeichnen dabei den vorhergesagten Wert der abhängigen Variable, wenn die unabhängige den Wert 1 hat (da log (1) = 0); der Exponent b1 bzw. z beschreibt dagegen die Geschwindigkeit der relativen Zunahme (z = 1 wäre eine lineare Beziehung).\nSolange nicht-lineare Regressionen nicht als einfach verfügbares statistisches Tool bereitstanden, wurden Potenzgesetze durch Logarithmierung beider Achsen in eine lineare Beziehung überführt und mit linearen Modellen analysiert (log y = log b0 + b1 log x).\nDas geht gut, solange keine Nullwerte von y vorliegen (für die der Logarithmus nicht definiert wäre).\nMan muss aber beachten, dass sich die Ergebnisse unterscheiden, je nachdem, ob man y oder log (y) als abhängige Variable hat. Die beiden Parameterschätzungen sind meist ähnlich, p- und \\(R^2\\)-Werte können sich dagegen erheblich unterscheiden und sind zwischen beiden Herangehensweisen nicht vergleichbar. Je nach Situation können aber beide ihre Berechtigung haben (vgl. Dengler 2009).\n\nBeispiel 2: Sättigungsfunktionen\n\nSogenannte Sättigungsfunktionen finden Anwendung, wenn es nach der Theorie einen oberen Grenzwert für y gibt, dem sich die Funktion mit zunehmendem x asymptotisch annähert.\nEine aus der Enzymkinetik stammende, wegen ihrer Einfachheit aber auch in diversen anderen Disziplinen angewandte Sättigungsfunktion ist die Michaelis-Menten-Funktion:\n\\(y = \\frac{b_0}{b_1+x}\\)\nHierbei steht b0 für den oberen Grenzwert, b1 steht für die Steilheit des Anstiegs.\nEs gibt zahlreiche weitere Sättigungsfunktionen, etwa auch eine Verallgemeinerung der logistischen Funktion (die wir als eines der GLM-Modelle kennengelernt haben). Siehe dazu das Unterkapitel “Umsetzung in R” unten.\n\n\n\nUnterschiede von linearen und nicht-linearen Regressionen\n\n\n\n\n\n\nLineare Regression\n\n\n\n\\[\nY \\backsim \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3 + ...\n\\]\n\\(X_i\\) kann sein\n\nein einzelner Prädiktor\nein transformierter Prädiktor, z.B. \\(\\log(X),X^2\\)\neine Interaktion \\(X_j \\times X_k\\)\n\n\n\n\n\n\n\n\n\nNicht-lineare Regression\n\n\n\n\\[\nY \\backsim \\text{beliebige Funktion von } X_1, X_2, ...\n\\]\n“beliebige Funktion” schliesst ein:\n\nVerhältnisse, z. B.: \\(\\frac{1}{X}; \\frac{X_i}{X_j}\\)\nPotenzen, z. B. \\(X^b, b^X\\)\nbreakpoints, z. B.: \\(\\text{for} X &lt; b: ...; \\text{for} X \\geq b\\)\n\n\n\nBei der Berechnung von linearen vs. nicht-linearen Regressionen gelten folgende Besonderheiten:\n\nLineare Regressionen haben eindeutige Ergebnisse, die direkt berechnet werden können.\nErgebnisse nicht-linearer Regressionen sind nicht direkt analytisch zugängiglich, sondern nur über eine iterative Optimierungsprozedur. Das hat folgende Implikationen:\n\nFür die Iteratation sind Startwerte und (anfängliche) Schrittweiten erforderlich\nMan weiss nie sicher, ob man das globale Optimum gefunden hat (oder in einem lokalen Optimum geendet ist).\nBei ungünstig gewählten Startwerten konvergiert die Iteration möglicherweise gar nicht.\n\n\n\n\nUmsetzung in R\nDer Befehl für nicht-lineare Regressionen ist nls, seine Syntax ganz ähnlich zu lm und glm. Die zu schätzenden Parameter muss man selbst benennen. Da die Lösung iterativ gefunden wird, muss man dem Befehl Startwerte für diese Parameter mitgeben.\nMan kann beliebige Funktionen selbst definieren, hier gezeigt am Beispiel einer Potenzfunktion:\n# Selbsdefinierte Funktionen #\npower.model &lt;- nls(ABUND~c*AREA^z, start=(list(c=0,z=1)))\nsummary(power.model)\nFormula: ABUND ~ c * AREA^z\nParameters:\n  Estimate Std. Error t value Pr(&gt;|t|)    \nc 13.39416    1.30721  10.246 2.87e-14 ***\nz  0.16010    0.02438   6.566 2.09e-08 ***\nOder man greift auf die in R bereits vordefinierten Funktionen (sogenannte Selbststartfunktionen [SS] zurück). Hier am Beispiel der logistischen Funktion als einer möglichen Sättigungsfunktion gezeigt (Man beachte, dass diese logistische Funktion nicht identisch mit jener aus der logistischen Regression ist, da wir es (a) mit einer nicht-binären Antwortvariable zu tun haben und (b) der Sättigungswert nicht automatisch 1 ist, sondern aus den Daten geschätzt wird). Mehr zu Selbststartfunktionen von nls findet man in der R-Hilfe, im Buch von Ritz & Streibig (2008), sowie dem Auszug daraus, der in Moodle bereitsteht.\n# Vordefinierte \"Selbststartfunktionen\" #\nlogistic.model &lt;- nls(ABUND~SSlogis(AREA, Asym, xmid, scal))\nsummary(logistic.model)\nFormula: ABUND ~ SSlogis(ABUND, Asym, xmid, scal)\nParameters:\n     Estimate Std. Error t value Pr(&gt;|t|)    \nAsym   31.306      2.207  14.182  &lt; 2e-16 ***\nxmid    6.501      2.278   2.854  0.00614 ** \nscal    9.880      3.152   3.135  0.00280 ** \nDie grösste Herausforderung bei nls sind die Startwerte, da bei ungeeigneten Startwerten, das Modell möglicherweise gar nicht konvegiert oder in einem lokalen Optimum hängen bleibt und das globale Optimum nicht findet. Hier ist es wichtig, ein gutes Verständnis für die Funktionsparameter der jeweiligen Funktion zu haben und damit eine Erwartungshaltung, wie gross sie im Allgemeinen sind bzw. wie gross sie im konkreten Fall sein könnten. Für’s Allgemeine können wir die Theorie und ähnliche Untersuchungen in der Literatur konsultieren. Für den z-Wert einer Artenzahl-Areal-Beziehung, die mit Potenzgesetz modelliert wird, sagt uns die Theorie, dass dieser zwischen 0 und 1 liegen muss, und empirische Ergebnisse zeigen, dass er meist zwischen 0.2 und 0.25 liegt. Wenn wir hier also einen Startwert für z von –1 oder 1000 eingeben würden, hätte nls vermutlich ein Problem und würde kein Ergebnis oder ein falsches Ergebnis ausspucken. Bei der logistischen Regression wissen wir, dass der Parameter Asym für den Sättigungswert steht. In unserem Fall wäre also die maximale tatsächliche Artenzahl ein brauchbarer Startwert, den wir mit Blick auf die Originaldaten (summary oder Scatterplot) ermitten können.\nWenn wir zwischen unterschiedlichen nicht-linearen Modellen auswählen wollen, dann kommen dafür nur die Informationskriterien in Frage, da eine ANOVA hier nicht funktioniert (diese funktioniert nur für geschachtelte Modelle). Wollen wir unsere beiden zuvor berechneten Modelle vergleichen, brauchen wir das Package AICcmodavg:\nlibrary(AICcmodavg)\ncand.models &lt;- list()\ncand.models[[1]] &lt;- power.model\ncand.models[[2]] &lt;- logistic.model\nModnames &lt;- c(\"Power\", \"Logistic\")\naictab(cand.set = cand.models, modnames = Modnames)\n         K   AICc Delta_AICc AICcWt Cum.Wt      LL\nLogistic 4 386.86       0.00   0.99   0.99 -189.04\nPower    3 396.17       9.31   0.01   1.00 -194.86\nIn unserem Fall wäre also das logistische Modell trotz einem zusätzlichen gefitteten Parameter (k = 4 statt k = 3; hier ist die geschätzte Varianz mitgezählt) das klar bessere Modell (Akaike Weight von 0.99)."
  },
  {
    "objectID": "Statistik_4.html#glättungsfunktionen-und-gams",
    "href": "Statistik_4.html#glättungsfunktionen-und-gams",
    "title": "Statistik 4",
    "section": "Glättungsfunktionen und GAMs",
    "text": "Glättungsfunktionen und GAMs\n\nGlättungsfunktionen\nGlättungsfunktionen (smoother) sind keine statistischen Verfahren** im eigentlichen Sinn. Vielmehr dienen sie der Visualisierung eines komplexen Zusammenhanges und können so helfen, geeignete inferenzstatistische Verfahren auszuwählen. Es gibt zahlreiche solche smoother:\n\nGleitender Median\nLOESS\nLOWESS\nKernel\nSplines\n[…]\n\nAnhand von LOWESS (Locally weighte scatterplot smoothing) soll gezeigt werden, was ein smoother macht. In der Regel hat eine Glättungsfunktion zumindest einen wählbaren Parameter, welcher bestimmt, wie stark die Glättung ausfällt, im Fall von LOWESS ist dies f:\nplot(ABUND~log_AREA)\nlines(lowess(log_AREA,ABUND,f=0.25), lwd=2, col=\"red\")\nlines(lowess(log_AREA,ABUND,f=0.5), lwd=2, col=\"blue\")\nlines(lowess(log_AREA,ABUND,f=1), lwd=2, col=\"green\")\n\n\n\n\n\n\n\nGAMs (Generalized additive models)\nGeneralised additive modesls (GAMs) arbeiten auf den ersten Blick ähnlich wie Smoother, doch handelt es sich bei GAMs um ein inferenzstatistisches Verfahren:\n\nBei einem GAM handelt es sich im Prinzip um ein lineares Modell (oder ein GLM), bei dem die einzelnen Parameter nicht fix, sondern eine smoothing function sind:\n\\(y = \\beta_0 + f_1 (x) + f_2 (x) + ...\\)\nMan bekommt ein Modell mit den üblichen Gütemassen wie p oder AICc.\nDie Freiheitsgrade sind geschätzt und nicht ganzzahlig.\nMan muss smoothing function und smoothing parameter definieren.\n(Man muss auch Link-Funktion und Wahrscheinlichkeitsverteilung angeben, wie bei GLMs).\n\nIn R geht das folgendermassen (für den gleichen Datensatz, über den wir vorhin die Smoother haben laufen lassen). Da das Festlegen der smoothing parameter eine Kunst für sich ist, nehmen wir hier die default-Werte des Programms.\nlibrary(mgcv)\nmodel &lt;- gam(ABUND~s(log_AREA))\nsummary(model)\nParametric coefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  19.5143     0.9309   20.96   &lt;2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\nApproximate significance of smooth terms:\n              edf Ref.df     F  p-value    \ns(log_AREA) 2.884  3.628 21.14 6.63e-11 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\nR-sq.(adj) =  0.579   Deviance explained = 60.1%\nGCV = 52.145  Scale est. = 48.529    n = 56\nWie wir sehen, bekommen wir für die Beziehung geschätzte Freiheitsgrade und einen geschätzten p-Wert. Der eigentliche Kurvenverlauf wird dagegen nicht in Parametern ausgedrückt und ist nicht direkt zugänglich. Wir können ihn jedoch plotten:\nplot(log_AREA, ABUND, pch=16) \nxv &lt;- seq(-1, 4, by=0.1) \nyv &lt;- predict(model, list(log_AREA=xv)) lines(xv, yv, lwd=2, col=\"red\")\n\n\n\n\n\nZusammenfassend lässt sich sagen, dass GAMs zwar zu den inferenzstatistischen Verfahren gehörten, aber anders als alle anderen derartigen Verfahren, die wir im Kurs kennenlernen kein direkt zugängliches und interpretierbares Modell auspucken. Es ist also kaum möglich, GAMs zwischen verschiedenen Situationen zu vergleichen oder GAMs heranzuziehen, um ein mechanistisches Verständnis der zugrundeliegenden Prozesse zu entwickeln. GAMs sind vor allem dann beliebt, wenn man mutmasslich komplexe Beziehungen mit vielen Prädiktoren hat und es einem nicht um das Modell und seine Parameter an sich geht, sondern um möglichst gute Inter- und Extrapolation auf neue x-Werte. Ein beliebtes Feld sind sogenannte species distribution models (SDMs), die mit aktuellen Artvorkommens- und Umweltdaten “gefüttert” werden, um dann vorherzusagen, wie die Artverbreitung sich unter geänderten Umweltbedingungen (global change-Szenarien) ändern wird."
  },
  {
    "objectID": "Statistik_4.html#zusammenfassung",
    "href": "Statistik_4.html#zusammenfassung",
    "title": "Statistik 4",
    "section": "Zusammenfassung",
    "text": "Zusammenfassung\n\n\n\n\n\n\n\nGeneralized linear models (GLMs) erlauben Regressionen mit anderen Varianzstrukturen und Residuenverteilungen als lineare Regressionen.\nUnter den GLMs sind zwei besonders gebräuchlich: logistische Regressionen werden für binäre Daten, (Quasi-) Poisson-Regressionen für Zähldaten verwendet.\nNicht-lineare Regressionen erlauben die direkte Modellierung nicht-linearer und nicht-polynomialer Beziehungen.\nTypische Fälle für nicht-lineare Regressionen sind die Potenzfunktion und verschiedene “Sättigungsfunktionen” (z. B. Michaelis-Menten-Funktion).\nLOWESS dient der Visualisierung eines Trends (explorative Datenanalyse).\nGeneralized additive models (GAMs) können sowohl zum selben Zweck aber auch zum Aufbauen von prädiktiven Modellen verwendet werden, haben aber anders als typische Regressionstechniken keine leicht interpretier- und vergleichbare Parameter."
  },
  {
    "objectID": "Statistik_4.html#weiterführende-literatur",
    "href": "Statistik_4.html#weiterführende-literatur",
    "title": "Statistik 4",
    "section": "Weiterführende Literatur",
    "text": "Weiterführende Literatur\n\nCrawley, M.J. 2015. Statistics – An introduction using R. 2nd ed. John Wiley & Sons, Chichester, UK: 339 pp.\n\nChapter 7: Regression (pp. 142–145 [Non-linear regression], pp. 146–148 [GAMs])\nChapter 12: Other Response Variables\nChapter 13: Count Data\nChapter 15: Binary Response Variable\n\nDengler, J. 2009. Which function describes the species-area relationshipbest? – A review and empirical evaluation. Journal of Biogeography 36: 728–744.\nDunn, P.K. & Smyth, G.K. 2018. Generalized linear models with examples in R. Springer, New York, US: 562 pp.\nFox, J. & Weisberg, S. 2019. An R companion to applied regression. 3rd ed. SAGE Publications, Thousand Oaks, CA, US: 577 pp.\nLogan, M. 2010. Biostatistical design and analysis using R. A practical guide. Wiley-Blackwell, Oxford, UK: 546 pp., v.a.\n\npp. 178-179 (Smoother)\npp. 208-253 (Multiple und nicht-lineare Regressionen)\npp. 525-530 (GAMs)\npp. 483-530 (GLMs)\n\nNekola, J.C. & Brown, J.H. 2007. The wealth of species: ecological communities, complex systems and the legacy of Frank Preston. Ecology Letters 10: 188–196.\nQuinn, P.Q. & Keough, M.J. 2002. Experimental design and data analysis for biologists. Cambridge University Press, Cambridge, UK: 537 pp.\nRitz, C. & Streibig, J.C. 2008. Nonlinear regression with R. Springer, New York, US: 114 pp.\nŠmilauer, P. 2017. Modern regression methods. Chapter 2: Generalised linear models for counts and ratios. Unpublished script, České Budějovice, CZ.\nVer Hoef, J.M. & Boveng, P.L. 2007. Quasi-Poisson vs. negative binomial regression: how should we model overdispersed count data? Ecology 88:2766–2772."
  },
  {
    "objectID": "Statistik_5.html#lernziele",
    "href": "Statistik_5.html#lernziele",
    "title": "Statistik 5",
    "section": "Lernziele",
    "text": "Lernziele\n\n\n\n\n\n\nIhr…\n\nhabt verstanden, welche Versuchsdesigns mit einer normalen (Typ I) zweifaktoriellen ANOVA analysiert werden können und welche die Spezifikation eines random factors erfordern;\nkönnt einfache Fälle von Repeated measures- und Split-plot ANOVAs in R spezifizieren und durchführen (mit aov bzw. lme); und\nwisst, wann man generalized linear mixed effect models (GLMMs)- anwenden sollte und wie das im Prinzip geht."
  },
  {
    "objectID": "Statistik_5.html#split-plot-und-repeated-measures-anovas",
    "href": "Statistik_5.html#split-plot-und-repeated-measures-anovas",
    "title": "Statistik 5",
    "section": "Split-plot und Repeated-measures ANOVAs",
    "text": "Split-plot und Repeated-measures ANOVAs\n\nDie Idee\nBeginnen wir mit einer konventionellen 2-faktoriellen ANOVA wie wir sie aus Statistik 2 kennen. Wie in allen linearen Modellen (und ebenso in GLMs) ist eine wesentliche Modellvoraussetzung die Unabhängigkeit der Beobachtungen voneinander. In der folgenden Abbildung ist das für ein experimentelles Setting veranschaulicht, etwa unseren Sortenversuch mit Sorte A und B und den beiden Treatments Freiland und Gewächshaus:\n\n\n\n(aus Logan 2010)\n\n\nWir sehen, dass alle denkbaren Faktorenkombinationen (hier vier) auftreten (optimalerweise gleich häufig: balanciertes Design), sie aber räumlich zufällig, d. h. voneinander unabhängig angeordnet sind.\nIm Gegensatz dazu stehen mehrfaktorielle ANOVAs, bei denen nicht alle Faktorenkombinationen existieren oder es Abhängigkeiten zwischen den Treatments gibt. Hier gibt es zwei Typen:\n\nSplit plot-Design: Dies bezeichnet Situationen, bei denen die Kombinationen der beiden Faktoren nicht unabhängig voneinander räumlich verteilt sind, etwa weil dies mit zu grossem Aufwand verbunden wäre. Stellen wir etwa das Beispiel mit dem Gewächshaus-Freiland-Versuch von oben vor: Schon für die extrem geringe Replizierung von nur drei Wiederholungen pro Faktorenkombination müsste man sechs Gewächshäuser haben, jedes entweder mit Sorte A oder mit Sorte B, die man zudem räumlich zufällig platzieren kann. Logischerweise geht das oftmals nicht. Stattdessen könnte man drei Gewächshäuser haben, in denen man jeweils beide Sorten pflanzt. Dann wäre das Gewächshaus bzw. das entsprechende Freilandbeet der “plot”, der dann zwischen den beiden Sorten aufgeteilt (split) wird. Damit ist aber die Unabhängigkeitsannahme linearer Modelle verletzt, da sich ja die Gewächshäuser unterscheiden könnten, etwa in ihrer Thermoregulation, ihrer Lichtdurchlässigkeit oder ihrer Beschattung durch umstehende Bäume oder Gebäude. Deshalb hat potenziell die Frage, in welche Gewächshaus die Pflanzen standen, auch einen Einfluss auf das Ergebnis, muss mithin im statistischen Modell berücksichtigt werden\n\n\n\n\n(aus Logan 2010)\n\n\n\nRepeated measures-Design: Hier geht es nicht um eine räumliche Bindung (enges Nebeneinander), sondern um eine zeitliche Bindung (zeitliches Nacheinander). Das heisst, an bestimmten Untersuchungsobjekten (Personen, Pflanzenindividuen, Untersuchungsflächen) wird zu verschiedenen Zeitpunkten eine Untersuchung vorgenommen, wie die folgende Abbildung es veranschaulicht:\n\n\n\n\n(aus Logan 2010)\n\n\nWährend split plot-Design und repeated measures-Design auf den ersten Blick wie etwas Verschiedenes aussehen, so sind sie statistisch doch äquivalent.\n\n\n\n\n\n\nFrage\n\n\n\nWir hatten eine Situations wie im split plot/repeated measures-Design schon einmal: Bei welchem Verfahren war das?\n\n\n\n\nEin Beispiel\nFragestellung: Uns interessiert die Reaktionszeit von Personen auf Signale in Abhängigkeit von der Art der Signale (akustisch, visuell).\nVersuchsanordnung:\n\n8 Versuchspersonen (VP1–VP8)\nJe 4 davon zufällig den beiden Signaltypen (akustisch, visuell) zugeordnet\nMessung der Reaktionszeit nach 1, 2, 3 und 4 h (H1–H4)\n\nWir haben hier drei wesentliche Abweichungen von einer normalen TypI-ANOVA:\n\nWir sind nicht am spezifischen Verhalten der Versuchspersonen VP1–VP8 interessiert, sondern haben sie “zufällig” ausgewählt um alle möglichen Personen zu repräsentieren.\nJede Versuchsperson bekommt nur ein “Treatment”, d. h. es gibt nicht alle VP × Signal-Kombinationen.\nDie vier gemessenen Reaktionszeiten einer Person sind nicht unabhängig voneinander: So könnten bestimmte Personen vielleicht immer etwas schneller oder langsamer sein als andere.\n\n\n\nUmsetzung in R\nIn unserem Fall ist also der Block-Faktor die Versuchperson (VP), einerseits, da jede Person nur einem der beiden Signaltypen ausgesetzt wurde, andererseits, weil wir mehrere Messungen über die Zeit mit ihr durchgeführt haben. Im aov-Befehl lässt sich das mit dem Error-Term spezifizieren:\nspf.aov &lt;- aov(Reaktion~Signal\\*Messung + Error(VP), data = spf))\nsummary(spf.aov)\nError: VP\n          Df Sum Sq Mean Sq F value Pr(&gt;F)\nSignal     1  3.125   3.125       2  0.207\nResiduals  6  9.375   1.562               \nError: Within\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nMessung         3 194.50   64.83  127.89 2.52e-12 ***\nSignal:Messung3  19.37    6.46   12.74 0.000105 ***\nResiduals 18   9.13    0.51\nIm Ergebnis erhalten wir eine zweigeteilte ANOVA-Tabelle: Der obere Teil sagt uns, dass der Effekt von Signal (Art des Signals), der in den Personen (VP) geblockt ist, nicht signifikant (p = 0.207) ist. Der untere Teil sagt uns, dass es einen signifikanten Effekt der Zeit sowie eine signifikante Interaktion Signaltyp × Zeit gibt. Ein Interkationsplot zeigt uns genau dieses:\ninteraction.plot(spf$Messung, spf$Signal, spf$Reaktion)\n\n\n\n\n\nDer Plot macht klar, dass sich die Reaktionsheiten zwischen akustisch und optisch im Mittel nicht unterscheiden, sie aber im Fall von A2 schneller ansteigen als im Fall von A1\nMit dem Error-Term kann man auch mehrfache Schachtelungen codieren, jeweils links beginnend mit der obersten Ebene der Schachtelung:\nmodel2 &lt;- aov (Y ~ A \\* B \\* C + Error (Block/A/B), data = beispiel)"
  },
  {
    "objectID": "Statistik_5.html#linear-mixed-effect-models-lmms",
    "href": "Statistik_5.html#linear-mixed-effect-models-lmms",
    "title": "Statistik 5",
    "section": "Linear mixed effect models (LMMs)",
    "text": "Linear mixed effect models (LMMs)\n\nDie Idee\nLinear mixed effect models (LMMs) verallgemeinern LMs, um Folgendes modellieren zu können:\n\nAbhängigkeiten/Schachtelungen zwischen Faktoren (um der Verletzung der LM-Voraussetzungen Rechnung zu tragen).\nFaktoren, die uns nicht interessieren. Diese werden als sogenannte random factors modelliert, damit “sparen” wir Freiheitsgrade und gewinnen Teststärke für die uns interssierenden Faktoren.\n\nDie einfachsten LMMs, d. h. Repeated measures- und Split plot-ANOVA gehen (mit Limitierungen) noch mit dem aov-Befehl. Für komplexere Situationen bzw. im allgemeinen Fall (einschliesslich Regressionen und ANCOVAs) benötigt man dagegen lme aus dem Package nlme.\nAnalog zum Error-Term in aov spezifiziert man hier einen random-Term, wobei es zusätzlich die Möglichkeit gibt, zu entscheiden, ob man nur einen zufälligen Achsenabschnitt (random intercept) oder auch eine zufällige Steigung (random slope) modellieren möchte:\n\n\nUmsetzung in R\nlibrary(nlme)\n\n# mit random intercept (VP) und random slope (Messung):\nspf.lme.1 &lt;- lme(Reaktion~Signal*Messung, random = ~Messung | VP, data = spf) \n\n# nur random intercept:\nspf.lme.2 &lt;- lme(Reaktion~Signal*Messung, random = ~1 | VP, data = spf) \n\nanova(spf.lme.1)\n               numDF denDF   F-value p-value\n(Intercept)        1    18 1488.1631  &lt;.0001\nSignal             1     6    2.0808  0.1993\nMessung            3    18   70.7887  &lt;.0001\nSignal:Messung     3    18   11.8592  0.0002\nanova(spf.lme.2)\n               numDF denDF  F-value p-value\n(Intercept)        1    18 591.6800  &lt;.0001\nSignal             1     6   2.0000  0.2070\nMessung            3    18 127.8904  &lt;.0001\nSignal:Messung     3    18  12.7397  0.0001 \nLMMs, ihr korrekte Implementierung und Interpretation können u. U. sehr komplex sein, weswegen wir sie in unserem Kurs nicht mit viel Details besprechen können. Wer weitergehende benutzerfreundliche Informationen sucht, sei insbesondere auf Logan (2010: pp. 360–447) verwiesen."
  },
  {
    "objectID": "Statistik_5.html#generalized-linear-mixed-effect-models-glmms",
    "href": "Statistik_5.html#generalized-linear-mixed-effect-models-glmms",
    "title": "Statistik 5",
    "section": "Generalized linear mixed effect models (GLMMs)",
    "text": "Generalized linear mixed effect models (GLMMs)\n\nDie Idee\nGenerlized linear mixed effect models (GLMMs) verallgemeinern GLMs, um Folgendes modellieren zu können:\n\nGeschachtelte Daten\nZeitliche Korrelationen zwische Beobachtungen\nRäumliche Korrelationen zwischen Beobachtungen\nHeterogenität\nMesswiederholungen\n\nWährend dies alles wundervolle und oft benötigte Eigenschaften sind, sollte man sich auch der Nachteile/Limitierungen bewusst sein, wie die folgenden Zitate aus einem der führenden Lehrbücher zu GLMMs (Zuur et al. 2009) zeigen:\n\n“GLMM are at the frontier of statistical research”\n\n\n“This means that available documentation is rather technical and there are only few, if any, textbooks aimed at ecologists”\n\n\n“There are multiple approaches for obtaining estimated parameters”\n\n\n“There are at least four packages in R that can be used for GLMM”\n\n\n“This makes model selection in GLMM more of an art than a science”\n\nBezüglich der Anwendung von GLMMs, kommen Zuur et al. (2009) daher zu folgendem Schluss (der natürlich auch sonst in der Statistik gilt, hier aber besonders wichtig ist):\n\n“When applying GLMM, try to keep the models simple or you may get numerical estimation problems.”\n\n\n\nEin Beispiel und seine Umsetzung in R\nBefall von Rothirschen (Cervus elaphus) in spanischen Farmen mit dem Parasiten Elaphostrongylus cervi. Modelliert wird Vorkommen/Nichtvorkommen von L1-Larven dieser Nematode in Abhängigkeit von Körperlänge und Geschlecht der Hirsche. Erhoben wurden die Daten auf 24 Farmen.\nWir können das Ganze wie bisher mit einem binomialen GLM analysieren:\nDE.glm &lt;- glm(Ecervi.01 ~ CLength * fSex+fFarm,\n            family = binomial, data = DeerEcervi)\nsummary(DE.glm)\nCoefficients:\n                Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   -1.796e+00  5.900e-01  -3.044 0.002336 ** \nCLength        4.062e-02  7.132e-03   5.695 1.24e-08 ***\nfSex2          6.280e-01  2.292e-01   2.740 0.006150 ** \nfFarmAU        3.340e+00  7.841e-01   4.259 2.05e-05 ***\nfFarmBA        3.510e+00  7.150e-01   4.908 9.19e-07 ***\n[…]\nfFarmVY        3.974e+00  1.257e+00   3.162 0.001565 ** \nCLength:fSex2  3.618e-02  1.168e-02   3.097 0.001953 ** \nDas Modell, das wir erzeugt haben, liesse sich folgendermassen visualisieren:\n\n\n\n(aus Zuur et al. 2009)\n\n\nFür unseren Zweck hat die Lösung mit einem GLM zwei Nachteile:\n\nfFarm “verbraucht” 23 Freiheitsgrade, obwohl wir nicht am Farmeffekt interessiert sind.\nWir bekommen ein Modell für jede einzelne Farm, aber kein farmunabhängiges Modell.\n\nBeispiehaft analysieren wir dieses GLMM mit glmm.PQL aus dem Package MASS:\nlibrary(MASS)\nDE.PQL &lt;- glmmPQL(Ecervi.01 ~ CLength * fSex,\n                random = ~ 1 | fFarm, family = binomial, data = DeerEcervi)\nsummary(DE.PQL)\nRandom effects:\n Formula: ~1 | fFarm\n        (Intercept)  Residual\nStdDev:    1.462108 0.9620576\n[…]\nFixed effects: Ecervi.01 ~ CLength * fSex \n                  Value Std.Error  DF  t-value p-value\n(Intercept)   0.8883697 0.3373283 799 2.633547  0.0086\nCLength       0.0378608 0.0065269 799 5.800768  0.0000\nfSex2         0.6104570 0.2137293 799 2.856216  0.0044\nCLength:fSex2 0.0350666 0.0108558 799 3.230228  0.0013\nWie wir das schon von ANOVAs mit Error-Term oder LMMs kennen, ist die Ergebnistabelle in einen Teil für die Random effects und einen Teil für die Fixed effects aufgeteilt. Für fFarm gibt es jetzt aber anders als beim GLM nicht 23 Schätzwerte, sondern nur einen für die Standardabweichung. Der untere Teil entspricht dagegen dem Output eines GLMs, wenn wir fFarm völlig ignoriert hätten: wir haben die Effekte von Grösse, Geschlecht und deren Interaktion (alle signifikant).\nWas sagen uns die Ergebnisse nun?\n\nWahrscheinlichkeit des Parasitenbefalls für weibliche Hirsche:\n\\(\\text{logit}(p_{ij}) = 0.888 + 0.037 \\times \\text{Length}_{ij}\\)\nWahrscheinlichkeit des Parasitenbefalls für männliche Hirsche:\n\\(\\text{logit}(p_{ij}) = (0.888 + 0.610) + (0.037 + 0.035) \\times \\text{Length}_{ij}\\)\n\\(\\text{logit}(p_{ij}) = 1.498 + 0.072 \\times \\text{Length}_{ij}\\)\n\nDa die Codierung Sex2 = “männlich” war und wir sowohl ein “random intercept” als auch ein “random slope” modelliert haben, ergibt sich der Achsenabschnitt für die männlichen Hirsche durch die Addition des allgemeinen Achsenabschnitts (der sich auf Sex1 = “weiblich” bezieht) und dem Effekt von Sex2, während sich die Steigung für die männlichen Hirsche aus jener für die weiblichen + den Interkationsterm ergibt.\nDa wir es mit einem Binomial-GLMM zu tun haben, sagen uns die gefundenen Gleichungen immer noch nicht unmittelbar etwas über die Beziehungen, da auf der linken Seite der Gleichung jeweils \\(\\text{logit}(p_{ij}\\)) und nicht \\(p_{ij}\\) steht. Wir könnten wie in Statistik 4 nach \\(p_{ij}\\) auflösen oder wir nutzen eine Visualisierung. Im Folgenden ist z. B. die GLMM-Vorhersage für weibliche Hirsche mit Konfidenzintervall geplottet, was schön den Unterschied zum GLM zeigt:\n\n\n\n(aus Zuur et al. 2009)\n\n\n\n\nVerschiedene R-packages für GLMMs\nEs gibt mehrere R-packages für GLMMs, von denen die folgenden die gängisten sind:\n\nlibrary(MASS): glmmPQL\nlibrary(lme4): glmer\nlibrary(glmmML): glmmML\n\nDie Syntax der verschiedenen Packages unterscheidet sich im Detail, bitte bei Bedarf die jeweilige Hilfe-Funktion konsultieren.\nDa ein GLMM ein sehr komplexes Verfahren ist, sind die verschiedenen Implementierungen nicht genau gleich. Insofern kann es auch leichte Divergenzen in den Parameterschätzungen und den Parametern geben, wie die folgende Auswertung für unser Hirschbeispiel zeigt:\n\n\n\n(aus Zuur et al. 2009)\n\n\nIn diesem Fall (und meist) sind die Abweichungen zwischen den drei GLMMs aber gering. Dagegen ist die Aussage deutlich verschieden von der mit dem GLM ermittelten (massiv andere Parameterschätzung für Sex, etwas andere für Length und Length × Sex).\n\n\nRandom vs. fixed factors\nWann sollten wir random factors nehmen, wann fixed factors? Im Hirsch-Beispiel ist statistisch klar, dass wir die Farm-Identität in unser statistisches Modell aufnehmen müssen, da auf jeder Farm mehrere Hirsche untersucht wurden und unser wissen über as universelle Phänomen der räumlichen Autokorrelation es höchstwahrscheinlich macht, dass sich die Hirsche einer einzelnen Farm (wg. räumlicher Nähe) ähnlicher verhalten als zufällig herausgegriffene Paare von Farm-Hirschen aus ganz Spanien.\nOb wir die Farm-Identität dagegen als fixed factor aufnehmen (d. h. ein GLM rechnen) oder als random factor (d. h. ein GLMM rechnen), hängt von unserer Frage ab. In der Beschreibung der Studie wurde suggeriert, dass es uns um ein allgemeines farmunabhängiges Modell ging, wie sich der Parasitenbefall in Abhängigkeit von Geschlecht und Grösse entwickelt. Dann wäre unser Vorgehen richtig, fFarm als random factor zu definieren. Wir dürfen und können dann aber keine Aussage über eine einzelne Farm treffen. Wenn uns dagegen interessiert, ob und wie sich die Farmen bezüglich Parasitenbefall unterscheiden, etwa weil sie unterschiedliche Hygienekonzepte oder Populationsdichten haben, dann müssen wir fFarm als fixed factor einführen (also ein GLM rechnen). Ob wir in einer solchen Situation ein GLM oder ein GLMM rechnen, hängt also von unserer genauen Frage ab."
  },
  {
    "objectID": "Statistik_5.html#lms-glms-lmms-und-glmms-im-rückblick-und-überblick",
    "href": "Statistik_5.html#lms-glms-lmms-und-glmms-im-rückblick-und-überblick",
    "title": "Statistik 5",
    "section": "LMs, GLMs, LMMs und GLMMs im Rückblick und Überblick",
    "text": "LMs, GLMs, LMMs und GLMMs im Rückblick und Überblick\nZum Abschluss der fünf inferenzstatistischen Lektionen seien noch einmal die grundlegenden Ähnlichkeiten und Unterschiede von LMs, GLMs, LMMs und GLMMs zusammengefasst:\n\nLMs: Linear models\nGLMs: Generalized linear models\nLMMs: Linear mixed effect models\nGLMMs: Generalized linear mixed effects models"
  },
  {
    "objectID": "Statistik_5.html#zusammenfassung",
    "href": "Statistik_5.html#zusammenfassung",
    "title": "Statistik 5",
    "section": "Zusammenfassung",
    "text": "Zusammenfassung\n\n\n\n\n\n\n\nWenn in einem ANOVA-Design Schachtelungen oder Abhängigkeiten vorliegen, muss man diese im Modell spezifizieren, was entweder als Error in aov oder als random in lme (package nlme) geht.\nWährend GLMs lineare Modelle bezüglich der geforderten Residuen- und Varianzstruktur verallgemeinern, leisten linear mixed effect models (LMMs) dies bezüglich unterschiedlichster Abhängigkeiten zwischen Beobachtungen.\nGeneralized linear mixed effect models (GLMMs) schliesslich ermöglichen, beide Typen von Abweichungen von den Voraussetzungen linearer Modelle zu berücksichtigen."
  },
  {
    "objectID": "Statistik_5.html#weiterführende-literatur",
    "href": "Statistik_5.html#weiterführende-literatur",
    "title": "Statistik 5",
    "section": "Weiterführende Literatur",
    "text": "Weiterführende Literatur\n\nCrawley, M.J. 2015. Statistics – An introduction using R. 2nd ed. John Wiley & Sons, Chichester, UK: 339 pp.\n\nChapter 8: Analysis of Variance (pp. 173–182)\n\nLogan, M. 2010. Biostatistical design and analysis using R. A practical guide. Wiley-Blackwell, Oxford, UK: 546 pp., v.a.\n\npp. 399-447 (split-plot und repeated measures ANOVAs)\n\nZuur, A. E., Ieno, E. N., Walker, N. J., Saveliev, A. A., Smith, G. M. (eds.) 2009. Mixed effects models and extension in ecology with R. Springer, New York: 576 pp.\nZuur, A.E., Hilbe, J.M. & Ieno, E.N. 2013. A beginner’s guide to GLM and GLMM with R – A frequentist and Bayesian perspective for ecologists. Highland Statistics, Newburgh: 253 pp."
  },
  {
    "objectID": "Statistik_6.html#lernziele",
    "href": "Statistik_6.html#lernziele",
    "title": "Statistik 6",
    "section": "Lernziele",
    "text": "Lernziele\n\n\n\n\n\n\nIhr…\n\nversteht, was Ordinationen sollen, was sie leisten können und was nicht;\nkönnt das Prinzip einer PCA beschreiben, sie implementieren, und ihren Ergebnisoutput interpretieren;\nDie Annahmen einer PCA kennt, und wisst welche “Artefakte” bei einer Verletzung herauskommen; und\nhabt das Vorgehen im Prinzip verstanden, wie DCA und NMDS diese Probleme angehen."
  },
  {
    "objectID": "Statistik_6.html#einführung-in-multivariate-methoden",
    "href": "Statistik_6.html#einführung-in-multivariate-methoden",
    "title": "Statistik 6",
    "section": "Einführung in “multivariate” Methoden",
    "text": "Einführung in “multivariate” Methoden\n\nWas ist mit “multivariat” gemeint?\nWas ist mit “multivariat” gemeint? Zunächst einmal sagt das nur, dass pro Beobachtung (observation) mehr als zwei Variablen erhoben werden, deren Beziehungen zueinander analysiert werden. Im Wortsinne waren also auch schon die zweifaktorielle ANOVA und die multiple Regression “multivariate” Methoden.\nDie folgende Tabelle fasst die schon besprochenen und noch kommenden statistischen Verfahren bezüglich der Anzahl von Prädiktor- und Antwortvariablen zusammen:\n\n\n\n\n\nIn der Literatur wird der Begriff “multivariat” jedoch oft nur für die letzte Gruppe von Verfahren, also Ordinationen und Cluster-Analysen, gebraucht. Diese bilden den Gegenstand von Statistik 6–8.\n\n\nInferenzstatistik vs. deskriptive Statistik\nBislang haben wir statistische Verfahren überwiegend zum Testen von Hypothesen verwendet (inklusive des impliziten Hypothesentestens, wenn man eine offene Forschungsfrage beantwortet): Inferenzstatistik (schliessende Statistik).\nOrdinationen und Cluster-Analysen** sind überwiegend deskriptive Statistik** (ohne spezielle Zusatzschritte erlauben sie kein Testen von Hypothesen!).\n\n\nBeispiele multivariater Datensätze\nMultivariate Datensätze sind in unserer “datenreichen” Welt allgegenwärtig z. B.:\n\nBodenproben, an denen viele unterschiedliche physikalische und chemische Variablen, ggf. auch noch in verschiedenen Horizonten gemessen wurden.\nKlimadaten von Messstationen: zahlreiche Variablen wie Mittel/Minima/Maxima von Temperatur/Niederschlag/Sonnenschein/Bewölkung/Windstärke usw. und das für jeden Monat.\nZusammensetzungen von lokalen Pflanzengesellschaften oder Tiergemeinschaften: hier sind die Deckungen bzw. Individuenzahlen der einzelnen Arten die Variablen\nErgebnisse von Befragungen von Konsumenten: viele Variablen zu Präferenzen, Einstellungen usw.\n\n\n\nZiele multivariat-deskriptiver Analysen\nIm Prinzip können wir auch bei solchen Beobachtungsdaten mit vielen abhängigen Variablen wie bisher jede einzeln testen:\n\nDas kann vorteilhaft sein, wenn man konkrete Hypothesen testen will (was ja mit multivariat-deskriptiven Methoden normalerweise nicht geht).\nEin Problem sind die vielen Tests mit dem gleichen Datensatz, die zu einer “Inflation” der Typ I-Fehlerrate führen (wenn ich 20 Tests durchführe, würde ja bei α = 0.05 einer rein zufällig eine Signifikanz anzeigen, selbst wenn eigentlich für keinen einen Beziehung besteht). Für dieses Problem gibt es aber Korrekturmöglichkeiten (z. B. “Bonferroni”-Korrektur).\nProblematischer ist, dass es sehr schwierig ist, aus den vielen Einzelergebnissen am Ende ein aussagekräftiges Gesamtbild zu synthetisieren.\n\nHier setzen die multivariat-deskriptiven Methoden mit ihren beiden Hauptzielen an:\n\nMuster und Beziehungen im n-dimensionalen Hyperraum erkennen und beschreiben.\nDimensionsreduktion: die wesentliche Information aus den n Dimensionen wird auf 2 bis wenige Dimensionen reduziert, die vorstellbar und visualisierbar sind.\n\nDer n-dimensionale Hyperraum ist das Konzept, das uns durchgängig bei den multivariat-deskriptiven Methoden begleitet. Dahinter verbirgt sich die Idee, dass jede der n Variablen eine orthogonale Achse ist, auf der die Ausprägungen der Variablen (metrisch oder kategorial) aufgetragen sind. Während wir uns einen 3-dimensionalen Raum noch vorstellen können, ist es mit der Vorstellungskraft bei vier oder gar 100 Dimensionen schnell zu Ende. Aber das ist ja genau der Grund für die multivariat-deskriptiven Methoden…\n\n\nZwei komplementäre Ansätze\nInnerhalb der multivariat-deskriptiven Statistik stellen Ordinationen und Cluster-Analysen (Klassifikationen) zwei komplementäre Ansätze dar. Sie betonen unterschiedliche Aspekte des Datensatzes und können oftmals sogar sinnvoll parallel verwendet werden. Die wesentlichen Unterschiede zeigt die folgende Tabelle:"
  },
  {
    "objectID": "Statistik_6.html#die-idee-von-ordinationen",
    "href": "Statistik_6.html#die-idee-von-ordinationen",
    "title": "Statistik 6",
    "section": "Die Idee von Ordinationen",
    "text": "Die Idee von Ordinationen\nOrdinationen versuchen nun im Prinzip im n-dimensionalen Raum der (Antwort-) Variablen diejenigen Ebenen zu finden, welche die meiste Varianz erklären. Dies geschieht durch die folgenden Schritte:\n\nZentrieren der Punktwolke, so dass der Schwerpunkt im Ursprung des Koordinatensystems liegt.\nRotieren der Punktwolke, bis die erste Achse die maximal mögliche Varianz abbildet.\nNach Fixierung der ersten Achse Fortsetzen des Rotierens, bis die zweite Achse wiederum das maximal Mögliche der verbleibenden Varianz abbildet, usw. bis zur n-ten Achse.\nVisualisierung der Ergebnisse bei Beschränkung auf die relevanten ersten Achsen.\n\nUm diese Idee zu visualisieren, nehmen wir ein System von nur zwei Variablen, da wir diese noch auf einer Ebene (d. h. im gedruckten Skript) visualisieren können. Stellen wir uns sechs Beobachtungspunkte entlang eines Umweltgradienten (z. B. Meereshöhe) vor. An jedem dieser Beobachtungspunkte wird die Häufigkeit von zwei Arten ermittelt, etwa folgendermassen:\n\n\n\n\n\nWenn wir das jetzt im “Artenraum” zeigen, also mit der Häufigkeit von Art 1 auf der x-Achse und der Häufigkeit von Art 2 auf der y-Achse, dan bekämen wir das grüne Muster. Zentriert (d. h. so dass die Mittelwerte aller x- und y-Werte jeweils 0 sind), ergibt sich die rote Figur. Dies wird schliesslich so rotiert, dass die maximale Varianz (hier im simplen Fall einfach die Distanz zwischen den extremen Punkten) paralle zur x-Achse liegt (blau)."
  },
  {
    "objectID": "Statistik_6.html#hauptkomponentenanalyse-pca",
    "href": "Statistik_6.html#hauptkomponentenanalyse-pca",
    "title": "Statistik 6",
    "section": "Hauptkomponentenanalyse (PCA)",
    "text": "Hauptkomponentenanalyse (PCA)\n\nDas Prinzip\nDas im vorigen Abschnitt skizzierte Vorgehen, ist genau das, was eine Hauptkomponentenanalyse (Principal component analysis, PCA) macht:\n\nBasiert auf einer linearen Beziehung zwischen den Attributen.\nAchsen sind orthogonal (und die Varianzen daher additiv).\nDie ursprünglichen Distanzen zwischen den Objekten (Beobachtungen) bleiben daher unverändert.\n\nPCAs eignen sich für:\n\nEinfache Visualisierung, wenn die Linearität gegeben ist.\nBei multiplen Regressionen mit vielen, korrelierten Prädiktoren kann man die PCA-Achsen als synthetische Prädiktoren verwenden, da sie vollständig unkorreliert sind.\n\nPCAs eignen sich nicht(und das gilt fast immer für für Daten zur Artenzusammensetzung ökologischer Gemeinschaften) für:\n\nNicht-lineare Beziehungen.\nViele Nullen in der Matrix.\n\nDie PCA findet die beste Rotation mittels der sogenannten “Eigenanalyse”, wie die folgende Abbildung veranschaulicht:\n\n\n\n\n\n\n\n\n\n\n\n\n\n(aus Wildi 2013)\n\n\n\nDabei gilt:\n\\[\\begin{align*}\n\\alpha &= \\text{Eigenvektormatrix}\\\\\n&= \\text{Korrelationskoeffizient (der Arten/Variablen) mit den\nOrdinationsachsen}\n\\end{align*}\\]\n\\[\n\\text{Eigenwerte einre Achse} = \\text{Sum of Squares der Achse}\n\\]\n\n\nIn R\nPCAs sind z. B. im Package labdsv implementiert:\nlibrary(labdsv)\no.pca &lt;- pca(raw)\no.pca$scores\n          PC1         PC2\nr1 -1.9216223 -0.09357697\nr2 -0.6353776 -0.68143293\nr3  0.4762699 -0.80076373\nr4  2.3503705 -0.10237502\nr5  0.8895287  0.95400610\nr6 -1.1591692  0.72414255\no.pca$loadings\n             PC1        PC2\nspec.1 0.3491944 -0.9370503\nspec.2 0.9370503  0.3491944\n# Erklärte Varianz der Achsen\nE &lt;- o.pca$sdev^2/o.pca$totdev*100\nE\n[1] 82.40009 17.59991\nZunächst wird die PCA ausgeführt und das Ergebnis in einem Objekt (o.pca) gespeichert. Die uns Interessierten Informationen kann man wie oben gezeigt abrufen: ...$scores enthält die resultierenden Koordinaten der Beobachtungen nach der Ordination; ...$loadings gibt die Vektoren wieder, die nach der Rotation den beiden Arten entsprechen (Art 1 hat also den Vektor 0.35/–0.94). Die erklärte Varianz ist ein uns schon bekanntes Konzept. Die Gesamtvarianz ist alles, was im Datensatz mit seinen n Achsen drin steckt (100%), hier wird dieser Wert auf die Achsen aufgeteilt, also 82 % auf der ersten Achse, 18 % auf der zweiten. Alle n Achsen zusammen ergeben immer 100 %.\nZiel einer PCA ist ja meist eine Visualisierung. Für unsere sechs Beobachtungen von zwei Arten haben wir das oben ja schon gemacht (und da hat es auch keine Dimensionsreduktion gebracht, da es eh nur zwei Arten waren). Wenn wir uns nun aber einen Datensatz mit 63 Beobachtungspunkten (hier: Vegetationsaufnahmen) und 119 Variablen (hier: Pflanzenarten) anschauen, dann haben wir eine Dimensionsreduktion von 119 auf 2. Das aufbereitete Ergebnis kann dann wie folgt aussehen (den R Code dazu gibt es im Demoskript):\n\n\nBitte beachten, dass wir hier eine PCA für einen Fall gerechnet haben (ökologische Gemeinschaftsdaten), für den sie mit seltenen Ausnahmen ungeeignet ist. Warum sie hier problematisch war, werden wir weiter unten ansehen wie auch Lösungen dafür.\n\n\nBeispiele von Anwendungen von PCAs\nZunächst sollen aber einige gängige und korrekte Anwendungen auf sehr grossen Datensets gezeigt werden:\n\nVisualisierung 1: Hier wurden etwa 20 verschiedene bioklimatische Variablen für alle Rasterzellen der Erdoberfläche (Farbkodierung gibt die Häufigkeit wieder) einer PCA unterworfen. Die Klimadaten sind so hoch korreliert, dass die ersten beiden Achen (Hauptkomponenten) PC1 und PC2 zusammen 76 % der Varianz im Gesamtdatensatz kodieren. Es wäre also unsinnig, die 20 Variablen einzeln zu analysieren. Durch die rechts gezeigten Korrelationen der Originalvariablen mit PC1 und PC2 kann man die beiden synthetischen Achsen näherungsweise interpretieren (siehe die Achsenbeschriftung links).\n\n\n\n\n\n\n\n\n\n\n\n\n(aus Bruelheide et al. 2019)\n\n\n\n\nVisualisierung 2: Hier wurden 6 funktionelle Merkmale (traits) von Pflanzenarten weltweit einer PCA unterworfen. Diese erweisen sich so weit korreliert, dass die ersten beiden Achen (Hauptkomponenten) PC1 und PC2 zusammen 74% der Varianz kodieren. Der eine wesentliche Gradient (etwas gegen PC1 nach links verdreht) ist jender von winizigen, kleinsamigen Arten zu grossen Arten mit schweren Samen. Dazu weitgehend orthogonal ist der Gradient von Pflanzen mit stickstoffreichen Blättern (links oben) zu Pflanzen mit stickstoffarmen Blättern (rechts unten).\n\n\n\n\naus Díaz et al. 2016\n\n\n\nPrincipal Components (PCs) in multiplen Regressionen: Hier rechnet man zunächst eine PCA mit vielen Umweltvariablen ohne Rücksicht auf ihre wechselseitigen Korrelationen. Dann nimmt man die (ersten) PC-Achsen mit der meisten Information als sogenannte “synthetische” Prädiktoren.\n\n\nVorteil: Die PC-Achsen sind vollständig unkorreliert.\nNachteil: Die PC-Achsen sind nicht so direkt interpretierbar wie die Original-Umweltparameter, das sie zwar oft stark mit mehreren Umweltparametern korrelieren, aber eben nicht 100 %.\nWichtig: Hochladende Achsen sind nicht unbedingt auch die wichtigsten für die Regression."
  },
  {
    "objectID": "Statistik_6.html#ordinationen-für-problematische-fälle",
    "href": "Statistik_6.html#ordinationen-für-problematische-fälle",
    "title": "Statistik 6",
    "section": "Ordinationen für “problematische” Fälle",
    "text": "Ordinationen für “problematische” Fälle\n\nWann sind PCAs problematisch?\nWie schon erwähnt, ist die Anwendung von PCAs problematisch/falsch, wenn einer oder beide der folgenden Fälle vorliegen:\n\nNicht-lineare Beziehungen.\nVielen Nullen in der Matrix.\n\nIn der Ökologie ist das besonders relevant, da beides für Artdaten in der Gemeinschaftsökologie (community ecology) nicht die Ausnahme, sondern der Normalfall ist. Arten reagieren auf Umweltfaktoren meist nicht linear, sondern unimodal (humpshaped) und in grossen Matrizen von Artvorkommen in Vegetationsaufnahmen und Gebieten ist es normal, dass die meisten Arten in den meisten Aufnahmeflächen nicht vorkommen, also ihre Deckung oder Abundanz Null ist. Dagegen lassen sich Matrizen von Umweltdaten der Untersuchungsgebiete (etwa von Boden- und Klimadaten) problemlos mit einer PCA analysieren (siehe Beispiel (a) im vorigen Abschnitt, da es ja keine Nullwerte gibt).\nWarum sind nicht-lineare Beziehungen in einer PCA problematisch? Sehen wir uns dazu noch einmal unser Eingangsbeispiel der zwei Arten entlang eines Umweltgradienten von 1 bis 6 an:\n\n\n\n\n\n\n\n\n\n\nAufgrund des Umweltgradienten sollten die Beobachtungen/Standorte 1 und 6 maximal unähnlich sein. Tatsächlich kommen sie aber im Ordinationsdiagramm sehr nahe beieinander zu liegen. Das liegt daran, dass beide Arten unimodal (mit einer Optimumskurve) auf den Umweltgradienten reagieren. Wenn der Umweltgradient etwa die Bodenfeuchte wäre, hiesse das, dass beide bei mittlerer Bodenfeuchte am häufigsten sind und Richtung sehr nasser oder sehr trockener Böden seltener werden. Das heisst, an den Standorten 1 und 6 sind beide relativ selten, wenn auch aus unterschiedlichen Gründen, die Artenzusammensetzung daher ingesamt ähnlich.\nMan bezeichnet dieses Phänomen/Problem: als Hufeisen- oder Bogeneffekt (horse shoe/arch effect).\n\n\nKorrespondenzanalyse (CA)\nEin Verfahren, um solche Probleme (vor allem in der Gemeinschaftsökologie) anzugehen, ist die Korrespondenzanalyse (Correspondence Analysis, CA). Sie wird auch als Reciprocal Averging bezeichnet. Wichtige Aspekte der CA sind:\n\nHier wie in allen folgenden Ordinationsmethoden wird der Ordinationsraum transformiert (im Gegensatz zur PCA) durch die Anwendung eines Distanzmasses.\nCA hat als Distanzmass implizit die \\(\\chi^2\\)-Metrik. - CA ist spezifisch gedacht für Artenverteilungen entlang von Umweltgradienten, wobei jede Art für sich unimodal reagiert.\nWie die meisten weiteren Ordinationstechniken implementiert im package vegan für community ecology.\n\nIn R wird das wie folgt umgesetzt (man beachte, dass häufig die Artdeckungen eingangs noch wurzeltransformiert werden (^0.5), um Arten mit geringer Deckung relativ mehr Gewicht zu geben):\nlibrary(vegan)\nca.1 &lt;- cca(sveg^0.5)\n\n# Arten (o) und Communities (+) plotten\nplot(ca.1)\n\n# Nur Arten plotten\nplot(ca.1, display = \"species\", type = \"points\") \n# Anteilige Varianz, die durch die ersten beiden Achsen erklärt wird\no.ca$CA$eig[1:2]/sum(o.ca$CA$eig)\nWenn wir jetzt die Anwendung der PCA und der CA auf den Moordatensatz (63 Vegetationsaufnahmen mit 119 Arten) anschauen, den wir oben schon einmal kurz hatten, dann zeigt sich, dass aus dem Hufeisen im Prinzip ein (umgekehrtes) U oder V wird, die extremen Punkte des Gradienten also nicht mehr so nahe beisammen stehen:\n\n\n\n\n\nWie dieser Unterschied zustande kommt, visualisiert die folgende konzeptionelle Abbildung mit drei Arten:\n\n\n\naus Wildi 2013\n\n\n\n\nDCA\nWie wir im vorigen Abschnitt gesehen haben, löst die CA die Probleme der PCA bei Community-Daten in der Ökologie, aber eben nur teilweise. Aus einem Hufeisen wird ein U, aber eigentlich war der Umweltgradient (hier von feucht nach trocken) ja linear, nur die Artantworten waren eben unimodal. Insofern wurde die CA noch weiter verfeinert, um den sich ergebenden Hauptumweltgradienten möglichst linear abzubilden. Wir landen bei der Detrended Correspondence Analysis (DCA), man könnte auf Deutsch von einer “trendbereinigten Korrespondenzanalyse” sprechen, aber dieser deutsche Begriff wird eigentlich nie gebraucht.\nEs gibt verschiedene Detrending-Methoden, die gängigste ist “detrending by segments”. wie sie in folgendem Schema visualisiert ist:\n\n\n\naus Leyer & Wesche 2007\n\n\nDie mathematischen Schritte dahinter und die daraus resultierenden methodischen Entscheidungen sind etwas komplexer, so dass wir sie nicht im Detail behandeln. Wer die Dinge im Einzelnen nachvollziehen möchte, sei auf Leyer & Wesche (2007) bzw. Oksanen (2015) verwiesen. Der R Code (Funktion decorana im Package vegan) ist auch etwas länger, sodass wir ihn nicht hier im Skript wiedergeben, sondern nur in den R-Demos.\nAus dem Gesagten wird evident, dass eine DCA nach all den erfolgten Transformationen des Ordinationsraumes keine Methode der schliessenden Statistik ist, sondern ein (durchaus leistungsfähiges) Visualisierungstool komplexer Community-Daten. Da, wie geschildert, eine CA die Probleme der Ordination von Community-Daten nur unzureichend löst, findet sie als solche hier eigentlich nie Anwendung (siehe jedoch die CCA in Statistik 7), sondern entweder PCA oder DCA (oder eben NMDS, vgl. folgenden Abschnitt).\nWarum wird jetzt doch wieder die PCA für Community-Daten genannt, nachdem sie bislang mehrfach als ungeeignet angeführt wurde? Meist passt sie methodisch nicht, aber es gibt Fälle, bei denen die Umweltgradienten so kurz sind, dass die Artenreaktionen auf den oder die Umweltgradienten in guter Näherung als linear betrachtet werden können. Das ist dann der Fall, wenn man lauter sehr ähnliche Standorte untersucht hat, dann ist eine PCA ausnahmsweise das bessere Modell. Wie weiss man, ob das bei einem bestimmten Datensatz der Fall ist?\nZunächst vielleicht etwas überraschend lautet die Antwort: man berechnet zuerst eine DCA. Ein Standard-Output der DCA ist die geschätzte Gradientenlänge der ersten Achse. Die Länge des Gradienten wird in Standardabweichungen (SD) quantifiziert, was zunächst “schräg” klingt. Das bezieht sich auf die Annahme, dass die Artenhäufigkeit entlang des Umweltgradienten näherungsweise einer Normalverteilung folgt. Vielleicht habt ihr im Hinterkopf, dass 95 % aller Werte einer Normalverteilungskurve im Bereich von Mittelwert ± 2 SD liegt. Wenn der geschätzte Gradient also 4 SD-Einheiten oder mehr ist, gibt es zwischen den beiden Enden des untersuchten Umweltgradienten praktisch keine gemeinsamen Arten (bzw. sie treten mit weniger als 1 % ihrer Maximalhäufigkeit auf), man spricht von einem vollständigen Arten-Turnover. Bei einer Gradientenlänge von 8 SD-Einheiten hätte man sogar zwei vollständige Arten-Turnovers, also letztlich drei komplett verschiedene Gesellschaften ohne Überlappung.\nDie Faustregel für die Anwendung von DCA vs. PCA besagt, dass bei einer Länge der ersten Achse von &lt; 3 SD-Einheiten mit der PCA gearbeitet werden sollte, bei einer Länge von 3–4 SD-Einheiten beide Methoden gehen und bei &gt;4 SD-Einheiten man bei der DCA bleiben sollte. Man könnte aber auch argumentieren, dass die Annahmen der PCA theoretisch für solche Datensätze nie zutreffen, man also per se mit der DCA arbeiten sollte.\nSchauen wir uns den Effekt noch im Fall unseres Moor-Datensatzes an:\n\n\n\n\n\nWie wir sehen, wurde aus dem umgekehrten U und eine relativ homogene Punktwolke, mit der längsten Ausdehnung entlang der ersten Achse (was ja die Grundidee einer Ordination ist). Die Gradientenlänge können wir auf der x-Achse ablesen, sie beträgt etwa 3.2 SD-Einheiten (Differenz der Position zwischen dem Punkt ganz links und dem Punkt ganz rechts).\n\n\nNMDS\nNMDS** steht für Non-metric Multi-Dimensional Scaling, wofür es keine gute/gängige deutsche Übersetzung gibt. Die wichtigsten Aspekte einer NMDS sind:\n\n“Non-metric”, da mit Rängen, nicht mit Distanzen gearbeitet wird.\nNMDS arbeitet mit einem Iterationsalgorithmus, der jedes Mal ein geringfügig anderes Ergebnis liefert.\nStartet mit einer beliebigen vorgegebenen Ordination, etwa einer PCA.\nDanach werden sukzessive die Punkte im niedrig-dimensionalen Ordinationsraum (meist 2D) geringfügig verschoben und geschaut, ob die originale Distanzmatrix besser wiedergegeben wird, so lange, bis ein (lokales) Optimum erreicht ist.\n\nIn R geht das folgendermassen. Dabei steht der Parameter k für die Zahl der gewünschten Dimensionen (normalerweise wählt man 2) (weitere Details dann in der Demo im Klassenverband):\n# Distanzmatrix als Start erzeugen\nmde &lt;- vegdist(sveg, method=\"euclidean\")\nmde\n\n# Zwei verschiedene NMDS-Methoden\nset.seed(1) # macht man, wenn man bei einer Wiederholung exakt die gleichen Ergebnisse will\nimds &lt;- isoMDS(mde, k=2)\nset.seed(1)\nmmds &lt;- metaMDS(mde, k=2)\n\nplot(imds$points)\nplot(mmds$points)\n\n\nplot(o.imds$points)\nplot(o.mmds$points)\n\n# Stress = S2 = Abweichung der zweidimensionalen NMDS-Lösung von der originalen Distanzmatrix\nstressplot(o.imds,mde)\nDas Ergebnis (hier mit dem Algorithmus isoMDS) sieht man links. Wie gut die NMDS die originale Struktur wiedergibt, zeigt sich rechts (erzeugt mit stressplot):\n\n\n\n\nZwei wichtige Aspekte sollte man hier noch erwähnen: Da NMDS mit einem interativen Algorithmus arbeitet, der eine Zufallskomponente enthält, kommen bei jedem Durchlauf geringfügig andere Ergebnisse heraus. Wenn man das verhindern will, kann man mit set.seed arbeiten, was erzwingt, dass die gleiche “Zufallswahl” auch bei neuerlichen Durchläufen des R-Scriptes getroffen wird. Das Mass für die Güte einer NMDS ist der sogenanante Stress:\n\\[\n\\text{Stress} = 1 - R^2\n\\]\nIn unserem Fall wäre der Stress also 1 – 0.977, also 2.3%, mithin sehr niedrig. Nur in 2.3% der Fälle würde die Lage im zweidimensionalen NMDS-Raum also das Ranking der Distanzen anders als das Ranking der Distanzen im ursprünglichen n-dimensionalen Hyperraum wiedergeben."
  },
  {
    "objectID": "Statistik_6.html#zusammenfassung",
    "href": "Statistik_6.html#zusammenfassung",
    "title": "Statistik 6",
    "section": "Zusammenfassung",
    "text": "Zusammenfassung\n\n\n\n\n\n\n\nOrdinationen sind im Kern deskriptive Verfahren für multivariate (abhängige) Variablen und komplementär zu Cluster-Analysen.\nIhre Ziele sind Dimensionsreduktion und Visualisierung.\nDie basale Form einer Ordination ist die PCA. Sie setzt lineare Beziehungen und wenige Nullwerte in der Matrix voraus.\nAbgesehen von Visualisierungen kann man PCAs auch zum Generieren unkorrelierter synthetischer Variablen für nachfolgende multiple Regressionsanalysen verwenden.\nAuf ökologische Gemeinschafts-Daten angewandt, ergeben PCA und CA normalerweise einen Hufeisen-Effekt, wobei standörtlich besonders unähnliche Plots nahe beieinander zu liegen kommen.\nDCA und NMDS versuchen das zu verhindern, indem sie entweder das Hufeisen “herausrechnen” oder von vornherein nur mit Rängen arbeiten."
  },
  {
    "objectID": "Statistik_6.html#weiterführende-literatur",
    "href": "Statistik_6.html#weiterführende-literatur",
    "title": "Statistik 6",
    "section": "Weiterführende Literatur",
    "text": "Weiterführende Literatur\n\nBorcard, D., Gillet, F. & Legendre, P. 2018. Numerical ecology with R. 2nd ed. Springer, Cham: 435 pp. [mit R]\nCrawley, M.J. 2013. The R book. 2nd ed. John Wiley & Sons, Chichester, UK: 1051 pp. [mit R]\nEveritt, B. & Hothorn, T. 2011. An introduction to applied multivariate analysis with R. Springer, New York: 273 pp. [mit R]\nLeyer, I. & Wesche, K. 2007. Multivariate Statistik in der Ökologie. Springer, Berlin: 221 pp. [einfache Erklärung von Ordinationsmethoden, ohne R]\nMcCune, B., Grace, J.B. & Urban, D.L. 2002. Analysis of ecological communities. MjM Software Design, Gleneden Beach, Oregon, US: 300 pp. [gut erklärte und detaillierte Einführung in Ordinationen u.a., ohne R]\nOksanen, L. 2015. Multivariate analysis of ecological communities in R: vegan tutorial. URL: http://cc.oulu.fi/~jarioksa/opetus/metodi/vegantutor.pdf. [gute Einführung in das R-package vegan mit vielen Ordinationsmethoden]\nWildi, O. 2013. Data analysis in vegetation ecology. 2nd ed.Wiley-Blackwell, Chichester, UK: 301 pp. [mit R]\nWildi, O. 2017. Data analysis in vegetation ecology. 3rd ed. CABI, Wallingford, UK: 333 pp. [mit R]"
  },
  {
    "objectID": "Statistik_6.html#quellen-der-beispiele",
    "href": "Statistik_6.html#quellen-der-beispiele",
    "title": "Statistik 6",
    "section": "Quellen der Beispiele",
    "text": "Quellen der Beispiele\n\nBruelheide, H., Dengler, J., Purschke, O., Lenoir, J., Jiménez-Alfaro, B., Hennekens, S.M., Botta-Dukát, Z., Chytrý, M., Field, R., (…) & Jandt, U. 2018. Global trait–environment relationships of plant communities. Nature Ecology and Evolution 2: 1906–1917.\nDíaz, S., Kattge, J., Cornelissen, J.H.C., Wright, I.J., Lavorel, S., Dray, S., Reu, B., Kleyer, M., Wirth, C.(…) & Gorné, L.D. 2016. The global spectrum of plant form and function. Nature 529: 167–171."
  },
  {
    "objectID": "Statistik_7.html#lernziele",
    "href": "Statistik_7.html#lernziele",
    "title": "Statistik 7",
    "section": "Lernziele",
    "text": "Lernziele\n\n\n\n\n\n\nIhr…\n\nwisst, wie man durch post-hoc gefittete Umweltvariablen (als Vektoren oder response surfaces) Ordinationen informativer machen kann;\nhabt verstanden, was “constrained” Ordinationen von normalen Ordinationen unterscheidet; und\nkönnt eine RDA anwenden und ihre Ergebnisse interpretieren, um einen multivariaten Datensatz effektiv zu analysieren."
  },
  {
    "objectID": "Statistik_7.html#interpretation-von-ordinationsergebnissen",
    "href": "Statistik_7.html#interpretation-von-ordinationsergebnissen",
    "title": "Statistik 7",
    "section": "Interpretation von Ordinationsergebnissen",
    "text": "Interpretation von Ordinationsergebnissen\n\nBeschriftung der Variablen\nDie Interpretation eines Ordinationsdiagramms wird durch Beschriftung der Variablen (und ggf. der Beobachtungen) wesentlich unterstützt. Bei der Ordination von gemeinschaftsökologischen Daten stellen allerdings die grosse Zahl der Artnamen und ihre grosse Länge eine Herausforderung dar. Wenn man in unserem Moordatensatz aus der letzten Lektion mit seinen 119 Arten einfach alle ungefiltert und ungekürzt in das Diagramm plotten würde, wären weder die Punkte des Diagramms erkennbar, noch die Namen lesbar. Insofern bietet es sich an, eine Teilmenge besonders aussagekräftiger Arten (d. h. Variablen) auszuwählen. Mit dem in vegan implementierten Befehl make.cepnames werden diese auf 8 Buchstaben gekürzt (4 vom Gattungsnamen und 4 vom Artepithet), was in fast allen Fällen eindeutig ist. Zudem kann man die relative Position der Beschriftung zum jeweiligen Punkt durch den Parameter pos steuern (oben, unten, rechts, links)).\n# 4+4-Abkürzung der Namen\nsnames &lt;- make.cepnames(snames)\n\n# Individuelle Position der Namen\ntext(sx,sy,snames,pos=c(1,2,1,1,3,2,4,3,1),cex=0.8)\n\n\n\n\n\n\n\nPost hoc-Korrelation von Umweltvariablen\nIn gemeinschaftsökologischen Datensätzen ist ja eine wichtige Frage meist, welche Umweltvariablen für die Verteilung der Arten in den Gemeinschaften/Vegetationsaufnahmen verantwortlich sind. Zur Rekapitulation: unsere bisherigen Ordinationsmethoden haben einzig die Artenvorkommen als Informationen (Variablen) genutzt. Eine Interpretationen der dahinterliegenden Umweltgradienten geschah bislang nur auf Basis unseres ökologischen Wissens über die Arten (sofern vorhanden). Sofern es jedoch auch erhobene Umweltdaten zu jeder Beobachtung gibt, können wir diese nachträglich (post hoc) zur Interpretation heranziehen. Wichtig ist dabei, dass diese zusätzlichen Umweltvariablen hier nicht die eigentliche Ordination beeinflusst haben, sondern nur zur nachträglichen Interpretation herangezogen werden (daher post hoc). Für unseren Moordatensatz gibt es tatsächich auch einen zusätzlichen Datensatz mit Umweltvariablen, die in jeder Vegetationsaufnahme erhoben wurden (enthalten im data frame ssit). Wir wählen davon fünf aus, um das Prinzip post hoc-gefitteter Umweltvariablen im Fall einer CA vorzustellen:\nsel.sites &lt;- c(\"pH.peat\", \"Acidity.peat\", \"CEC.peat\", \"P.peat\", \"Waterlev.max\")\nev &lt;- envfit(ca, ssit[,sel.sites])\nplot(ca, display = \"sites\", type = \"point\")\nplot(ev, add=T, cex=0.8)\n\n\n\n\n\n\n\nResponse surfaces\nDie nachträglich gefitteten Vektoren der Umweltvariablen suggerieren allerdings eine Linearität im Ordinationsraum, die oftmals nicht gegeben ist. Daher ist es oft angemessener stattdessen Response surfaces zu visualisieren, was mit dem Befehl ordisurf in vegan geht. Diese werden vom Programm mit GAMs gefittet. Allerdings kann man so kaum mehr als zwei Variablen auf einmal darsellen, weswegen die Variante mit den Vektorpfeilen oben weiterhin ihre Berechtigung hat:\nplot(ca, display = \"sites\", type = \"point\")ordisurf(ca, ssit$pH.peat, add=T)\n\n\n\n\n\n\n\nZeitliche Entwicklung\nBesonders aufschlussreich können Ordinationen von gemeinschaftsökologischen Daten sein, wenn zeitliche Entwicklungen analysiert, d. h. die gleiche Gemeinschaft mehrfach im Abstand von Jahren oder Jahrzehnten erhebt. Dies zeigt die Abbildung aus einer unserer Publikationen, wo 16 Vegetationsaufnahmen aus vier verschiedenen Vegetationstypen im Abstand von zwanzig Jahren wieder aufgenommen wurden. Die Vegetationstypen sind farbig codiert, die alten Aufnahmen gestrichelt, die neuen gefüllt und die Richtung der Veränderung wurde für jeden Vegetationstyp als Vektor zwischen dem alten und neuen Zentroid des Vegetationstyps dargestellt. Der zugehörige R-Code ist allerdings etwas komplexer, so dass wir ihn hier nicht besprechen:\n\n\n\naus Hüllbusch et al. 2016"
  },
  {
    "objectID": "Statistik_7.html#einführung-constrained-ordinations",
    "href": "Statistik_7.html#einführung-constrained-ordinations",
    "title": "Statistik 7",
    "section": "Einführung Constrained Ordinations",
    "text": "Einführung Constrained Ordinations\nBislang haben wir mit normalen (unconstrained) Ordinationen gearbeitet, was das gängige Verfahren für Datensätze aus allen Disziplinen ist. Hier wurde die Transformation des ursprünglichen n-dimensionalen Hyperraumes auf eine oder wenige Ordinationsebenen allein basierend auf den Informationen in unseren Variablen vorgenommen.\nIm Fall von gemeinschaftsökologischen Daten sind unsere Variablen die einzelnen Arten (bzw. deren Häufigkeit in den einzelnen Gemeinschaften/Vegetationsaufnahmen). In diesem Fall interessiert uns aber oft primär, welche Umweltvariablen für das sich ergebende Ordinationsmuster hauptsächlich verantwortlich sind. Dafür können wir zwei Wege wählen:\n\nWir können post hoc die Umweltvariablen als Vektoren oder Response surfaces in das Ordinationsdiagramm plotten, das ohne sie gerechnet wurde (siehe voriges Kapitel).\nWir können die Umweltvariablen schon direkt bei der Berechnung der Ordination einbeziehen. Dann spricht man von einer “constrained” = “canonical” Ordination. Diese betrachtet nur den Anteil der Artverteilungsmuster, der durch die erhobenen Umweltvariablen erklärt werden kann.\n\n\n\n\n\n\n\nFrage\n\n\n\nKennt ihr eine Situation in anderen Disziplinen ausser der Community Ecology, wo “constrained“ Ordinationen zum Einsatz kommen (könnten)?\n(Dafür brauchen wir einen multivariaten Satz abhängiger und einen multivariaten Satz unabhängiger Variablen)\n\n\nFür die beiden wesentlichen besprochenen Ordinationsverfahren PCA (für lineare Beziehungen) und CA (für unimodale Beziehungen) gibt es jeweils eine unconstrained- und eine constrained-Variante:\n\n\n\n\n\nDas Prinzip und der konzeptionelle Ablauf einer “constrained” Ordination sei am Beispiel eines gemeinschaftsökologischen Datensatzes kurz skizziert:\n\nMan hat für jede Vegetationsaufnahme (o. ä.) zusätzlich zu den Artdaten (abhängige Variablen) ein Set von dort erhobenen Umweltvariablen (unabhängige Variablen).\nZunächst werden die Artmächtigkeiten der einzelnen Arten zu den betrachteten Umweltvariablen jeweils mit einer multiplen linearen Regression in Beziehung gesetzt.\nFür die Ordination (PCA bzw. CA) werden dann statt der tatsächlichen Artmächtigkeiten die von der multiplen Regression vorhergesagten Artmächtigkeiten genommen\nMan kann anschliessend ermitteln, wie viel der Gesamtvarianz durch die verwendeten Umweltvariablen erklärt wird\n\nIn R passiert all das automatisch, wenn wir in vegan z. B. den Befehl cca für Canonical Correspondence Analysis wählen:\ns5 &lt;- c(\"pH.peat\",\"P.peat\",\"Waterlev.av\",\"CEC.peat\",\"Acidity.peat\")\nssit5 &lt;- ssit[s5]\no.cca &lt;- cca(sveg~. ,data=ssit5)\nplot(o.cca)"
  },
  {
    "objectID": "Statistik_7.html#redundancy-analysis-rda-im-detail",
    "href": "Statistik_7.html#redundancy-analysis-rda-im-detail",
    "title": "Statistik 7",
    "section": "Redundancy Analysis (RDA) im Detail",
    "text": "Redundancy Analysis (RDA) im Detail\n\nDie Idee\nWir schauen uns nun die Redundanzanalyse (RDA) im Detail an, welche die “constrained”-Variante der Hauptkomponentenanalyse (PCA) ist (deswegen werden in vegan beide mit dem gleichen Befehl rda gerechnet, vgl. Statistik 6).\nEine RDA wird für Datensätze angewandt, in denen man zahlreiche Objekte (observations) mit jeweils vielen abhängigen und vielen unabhängigen Variablen hat und erklären will, welche von den unabhängigen Variablen für die multivariate Antwort verantwortlich sind.\nZwei typische Beispie sollen das Prinzip verdeutlichen, das natürlich auch in anderen Disziplinen auftreten kann (Die Tilde ~ wird hier in typischer R-Schreibweise genutzt, um die abhängigen Variablen links von den unabhängigen rechts zu trennen):\n\nZusammensetzung von Pflanzengesellschaften (Anteile von Arten in Probeflächen) ~ Umweltparameter in diesen Probeflächen\nPolitische Einstellungen von Menschen (z. B. als Beantwortung diverser Fragen auf einer Skala) ~ sozioökonomische Eigenschaften dieser Personen (z. B. Geschlecht, Alter, Bildung, Einkommen, Wohnort,…)\n\n\n\nNotwendige Datentransformation für gemeinschaftsökologische Daten\nWir erinnern uns, dass in Statistik 5, von der Verwendung der PCA im Fall von gemeinschaftsökologischen Daten generell abgeraten wurde. Eine Hauptursache für die schlechte Eignung in diesen Fällen, ist dass die PCA (und damit auch die RDA) standardmässig mit der euklidischen Distanz zwischen zwei Objekten arbeitet, also der Länge der Gerade zwischen den beiden Objekten im multivariaten Raum (im zweidimensionalen Fall wäre das die Hypothenuse des rechtwinkligen Dreiecks, das durch die x/y-Koordinaten der beiden Beobachtungen gebildet wird; die Entfernung (= euklidische Distanz) berechnet sich dann einfach mit dem Satz des Pythagoras, analog auch für alle höheren Dimensionen). Für Daten von Artengemeinschaften (mit typischerweie vielen Nullwerten und unimodalen Verteilungen) ist die euklidische Distanz aber ungeeignet, da sie unerwünschte Artefakte (wie den diskutierten Hufeiseneffekt) erzeugt.\nDies haben Legendre & Gallagher (2001) schön mit einer Simulation gezeigt. Zugleich konnten sie zeigen, dass ein anderes Distanzmass, die Hellinger-Distanz diese Probleme in viel geringerem Umfang hat. Hier zunächst noch einmal die Definition der beiden Distanzmasse, mit x1, x2: Standort, j = 1… p: Arten, yi,j: Artmächtigkeit Art j an Standort i:\nEuklidische Distanz:\n\n\n\n\n\n\n\nHellinger-Distanz:\n\n\n\n\n\n\n\nUm das “Verhalten” dieser beiden Distanzmasse wurde ein Datensatz mit einem geografischen bzw. Umweltgradienten simuliert, entlang dem insgesamt neun Arten mit unimodalen Verteilungen (ungefähr Gauss’schen response curves) auftreten. Nach unserer Notation von Statistik 6 würden diese 19 Beobachtungspunkte (sites) zusammen einen Diversitätsgradienten von mehr als 8 SD-Einheiten repräsentieren (d.h. zwei vollständige Artenturnovers, vgl. die Kurven für Species 2 and Species 4). Wie man sieht, ist die Rangkorrelation zwischen Distanzmass und tatsächlicher geographischer Distand nach erfolgter Hellinger-Transformation viel besser (95 %), allerdings findet auch hier bei einer geografischen Distanz &gt; 8 keine weitere Differenzierung statt, da die Artengemeinschaften dann keine gemeinsame Art mehr haben.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(aus Legendre & Gallagher 2001)\n\n\n\nDie Schlussfolgerung ist, dass man mit der Hellinger-Distanz auch für gemeinschaftsökologische Daten RDAs (und PCAs) andwenden kann.\n\n\nEin Beispiel\nUnser Beispiel stammt aus dem sehr empfehlenswerten Buch von Borcard et al. (2018), das insbesondere deskriptiv-multivariate Verfahren im Bereich der Ökologie umfangreich erklärt und dazu die R-Codes liefert:\nEiner der Datensätze aus dem Buch beschreibt die Fischgemeinschaften an 30 Probestellen (sites) des Flusses Doubs im schweizerisch-französischen Grenzgebiet. An allen Probestellen wurden relative Abundanzen von 27 Fischarten (jeweils 0–5; dependent variables) und 11 Umweltvariablen (independent variables) erhoben. Die folgende Abbildung zeigt für vier häufige Arten die Vereilungsmuster in simplen R-genierten Kärtchen:\n\n\n\n\n\n\n\nGenerelles zum rda-Befehl\nHier seien kurz drei Syntax-Varianten des rda-Befehls im Package vegan vorgestellt:\nsimpleRDA &lt;- rda (Y, X, W)\n\nY = Antwort-Matrix\nX = Matrix der erklärenden Variablen (nur numerisch)\nW = Matrix der Co-Variablen (optional, für partielle RDAs)\n\nformulaRDA &lt;- rda (Y ~ var1 + factorA + var2*var3 + Condition(var4),\ndata = Xwdata)\n\nHier auch möglich\n- Faktoren (d. h. kategoriale Variable)\n- Interaktionen\n\nspe.rda &lt;- rda (spe.hel ~ ., env3)\n\nKurzschreibweise\n&gt; bedeutet: alle Variablen aus dataframe env3\n\n\n\nInterpretation der Ergebnisse\nWir schauen uns nun die Ergebnisse an, wenn wir die RDA mit Hellingertransformierten Arthäufigkeiten und allen 10 Umweltvariablen rechnen:\nrda(formula = spe.hel ~ ele + slo + dis + pH + har + pho + nit +      amm + oxy + bod, data = env3) \nPartitioning of variance:\n              Inertia Proportion\nTotal          0.5025     1.0000\nConstrained    0.3654     0.7271\nUnconstrained  0.1371     0.2729\nWie wir sehen, enthält der erste Teil des Ergebnis-Outputs eine Varianzpartitionierung. Die Gesamtvarianz wird aufgeteilt in jenen Anteil der durch die Umweltvariablen erklärt wird (constrained) und die unerklärte Restvarianz (unconstrained). Der Wert entspricht \\(R^2\\) in linearen Modellen, hat aber einen bias (s. u.).\nDer Output geht wie folgt weiter:\nImportance of components:\n                        RDA1   RDA2    RDA3    RDA4     RDA5     RDA6\nEigenvalue            0.2281 0.0537 0.03212 0.02321 0.008699 0.007218\nProportion Explained  0.4539 0.1069 0.06392 0.04618 0.017311 0.014363\nCumulative Proportion 0.4539 0.5607 0.62466 0.67084 0.688155 0.702518\n\n[…]\n\n                          RDA12     PC1     PC2     PC3     PC4\nEigenvalue            0.0003405 0.04581 0.02814 0.01528 0.01399\nProportion Explained  0.0006776 0.09116 0.05601 0.03042 0.02784\nCumulative Proportion 0.7270922 0.81825 0.87425 0.90467 0.93251\nWir sehen 12 RDA-Achsen (12 statt 10, da eine der Variablen ein Faktor war, der in drei dummy-Variablen zerlegt wurde). Die restliche Varianz findet sich dann auf den “unconstrained”-Achsen, die mit PC1, PC2 usw. benannt sind. Die Varianz auf diesen Achsen steht für nicht gemessene Variablen (oder auch Interkationen und unimodale Beziehungen dergemessenen Variablen).\nAccumulated constrained eigenvalues\nImportance of components:\n                        RDA1   RDA2    RDA3    RDA4     RDA5     RDA6\nEigenvalue            0.2281 0.0537 0.03212 0.02321 0.008699 0.007218\nProportion Explained  0.6243 0.1470 0.08791 0.06351 0.023808 0.019755\nCumulative Proportion 0.6243 0.7712 0.85913 0.92264 0.946448 0.966202\nIn diesem Fall erklärt die erste RDA-Achse schon ungewöhnlich hohe 62% der Gesamtvarianz, mit der zweiten Achse zusammen gar 77%. Der Output geht aber noch weiter…\nScaling 2 for species and site scores\n* Species are scaled proportional to eigenvalues\n* Sites are unscaled: weighted dispersion equal on all dimensions\n* General scaling constant of scores:  1.93676 \nSpecies scores\n         RDA1     RDA2      RDA3      RDA4      RDA5      RDA6\nCogo  0.13386  0.11619 -0.238205  0.018531  0.043161 -0.029728\nSatr  0.64240  0.06654  0.123649  0.181606 -0.009584  0.029785\nPhph  0.47477  0.07009 -0.010153 -0.115349 -0.045312 -0.030034\nBabl  0.36260  0.06966  0.041311 -0.190563 -0.046944  0.006446\nThth  0.13081  0.10707 -0.239273  0.043512  0.065818  0.003468\n[…]\nSpecies scoressind die Koordinaten der Spitzen von Artvektoren in Bi- und Triplots. Es gibt zwei Scaling-Optionen, wobei Scaling 2 der default ist. Und es geht noch weiter:\n\nSite scores (weighted sums of species scores)\n       RDA1      RDA2     RDA3      RDA4      RDA5      RDA6\n1   0.40149 -0.154133  0.55506  1.601005  0.193044  0.916850\n2   0.53522 -0.025131  0.43393  0.294832 -0.518997  0.458849\n3   0.49429 -0.014617  0.49415  0.169258 -0.246061  0.163409\n4   0.33451  0.001188  0.51644 -0.320793  0.089569 -0.219820\nSite scores sind die Koordinaten der Untersuchungsflächen im Raum der abhängigen Variablen Y (hier also der Arten).\nSite constraints (linear combinations of constraining variables)\n       RDA1      RDA2     RDA3      RDA4      RDA5     RDA6\n1   0.55130  0.002681  0.47744  0.626961 -0.210684  0.31503\n2   0.29736  0.105880  0.64854  0.261364 -0.057127  0.09312\n3   0.36843 -0.185333  0.59805  0.324556 -0.001611  0.31093\n4   0.44346 -0.066361  0.33293 -0.344230 -0.279546 -0.37077\nSite constraints sind die Koordinaten der Untersuchungsflächen im Raum der Prädiktorvariablen X (hier also der Umweltvariablen).\nWährend dieser primäre Output schon sehr aufschlussreich war, gibt es noch weitere Dinge, die uns interessieren (sollten):\ncoef(spe.rda)\n                        RDA1          RDA2          RDA3\nele             0.0004483347  7.795777e-05  0.0005188756\nslo.moderate   -0.0123140760 -1.655649e-02  0.0160736225\nslo.steep       0.0480170930  4.905556e-02  0.1023432587\nslo.very_steep  0.0181630025 -5.708251e-02  0.2326204779\ndis            -0.0014041126  4.456720e-03  0.0089169975\ncoef (spe.rda) sind die Regressionskoeffizienten der Variablen zu den Achsen.\n\\# Unadjusted R\\^2 und Adjusted R\\^2\n(R2 &lt;- RsquareAdj(spe.rda))\n$r.squared\n[1] 0.7270922\n\n$adj.r.squared\n[1] 0.5224114\nDer originale (unadjusted) \\(R^2\\) ist derselbe, den wir oben im Haupt-Output bekommen haben. R2-adjusted dagegen misst die erklärte Varianz ohne bias (bias resultiert daraus, dass bei vielen Variablen zwischen diesen auch rein zufällig Korrelationen auftreten).\n\n\nVisualisierung der Ergebnisse\nDa eine RDA ein statistisch komplexes Verfahren ist, gibt es auch nicht nur eine Art und Weise, die Ergebnisse zu visualisieren, sondern zwei, Scaling 1 und Scaling 2. Diese sind im Folgenden gezeigt und ihre Unterschiede stichpunktartig erklärt. Scaling 1 eignet sich meist besser für die Visualisierung von Objekten (sites) und Scaling 2 meist bessser für die Visualisierung von Antwortvariablen (species).\nDistanz-Triplot (Scaling 1):\n Winkel zwischen Antwort- und erklärenden Variablen entsprechen deren Korrelationen (aber nicht jene zwischen Antwortvariablen)\n\nDie Beziehung von Zentroiden qualitativer Variablen (Faktoren) und Antwortvariablen ergibt sich aus der Projektion der Zentroide im rechten Winkel auf die Anwortvariable.\nDistanzen zwischen Zentroiden und zwischen individuellen Objekten (sites) entsprechen ungefähr deren Distanzen im multivariaten Raum.\n\nKorrelations-Triplot (Scaling 2):\n\n\n\n1\n\n\nDie Projektion eines Objektes im rechten Winkel auf eine Antwort- oder eine numerische Prädiktorvariable entspricht dessen Wert entlang dieser Achse.\n\nWinkel zwischen Antwort- und erklärenden Variablen wie auch innerhalb beider Gruppen entsprechen deren Korrelationen\nDie Beziehung eines Zentroids einer qualitativen Variablen und der Antwortvariablen, ergibt sich aus seiner rechtwinkligen Projektion auf letztere.\nDistanzen zwischen Zentroiden und zwischen individuellen Objekten (sites) entsprechen nicht deren Distanzen im multivariaten Raum.\n\n\n\nSignifikanz der Achsen\nEine RDA produziert immer viele Achsen, aber die entscheidende Frage ist, welche davon signifikant sind (eine Frage, die wir nur im Falle von constrained-Ordinationen stellen können, da diese im Gegensatz zu den rein deskriptiven unconstrained-Ordinationen eine inferenzstatistische Komponente haben). Da die Voraussetzungen parametrischer Tests in der Regel massiv verletzt sind, kann die Signifikanz nur mit Permutationen gestestet werden:\n# Global test of the RDA result\nanova(spe.rda, permutations = how(nperm = 999))\nPermutation test for rda under reduced model\nPermutation: free\nNumber of permutations: 999\nModel: rda(formula = spe.hel ~ ele + slo + dis + pH + har + pho + nit + amm + oxy + bod, data = env3)\n         Df Variance      F Pr(&gt;F)    \nModel    12  0.36537 3.5523  0.001 ***\nResidual 16  0.13714\n# Tests of all canonical axes\nanova(spe.rda, by = \"axis\", permutations = how(nperm = 999))\nPermutation test for rda under reduced model\nForward tests for axes\nPermutation: free\nNumber of permutations: 999\nModel: rda(formula = spe.hel ~ ele + slo + dis + pH + har + pho + nit + amm + oxy + bod, data = env3)\n         Df Variance       F Pr(&gt;F)    \nRDA1      1 0.228083 26.6105  0.001 ***\nRDA2      1 0.053698  6.2649  0.004 ** \nRDA3      1 0.032119  3.7473  0.333    \nRDA4      1 0.023206  2.7074  0.775    \nRDA5      1 0.008699  1.0149  1.000 \nWir sehen, dass in diesem Fall die ersten beiden Achsen (RDA1, RDA2) signifikant sind. Nur diese sollten abgebildet werden!\n\n\nPartielle RDA und Varianzpartitionierung\nBei vielen Umweltvariablen können ggf. partielle RDAs aufschlussreich sein, die im Prinzip analog zu partiellen Regressionsplots (vgl. Statistik 3) funktionieren. Man kann dies für einzelne Variablen oder für Gruppen von Variablen machen. Zum Beispiel könnten wir fragen: Wie viel von der Zusammensetzung der Firschgemeinschaften erklärt die Wasserchemie, wenn man die topografischen Variablen konstant hält? Mit vegan geht das folgendermassen, einschliesslich Visualisierung in einem sogenannten Venn-Diagramm:\n# Formula interface; X and W variables must be in the same \n# data frame\n(spechem.physio2 &lt;- \n    rda(spe.hel ~ pH + har + pho + nit + amm + oxy + bod \n        + Condition(ele + slo + dis), data = env2))\nanova(spechem.physio2, permutations = how(nperm = 999))\nanova(spechem.physio2, permutations = how(nperm = 999), by = \"axis\")\n(spe.part.all &lt;- varpart(spe.hel, envchem, envtopo))\n\n# Plot of the partitioning results\ndev.new(title = \"Variation partitioning - all variables\", \n        noRStudioGD = TRUE)\nplot(spe.part.all, digits = 2, bg = c(\"red\", \"blue\"),\n     Xnames = c(\"Chemistry\", \"Physiography\"), \n     id.size = 0.7)\n\n\n\n\n\nDas Venn-Diagramm visualisiert die Varianzaufteilung zwischen zwei (oder mehr Variablen oder Gruppen von Variablen). Hier erkären die chemischen Variablen 24 %, die pysiographischen (topographischen) 11 % jeweils unabhängig voneinander, wohingegen ein grosser Teil der Varianz (23 %) von beiden Variablengruppen gemeinsam erklärt wird (weil sie nicht völlig unkorreliert sind)."
  },
  {
    "objectID": "Statistik_7.html#zusammenfassung",
    "href": "Statistik_7.html#zusammenfassung",
    "title": "Statistik 7",
    "section": "Zusammenfassung",
    "text": "Zusammenfassung\n\n\n\n\n\n\n\nPost-hoc gefittete Umweltvariablen dienen der nachträglichen Beschreibung der allein aufgrund der Artdaten gefundenen Ähnlichkeitsmuster.\n“Constrained” Ordinationen (RDA, CCA) betrachten dagegen von vornherein nur den Anteil der Ähnlichkeitsmuster in der Artenmatrix, der sich (in linearen Modellen) durch die gemessenen Umweltvariablen erklären lässt.\nEine RDA kann nicht nur deskriptiv gebraucht werden, sondern man kann auch die Signifikanz von Achsen analysieren oder Varianz partitionieren."
  },
  {
    "objectID": "Statistik_7.html#weiterführende-literatur",
    "href": "Statistik_7.html#weiterführende-literatur",
    "title": "Statistik 7",
    "section": "Weiterführende Literatur",
    "text": "Weiterführende Literatur\n\nBorcard, D., Gillet, F. & Legendre, P. 2018. Numerical ecology with R. 2nd ed. Springer, Cham: 435 pp. [mit R]\nEveritt, B. & Hothorn, T. 2011. An introduction to applied multivariate analysis with R. Springer, New York: 273 pp. [mit R]\nLegendre, P. & Gallagher, E.D. 2001. Ecologically meaningful transformation for ordination of species data. Oecologia 129: 271–280.\nLeyer, I. & Wesche, K. 2007. Multivariate Statistik in der Ökologie. Springer, Berlin: 221 pp. [einfache Erklärung von Ordinationsmethoden, ohne R]\nMcCune, B., Grace, J.B. & Urban, D.L. 2002. Analysis of ecological communities. MjM Software Design, Gleneden Beach, Oregon, US: 300 pp. [gut erklärte und detaillierte Einführung in Ordinationen u.a., ohne R]\nOksanen, L. 2015. Multivariate analysis of ecological communities in R: vegan tutorial. URL: http://cc.oulu.fi/~jarioksa/opetus/metodi/vegantutor.pdf. [gute Einführung in das R-package vegan mit vielen Ordinationsmethoden]\nWildi, O. 2017. Data analysis in vegetation ecology. 3rd ed. CABI, Wallingford, UK: 333 pp. [mit R]"
  },
  {
    "objectID": "Statistik_7.html#quellen-des-beispiels",
    "href": "Statistik_7.html#quellen-des-beispiels",
    "title": "Statistik 7",
    "section": "Quellen des Beispiels",
    "text": "Quellen des Beispiels\nHüllbusch, E., Brandt, L.M., Ende, P. & Dengler, J. 2016. Little vegetation change during two decades in a dry grassland complex in the Biosphere Reserve Schorfheide-Chorin (NE Germany). Tuexenia 36: 395−412."
  },
  {
    "objectID": "Statistik_8.html#lernziele",
    "href": "Statistik_8.html#lernziele",
    "title": "Statistik 8",
    "section": "Lernziele",
    "text": "Lernziele\n\n\n\n\n\n\nIhr…\n\nhabt eine prinzipielle Idee, wie Cluster-Analysen funktionieren;\nkönnt k-means clustering auf Datensätze anwenden; und\nkennt unterschiedliche Methoden der agglomerativen Clusteranalyse sowie der Bewertung von ihren Ergebnissen und könnt ihre jeweilige Eignung grob einschätzen."
  },
  {
    "objectID": "Statistik_8.html#clusteranalysen-allgemein",
    "href": "Statistik_8.html#clusteranalysen-allgemein",
    "title": "Statistik 8",
    "section": "Clusteranalysen allgemein",
    "text": "Clusteranalysen allgemein\nWie Ordinationen (Statistik 6 und 7) gehören Clusteranalysen zu den multivariat-deskriptiven Methoden. Wozu macht man dann Clusteranalysen?\n\nClusteranalysen sind komplementär zu Ordinationen: Bei Clusteranalysen liegt der Fokus auf den Unterschieden, während bei der Ordination der Fokus auf dem allmählichen Wandel entlang von Gradienten liegt. Insofern sind Ordinationen und Clusteranalysen Methoden, die für die gleichen Datensätze und z. T. ähnliche Fragestellungen angewendet werden können, aber mit Betonung unterschiedlicher Aspekte. Oftmals werden in einer Studie sogar beide Verfahren angewandt.\nPrinzipiell geht es bei Clusteranalysen um das Herausarbeiten von Gruppen von Objekten mit ähnlichen Eigenschaften, z. B.:\n\num diese zu beschreiben,\num diese auf Unterschiede zu testen oder\num deren Verbreitung in Karten darstellen zu können.\n\n\nEs gibt drei grundlegende Typen von Clusteranalysen, jeweils mit mehreren Methoden:\n\nPartitionierung (ohne Hierarchie)\nHierarchische Clusteranalyse\n\ndivisiv (der Gesamtdatensatz wird sukzessive in immer feinere Gruppen aufgeteilt)\nagglomerativ (beginnend mit den Einzelbeobachtungen werden diese immer weiter zu Gruppen zusammengefasst)\n\n\nIm Kurs behandeln wir nur die Partitionierung und verschiedene agglomerative Clusterferfahren. Ein divisives Clusterverfahren wäre z. B. TWINSPAN (Hill 1979; Roleček et al. 2009), welches in der Vegetationsökologie viel verwendet wird, m. W. nicht in R implementiert ist, dafür unter anderem im Freeware-Programm JUICE (Tichý 2002)."
  },
  {
    "objectID": "Statistik_8.html#k-means-clustering",
    "href": "Statistik_8.html#k-means-clustering",
    "title": "Statistik 8",
    "section": "k-means clustering",
    "text": "k-means clustering\nDas k-means clustering ist die einfachste Clustermethode überhaupt. Ihre Kernaspekte lassen sich wie folgt beschreiben:\n\nPartitionierung (ohne Hierarchie) in vom Benutzer vorgegebene k Cluster.\nVerfahren versucht die Summe der quadratische Abweichungen vom den Clusterzentren (Zentroide) zu minimieren.\nIn der Tendenz entstehen ± sphärische Cluster ähnlicher Grösse (sphärisch meint kugelförmig/isodiametrisch, aber eben nicht im dreidimensionalen, sondern im vieldimensionalen Variablenraum).\nDa das Ganze mit einem iterativen Optimierungsalgorithmus passiert, der mit zufällig gewählten Startpunkten beginnt, unterscheiden sich unterschiedliche Durchläufe im Ergebnis.\n\nDie Durchführung des k-means clustering eines multivariaten Datensatzes geschieht mit dem Befehl kmeans aus Base R, hier angewandt auf unseren Moordatensatz, den wir schon von den Ordinationen kennen:\nkmeans.2 &lt;- kmeans(sveg, 3)\nWie sehen unsere drei Cluster nun aus? Am besten plotten wir sie in das Ordinationsdiagramm, indem wir die Beobachtungen je nach Clusterzugehörigkeit einfärben:\nplot(pca, type = \"n\")\n\n\npoints(pca, display = \"sites\", pch=19, col=kmeans.2[[1]])\n\n\n\n\n\nWie viele Cluster sollte man nun unterscheiden? Oftmals ergibt sich die Zahl (oder zumindest eine Grössenordnung) aus dem Zweck, für den man die Clusteranalyse macht. Es gibt auch unterschiedliche numerische Kriterien, um die “beste” Partitionierung zu finden (allerdings liefern verschieden Gütemasse unterschiedliche Ergebnisse).\nEin Gütemass ist SSI = Simple Structure Index. Der SSI kombiniert drei Aspekte von Cluster-Güte: (a) maximale Differenz aller Variablen zwischen den Clustern, (b) Grössen der einzelnen Clustern und (c) Abweichung der Variablenwerte in den Clusterzentren vom Gesamtmittel. Der SSI reicht von 0 bis 1 und eine Partitionierung ist umso besser, je höher der Wert ist.\nWenn wir mit einem kurzen R-Code (wird in der Demo gezeigt) für unseren Moordatensatz die Partitionen von k = 2 bis 10 ausrechnen und jeweils den SSI berechnen, ergibt sich das folgende Bild:\n\n\n\n\n\nDie farbige Visualisierung links zeigt, dass es eben keine hierarchische Clusteranalyse ist. Bei k &gt; 2 bleibt die ursprüngliche Abgrenzung der zwei Hauptcluster nicht erhalten. Gemäss SSI wäre in diesem Fall die 10-Cluster-Lösung die beste (es sei aber empfohlen, solchen numerischen “Empfehlungen” nicht blindlings zu glauben)."
  },
  {
    "objectID": "Statistik_8.html#agglomerative-clusterverfahren",
    "href": "Statistik_8.html#agglomerative-clusterverfahren",
    "title": "Statistik 8",
    "section": "Agglomerative Clusterverfahren",
    "text": "Agglomerative Clusterverfahren\n\nEinführung\nBei agglomerativen Clusterverfahren folgt der Algorithmus immer dem folgenden Ablauf:\n\nSie fassen die beiden ähnlichsten Beobachtungen als initiales Cluster zusammen.\nDanach geht es mit dem Zusammenfassen des nächstähnlichen Paares von Einzelbeobachtungen bzw. Clustern so lange weiter, bis alle Cluster zu einem einzigen zusammengefasst sind.\n\nEs gibt deswegen so viele verschiedene agglomerative Clusterverfahren, da man zwei wesentliche Parameter im Prinzip frei kombinieren kann, das verwendete Distanzmass und den Modus für das Zusammenfügen von Clustern:\nAn Distanzmassen sind die folgenden beiden die gängigsten:\n\nEuklidische (pythagoreische) Distanz: Länge der Gerade, die die beiden Punkte im multidimensionalen Hyperraum miteinander verbindet.\nChord-Distanz: euklidische Distanz, nachdem alle Variablen auf Länge 1 standardisiert wurden.\n\nDie vier gängigsten Modi für das Zusammenfassen von Clustern sind:\n\nSingle linkage (nearest neighbour): Distanz zum nächsten Element eines Clusters wird genommen.\nComplete linkage (furthest neighbour): Distanz zum am weitesten entfernten Element eines Clusters wird genommen.\nAverage linkage (4 verschiedene Methoden, darunter besonders gängig UPGMA = unweighted pair-group method using arithmetic averages): Distanz zum Cluster”zentrum” wird genommen.\nWard’s mimimum variance clustering: Statt Distanzen zwischen Clustermitgliedern zu minimieren, wird hier die Clustervariabilität minimiert.\n\nSchauen wir uns an, welchen Effekt die vier Verfahren kombiniert mit der Chord-Distanz auf die Fischgemeinschaftsdaten des Doubs-Datensatzes haben:\n\n\n\n\n\n\n\n\nEs zeigt sich, dass die Cluster doch sehr unterschiedlich aussehen können. Die terminalen Cluster sind oft identisch (ein Cluster aus den Probestellen 17 und 18 gibt es etwas bei allen vier Methoden), doch auf höherer Ebene gibt es gravierende Unterschiede. Diese äussern sich insbesondere in der Anfälligkeit gegenüber Kettenbildung (Chaining), was meint, dass eine Aufnahme allen anderen gegenübergesellt wird und in diesem grossen Cluster im nächsten Schritt wieder eine einzige einzige Aufnahme dem Rest herausgegriffen usw. Single linkage ist methodenbedingt besonders anfällig für Chaining (siehe links oben). Da für die meisten Anwendungen solche Ein- Aufnahmen-Cluster unpraktisch sind, wird single linkage kaum noch verwendet. Complete linkage und UPGMA neigen weniger zu Chaining und die Ward-Methode am wenigsten.\n\n\nGüte von Clusterungen\nNun ist zwar Chaining unpraktisch, aber was, wenn es doch die realen Ähnlichkeitsbeziehungen am besten wiedergeben würde? Ein gutes Mass für die Güte eines Clusterergebnisses ist die Cophenetische Korrelation. Hier werden die Clusterpositionen in paarweise Distanzen zwischen Beobachtungen übersetzt und mit den ursprünglichen Distanzen verglichen (vergleichbar dem Stressplot im Falle einer NMDS-Ordination, vgl. Statistik 6). Schauen wir uns das Ergebnis für die vier Beispiele von oben an:\n\n\n\n\n\nAuch hier schneidet single linkage am schlechtesten ab. Wie meist, sind UPGMA und Ward am besten, wobei hier UPGMA sogar besser als Ward abschneidet.\n\n\nWie viele Cluster sollte man unterscheiden?\nWie schon bei der k-means-Partitionierung stellt sich auch beim hierarchischen Clustering die Frage nach der optimalen Zahl von unterschiedenen Clustern. Vielfach ergibt sich die Antwort darauf zumindest grössenordnungsmässig aus der geplanten Verwendung der Cluster. Es gibt auch verschiedene mathematische Gütemasse, u. a. Silhouette, Matrix-Korrelation und Indikatorarten:\nSihouette: mittlere Distanz eines Objektes zu allen Objekten eines Clusters zur mittleren Distanz zu allen Objekten des nächstähnlichen Clusters. Die Werte reichen von –1 bis +1.\n\n\n\n\n\nMatrix-Korrelation: Vergleich der originalen Unähnlichkeitsmatrix mit der binären Matrix basierend auf der Gruppenzusammengehörigkeit im Dendrogramm.\n\n\n\n\n\nIndikatorarten: Anzahl von Indikatorarten (links) bzw. Anteil von Clustern mit signifikanten Indikatorarten (rechts) (hier basierend auf dem IndVal-Konzept; siehe Borcard et al. 2018). Dieser Ansatz funktioniert natürlich nur, wenn es sich um Daten von Artengemeinschaften handelt.\n\n\n\n\n\n\n\nCharakterisierung von Clustern\nWie schon bei k-means können wir die Cluster dadurch charakterisieren, dass wir die Clusterzugehörigkeit in ein einfaches oder Biplot-Ordinationsdiagramm plotten. Weitere Möglichkeiten der Beschreibung/Charakterisierung von Clustern sind u. a. (jeweils visualisiert für die 4-Cluster-Lösung des Doubs-Datensatzes):\n\nEinfärbung im Dendrogramm (den R-Code dazu gibt es im Demoskript):\n\n Geordnete Community-Tabelle (im Fall von von gemeinschaftsökologischen Daten), ggf. mit Hervorhebung der signifikant konzentrierten Arten:\n      32222222222  111111     1111 \n      09876210543959876506473221341\n Icme 5432121......................\n Abbr 54332431.....1...............\n Blbj 54542432.1...1...............\n Anan 54432222.....111.............\n Gyce 5555443212...11..............\n Scer 522112221...21...............\n Cyca 53421321.....1111............\n Rham 55432333.....221.............\n Legi 35432322.1...1111............\n Alal 55555555352..322.............\n Chna 12111322.1...211.............\n Titi 53453444...1321111.21........\n Ruru 55554555121455221..1.........\n Albi 53111123.....2341............\n Baba 35342544.....23322.........1.\n Eslu 453423321...41111..12.1....1.\n Gogo 5544355421..242122111......1.\n Pefl 54211432....41321..12........\n Pato 2211.222.....3344............\n Sqce 3443242312152132232211..11.1.\n Lele 332213221...52235321.1.......\n Babl .1111112...32534554555534124.\n Teso .1...........11254........23.\n Phph .1....11...13334344454544455.\n Cogo ..............1123......2123.\n Satr .1..........2.123413455553553\n Thth .1............11.2......2134.\n  sites species \n     29      27 \n\nVergleich der (Umwelt-)Variablen zwischen den Clustern mittels ANOVA."
  },
  {
    "objectID": "Statistik_8.html#zusammenfassung",
    "href": "Statistik_8.html#zusammenfassung",
    "title": "Statistik 8",
    "section": "Zusammenfassung",
    "text": "Zusammenfassung\n\n\n\n\n\n\n\nk-means clustering ist eine einfache nicht-hierarchische Clustermethode, bei der der Benutzer vorgibt, wie viele Einheiten er haben möchte.\nAgglomerative Clusterverfahren fassen Einheiten sukzessive über ihre Ähnlichkeitsbeziehungen zusammen. Am Ende kann man dann subjektiv oder nach unterschiedlichen numerischen Kriterien entscheiden, welche Clusterauflösung dem Bedarf am besten entspricht."
  },
  {
    "objectID": "Statistik_8.html#weiterführende-literatur",
    "href": "Statistik_8.html#weiterführende-literatur",
    "title": "Statistik 8",
    "section": "Weiterführende Literatur",
    "text": "Weiterführende Literatur\n\nBorcard, D., Gillet, F. & Legendre, P. 2018. Numerical ecology with R. 2nd ed. Springer, Cham: 435 pp. [mit R]\nCrawley, M.J. 2013. The R book. 2nd ed. John Wiley & Sons, Chichester,UK: 1051 pp. [mit R]\nEveritt, B. & Hothorn, T. 2011. An introduction to applied multivariate analysis with R. Springer, New York: 273 pp. [mit R]\nHill, M.O. 1979. TWINSPAN – A FORTRAN program for arranging multivariate data in an ordered two-way table by classification of the individuals and attributes. Cornell University, Ithaca, NY: 90 pp.\nRoleček, J., Tichý, L., Zelený, D. & Chytrý, M. 2009. Modified TWINSPAN classification in which the hierarchy represents cluster heterogeneity. Journal of Vegetation Science 20: 596–602.\nTichý, L. 2002. JUICE, software for vegetation classification. Journal of Vegetation Science 13: 451–453.\nWildi, O. 2017. Data analysis in vegetation ecology. 3rd ed. CABI, Wallingford, UK: 333 pp. [mit R]"
  }
]